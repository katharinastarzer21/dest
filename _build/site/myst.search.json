{"version":"1","records":[{"hierarchy":{"lvl1":"Contribute to the Gallery"},"type":"lvl1","url":"/contribute","position":0},{"hierarchy":{"lvl1":"Contribute to the Gallery"},"content":"","type":"content","url":"/contribute","position":1},{"hierarchy":{"lvl1":"Contribute to the Gallery"},"type":"lvl1","url":"/contribute#contribute-to-the-gallery","position":2},{"hierarchy":{"lvl1":"Contribute to the Gallery"},"content":"If you’ve developed a Jupyter notebook that works with the DestinE Data Lake services and would like to share it with others, follow the steps below to add your repository to the Gallery:","type":"content","url":"/contribute#contribute-to-the-gallery","position":3},{"hierarchy":{"lvl1":"Contribute to the Gallery","lvl2":"Step-by-Step: Upload Your Repository"},"type":"lvl2","url":"/contribute#step-by-step-upload-your-repository","position":4},{"hierarchy":{"lvl1":"Contribute to the Gallery","lvl2":"Step-by-Step: Upload Your Repository"},"content":"Use the template repository\n\nClone the official template repository to your own GitHub account:git clone https://github.com/your-username/your-new-repo.git\n\nOr click the “Use this template” button on GitHub to create your own copy.\n\nAdd your content\n\nPlace your Jupyter Notebooks in the notebooks/ folder.\n\nFollow the example in notebooks/template.ipynb, paying close attention to the first Markdown cell, which must contain a YAML front matter block:---\ntitle: \"Your Notebook Title\"\nsubtitle: \"Brief description of what this notebook does.\"\nauthors: [\"Your Name\"]\ntags: [\"HDA\", \"Dask\", \"Workflow\"]\nthumbnail: /img/example.png\nlicense: MIT\ncopyright: \"© 2024 EUMETSAT\"\n---\n\nAdd a thumbnail image for each notebook, store it in the img/ folder, and reference it in the notebook metadata.\n\nConfigure repository settings\n\nEnable GitHub Pages\n\nGo to Settings → Pages.\n\nEnsure Branch is set to gh-pages and Folder is (root).\n\nIf it’s set to None, change it to gh-pages / (root) and click Save.\n\nEnable Required GitHub Actions Permissions\n\nGo to Settings → Actions → General.\n\nScroll to Workflow permissions.\n\nSelect Read and write permissions.\n\nClick Save if you made any changes.\n\nSubmit your repository\n\nOpen the \n\nGallery submission issue form.\n\nProvide:\n\nRepository URL\n\nShort title in UPPERCASE (for folder naming)\n\nReview process\n\nThe DestinE team will review your submission.\n\nIf accepted, it will be integrated into the official gallery and published automatically.","type":"content","url":"/contribute#step-by-step-upload-your-repository","position":5},{"hierarchy":{"lvl1":"Contribute to the Gallery","lvl2":"Best Practices"},"type":"lvl2","url":"/contribute#best-practices","position":6},{"hierarchy":{"lvl1":"Contribute to the Gallery","lvl2":"Best Practices"},"content":"Use clear titles and logical section headings.\n\nTag your notebooks meaningfully. Whenever possible, review the tags already used in the gallery and reuse them to maintain consistency. Pay close attention to correct spelling and case sensitivity (e.g., uppercase/lowercase).\n\nKeep dependencies minimal and list them explicitly.\n\nEnsure your notebook runs from top to bottom without errors.\n\nAdd explanations and context so users understand the workflow.","type":"content","url":"/contribute#best-practices","position":7},{"hierarchy":{"lvl1":"Contribute to the Gallery","lvl2":"Attention!"},"type":"lvl2","url":"/contribute#attention","position":8},{"hierarchy":{"lvl1":"Contribute to the Gallery","lvl2":"Attention!"},"content":"Once accepted, any changes you push to your repository will automatically appear on the website. Keep your repository clean, well-maintained, and up to date.","type":"content","url":"/contribute#attention","position":9},{"hierarchy":{"lvl1":"HDA Gallery"},"type":"lvl1","url":"/hda","position":0},{"hierarchy":{"lvl1":"HDA Gallery"},"content":"","type":"content","url":"/hda","position":1},{"hierarchy":{"lvl1":"HDA Gallery","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/hda#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"HDA Gallery","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nAVHRR Level 1B Metop Global - Data Access This notebook demonstrates how to search and access Metop data using HDA and how to read, process and visualize it using satpy.\n\nHDA\n\nMetop\n\nAVHRR\n\nsatpy\n\nView Notebook\n\n\n\nHigh rate SEVIRI Level 1.5 Image Data MSG - Data Access This notebook demonstrates how to access SEVIRI data using HDA and how to read, process and visualize it.\n\nHDA\n\nSEVIRI\n\nView Notebook\n\n\n\nOLCI Level 1B Reduced Resolution - Sentinel-3 This notebook demonstrates how to search and access Sentinel-3 data using HDA and how to read and visualize it using satpy.\n\nHDA\n\nOLCI\n\nSentinel-3\n\nsatpy\n\nView Notebook\n\n\n\nEODAG - DestinE Data Lake Provider This notebook demonstrates how to use the DEDL provider in EODAG.\n\nHDA\n\nEODAG\n\nView Notebook\n\n\n\nEODAG - A quick start with DEDL This notebook provides a quickstart guide for using the EODAG Python API and CLI to search, discover, and download DEDL data.\n\nHDA\n\nEODAG\n\nView Notebook\n\n\n\nERA5 hourly data on single levels from 1940 to present This notebook shows how to authenticate with the DestinE API, queries and downloads ERA5 single-level reanalysis data using the DEDL HDA service, and visualizes the result with EarthKit.\n\nHDA\n\nSTAC\n\nECMWF\n\nView Notebook\n\n\n\nUsing HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data In this notebook, we will present a simple example of how you can access data from DEDL using HDA and what you can do with it. We will demonstrate how to utilize thresholding techniques and compare the values of VV and VH polarizations to analyze urban areas. As an illustration, we will attempt to download Sentinel-1 images containing data of the urban area of Warsaw (Poland).\n\nSTAC\n\nAuthentication\n\nAccess Token\n\nView Notebook\n\n\n\nHow to use HDA to find and download data for conducting monitoring of Śniadrwy lake In this notebook, we will present a simple example on how you can access data from DEDL using HDA and what you can do with it. As an example, we will try to download Sentinel-2 images containining data of Śniadrwy lake from first week of July 2023. With usage of HDA and few Python packages, you will be able to obtain rasters with NDWI index.\n\nSTAC\n\nAuthentication\n\nAccess Token\n\nView Notebook\n\n\n\nHDA Tutorial - Queryables This notebook demonstrates how to use the queryables API to filter C3S and DestinE digital twin collections by leveraging variable terms that dynamically adjust based on user selections.\n\nHDA\n\nC3S\n\nDigital twin\n\nAuthentication\n\nView Notebook\n\n\n\nHDA Tutorial This notebook demonstrates the first steps using the Harmonised Data access API.\n\nHDA\n\nSTAC\n\nCore API\n\nView Notebook\n\n\n\nHDA Tutorial - Quick start This notebook demonstrates how to use the HDA (Harmonized Data Access) API by sending a few HTTP requests to the API, using Python code.\n\nHDA\n\nSTAC\n\nHTTP requests\n\nView Notebook\n\n\n\nUsing HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data This notebook demonstrates a simple example of how you can access data from DEDL using HDA and what you can do with it using an example with Sentinel-1 data.\n\nHDA\n\nSTAC\n\nSentinel-1\n\nThresholding techniques\n\nView Notebook\n\n\n\nHow to use HDA to find and download data for conducting monitoring of Śniadrwy lake This notebook demonstrates a simple example of how you can access data from DEDL using HDA and what you can do with it using an example with Sentinel-1 data.\n\nHDA\n\nSTAC\n\nSentinel-2\n\nsatpy\n\nView Notebook\n\n\n\nHDA PySTAC-Client Introduction This notebook shows the basic use of DestinE Data Lake Harmonised Data Access using pystac-client.\n\nSTAC\n\nAuthentication\n\nAccess Token\n\nView Notebook\n\n\n\nHDA Climate DT Parameter Plotter - Tutorial This notebook provides an interactive workflow to select, query, download, and visualize Climate Digital Twin parameters from the DestinE Data Lake using the DEDL HDA API.\n\nHDA\n\nECMWF\n\nEarthkit\n\nDigital Twin\n\nView Notebook\n\n\n\nClimate Change Adaptation Digital Twin Series This notebook authenticates a user with DestinE services, constructs and submits data requests to the DEDL HDA API for Climate Digital Twin projections, polls for availability, downloads GRIB data for multiple years, and visualizes it using EarthKit.\n\nHDA\n\nECMWF\n\nAuthentication\n\nDigital Twin\n\nView Notebook\n\n\n\nClimate Change Adaptation This notebook authenticates with the DestinE API, queries ECMWF Climate Digital Twin adaptation data based on ScenarioMIP parameters, downloads the selected forecast data using a robust retry mechanism, and visualizes it using EarthKit.\n\nECMWF\n\nAuthentication\n\nDigital Twin\n\nView Notebook\n\n\n\nWeather-Induced Extremes Digital Twin Series his notebook authenticates with the DestinE API, queries ECMWF Extremes Digital Twin forecast data for a user-selected date within the last 14 days, downloads it, and visualizes wind gust and sea-level pressure fields using EarthKit.\n\nHDA\n\nECMWF\n\nAuthentication\n\nDigital Twin\n\nView Notebook\n\n\n\nWeather-Induced Extremes - Data Access using DEDL HDA This notebook demonstrates how to access and download sea ice coverage data from the Weather-Induced Extremes Digital Twin using the DestinE Data Lake Harmonised Data Access (DEDL HDA) API, including authentication, filtering, polling, and visualizing the result on a map.\n\nECMWF\n\nEarthkit\n\nDigital Twin\n\nView Notebook\n\n\n\nHDA Extreme DT Parameter Plotter - Tutorial This notebook shows how to select, request, and download Climate Digital Twin Extremes data from the DestinE Data Lake (DEDL HDA), including user-defined parameter, scenario, date, and level selection, followed by secure authentication, API querying, and visualization using EarthKit.\n\nECMWF\n\nEarthkit\n\nDigital Twin\n\nView Notebook\n\n\n\nAviso notification for DT data availability This notebook shows how to check the data availablility for the Weather-Induced Extremes Digital Twin (Extremes DT) using the ECMWF Aviso package.\n\npyaviso\n\nDigital Twin\n\nView Notebook","type":"content","url":"/hda#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"HOOK Gallery"},"type":"lvl1","url":"/hook","position":0},{"hierarchy":{"lvl1":"HOOK Gallery"},"content":"","type":"content","url":"/hook","position":1},{"hierarchy":{"lvl1":"HOOK Gallery","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/hook#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"HOOK Gallery","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nAccess to Hook services This Notebook demonstrates the retrieval of a token appropriate for interaction with the OnDemand Processing API (Hook API) and listing of available Hooks (Processors) using the retrieved token.\n\nHook\n\nAuthentication\n\nToken\n\nView Notebook\n\n\n\nHook - Perform data harvesting This Notebook demonstrates how to perform data harvesting with Hook.\n\nHook\n\nAuthentication\n\nToken\n\nView Notebook\n\n\n\nHook Tutorial - Data Harvest This notebook demonstrates how to use the Hook service.\n\nHook\n\nAuthentication\n\nWorkflow\n\nStorage\n\nView Notebook","type":"content","url":"/hook#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"STACK Gallery"},"type":"lvl1","url":"/stack","position":0},{"hierarchy":{"lvl1":"STACK Gallery"},"content":"","type":"content","url":"/stack","position":1},{"hierarchy":{"lvl1":"STACK Gallery","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/stack#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"STACK Gallery","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nSTACK Service Dask This notebook introduces authentication and multi-cluster management using the DEDL Stack client with OIDC, enabling users to securely spawn, monitor, and scale Dask clusters across Central and LUMI locations within the DestinE Data Lake.\n\nSTACK\n\nDask\n\nGFM\n\nView Notebook\n\n\n\nInteractive Dashboard for ExtremeDT Weather Forecast Data with xcube This notebook guides users through accessing ExtremeDT weather data cubes, filtering them by region, converting units, and visualizing the results in an interactive dashboard using the xcube viewer.\n\nSTACK\n\nDataCube\n\nDigital Twin\n\nView Notebook\n\n\n\nExtremeDT Weather Data Cubes This notebook demonstrates how to access, explore, and visualize weather forecast data from the ExtremeDT data cubes using xarray and matplotlib, including spatial plots, time series analysis, and interactive dashboard preparation.\n\nSTACK\n\nDataCube\n\nDigital Twin\n\nView Notebook\n\n\n\nSTACK service - Dask 101 This notebook introduces Dask's core APIs and demonstrates how to use them for scalable, parallel, and distributed data processing, culminating in deploying and interacting with a Dask cluster on the DestinE Data Lake STACK service.\n\nSTACK\n\nDask\n\nCluster\n\nView Notebook\n\n\n\nSTACK service - Python Client Dask This notebook demonstrates how to use the DEDL Stack Python client to authenticate, manage, and execute parallel, multi-cloud Dask computations on distributed datasets stored across Central Site and LUMI bridge.\n\nSTACK\n\nDask\n\nGFM\n\nView Notebook","type":"content","url":"/stack#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"TESTING Gallery"},"type":"lvl1","url":"/testing","position":0},{"hierarchy":{"lvl1":"TESTING Gallery"},"content":"","type":"content","url":"/testing","position":1},{"hierarchy":{"lvl1":"TESTING Gallery","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/testing#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"TESTING Gallery","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nTemplate 1 Brief description of the notebook.\n\nTemplate\n\nView Notebook\n\n\n\nTemplate 2 Brief description of the notebook.\n\nTemplate\n\nView Notebook","type":"content","url":"/testing#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Access Token"},"type":"lvl1","url":"/tag-access-token","position":0},{"hierarchy":{"lvl1":"Notebooks Access Token"},"content":"","type":"content","url":"/tag-access-token","position":1},{"hierarchy":{"lvl1":"Notebooks Access Token","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-access-token#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Access Token","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nUsing HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data In this notebook, we will present a simple example of how you can access data from DEDL using HDA and what you can do with it. We will demonstrate how to utilize thresholding techniques and compare the values of VV and VH polarizations to analyze urban areas. As an illustration, we will attempt to download Sentinel-1 images containing data of the urban area of Warsaw (Poland).\n\nSTAC\n\nAuthentication\n\nAccess Token\n\nView Notebook\n\n\n\nHow to use HDA to find and download data for conducting monitoring of Śniadrwy lake In this notebook, we will present a simple example on how you can access data from DEDL using HDA and what you can do with it. As an example, we will try to download Sentinel-2 images containining data of Śniadrwy lake from first week of July 2023. With usage of HDA and few Python packages, you will be able to obtain rasters with NDWI index.\n\nSTAC\n\nAuthentication\n\nAccess Token\n\nView Notebook\n\n\n\nHDA PySTAC-Client Introduction This notebook shows the basic use of DestinE Data Lake Harmonised Data Access using pystac-client.\n\nSTAC\n\nAuthentication\n\nAccess Token\n\nView Notebook","type":"content","url":"/tag-access-token#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Authentication"},"type":"lvl1","url":"/tag-authentication","position":0},{"hierarchy":{"lvl1":"Notebooks Authentication"},"content":"","type":"content","url":"/tag-authentication","position":1},{"hierarchy":{"lvl1":"Notebooks Authentication","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-authentication#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Authentication","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nHDA Tutorial - Queryables This notebook demonstrates how to use the queryables API to filter C3S and DestinE digital twin collections by leveraging variable terms that dynamically adjust based on user selections.\n\nHDA\n\nC3S\n\nDigital twin\n\nAuthentication\n\nView Notebook\n\n\n\nUsing HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data In this notebook, we will present a simple example of how you can access data from DEDL using HDA and what you can do with it. We will demonstrate how to utilize thresholding techniques and compare the values of VV and VH polarizations to analyze urban areas. As an illustration, we will attempt to download Sentinel-1 images containing data of the urban area of Warsaw (Poland).\n\nSTAC\n\nAuthentication\n\nAccess Token\n\nView Notebook\n\n\n\nHow to use HDA to find and download data for conducting monitoring of Śniadrwy lake In this notebook, we will present a simple example on how you can access data from DEDL using HDA and what you can do with it. As an example, we will try to download Sentinel-2 images containining data of Śniadrwy lake from first week of July 2023. With usage of HDA and few Python packages, you will be able to obtain rasters with NDWI index.\n\nSTAC\n\nAuthentication\n\nAccess Token\n\nView Notebook\n\n\n\nHDA PySTAC-Client Introduction This notebook shows the basic use of DestinE Data Lake Harmonised Data Access using pystac-client.\n\nSTAC\n\nAuthentication\n\nAccess Token\n\nView Notebook\n\n\n\nClimate Change Adaptation This notebook authenticates with the DestinE API, queries ECMWF Climate Digital Twin adaptation data based on ScenarioMIP parameters, downloads the selected forecast data using a robust retry mechanism, and visualizes it using EarthKit.\n\nECMWF\n\nAuthentication\n\nDigital Twin\n\nView Notebook\n\n\n\nWeather-Induced Extremes Digital Twin Series his notebook authenticates with the DestinE API, queries ECMWF Extremes Digital Twin forecast data for a user-selected date within the last 14 days, downloads it, and visualizes wind gust and sea-level pressure fields using EarthKit.\n\nHDA\n\nECMWF\n\nAuthentication\n\nDigital Twin\n\nView Notebook\n\n\n\nClimate Change Adaptation Digital Twin Series This notebook authenticates a user with DestinE services, constructs and submits data requests to the DEDL HDA API for Climate Digital Twin projections, polls for availability, downloads GRIB data for multiple years, and visualizes it using EarthKit.\n\nHDA\n\nECMWF\n\nAuthentication\n\nDigital Twin\n\nView Notebook\n\n\n\nHook - Perform data harvesting This Notebook demonstrates how to perform data harvesting with Hook.\n\nHook\n\nAuthentication\n\nToken\n\nView Notebook\n\n\n\nHook Tutorial - Data Harvest This notebook demonstrates how to use the Hook service.\n\nHook\n\nAuthentication\n\nWorkflow\n\nStorage\n\nView Notebook\n\n\n\nAccess to Hook services This Notebook demonstrates the retrieval of a token appropriate for interaction with the OnDemand Processing API (Hook API) and listing of available Hooks (Processors) using the retrieved token.\n\nHook\n\nAuthentication\n\nToken\n\nView Notebook","type":"content","url":"/tag-authentication#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks AVHRR"},"type":"lvl1","url":"/tag-avhrr","position":0},{"hierarchy":{"lvl1":"Notebooks AVHRR"},"content":"","type":"content","url":"/tag-avhrr","position":1},{"hierarchy":{"lvl1":"Notebooks AVHRR","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-avhrr#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks AVHRR","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nAVHRR Level 1B Metop Global - Data Access This notebook demonstrates how to search and access Metop data using HDA and how to read, process and visualize it using satpy.\n\nHDA\n\nMetop\n\nAVHRR\n\nsatpy\n\nView Notebook","type":"content","url":"/tag-avhrr#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks C3S"},"type":"lvl1","url":"/tag-c3s","position":0},{"hierarchy":{"lvl1":"Notebooks C3S"},"content":"","type":"content","url":"/tag-c3s","position":1},{"hierarchy":{"lvl1":"Notebooks C3S","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-c3s#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks C3S","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nHDA Tutorial - Queryables This notebook demonstrates how to use the queryables API to filter C3S and DestinE digital twin collections by leveraging variable terms that dynamically adjust based on user selections.\n\nHDA\n\nC3S\n\nDigital twin\n\nAuthentication\n\nView Notebook","type":"content","url":"/tag-c3s#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Cluster"},"type":"lvl1","url":"/tag-cluster","position":0},{"hierarchy":{"lvl1":"Notebooks Cluster"},"content":"","type":"content","url":"/tag-cluster","position":1},{"hierarchy":{"lvl1":"Notebooks Cluster","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-cluster#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Cluster","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nSTACK service - Dask 101 This notebook introduces Dask's core APIs and demonstrates how to use them for scalable, parallel, and distributed data processing, culminating in deploying and interacting with a Dask cluster on the DestinE Data Lake STACK service.\n\nSTACK\n\nDask\n\nCluster\n\nView Notebook","type":"content","url":"/tag-cluster#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Core API"},"type":"lvl1","url":"/tag-core-api","position":0},{"hierarchy":{"lvl1":"Notebooks Core API"},"content":"","type":"content","url":"/tag-core-api","position":1},{"hierarchy":{"lvl1":"Notebooks Core API","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-core-api#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Core API","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nHDA Tutorial This notebook demonstrates the first steps using the Harmonised Data access API.\n\nHDA\n\nSTAC\n\nCore API\n\nView Notebook","type":"content","url":"/tag-core-api#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Dask"},"type":"lvl1","url":"/tag-dask","position":0},{"hierarchy":{"lvl1":"Notebooks Dask"},"content":"","type":"content","url":"/tag-dask","position":1},{"hierarchy":{"lvl1":"Notebooks Dask","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-dask#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Dask","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nSTACK service - Dask 101 This notebook introduces Dask's core APIs and demonstrates how to use them for scalable, parallel, and distributed data processing, culminating in deploying and interacting with a Dask cluster on the DestinE Data Lake STACK service.\n\nSTACK\n\nDask\n\nCluster\n\nView Notebook\n\n\n\nSTACK service - Python Client Dask This notebook demonstrates how to use the DEDL Stack Python client to authenticate, manage, and execute parallel, multi-cloud Dask computations on distributed datasets stored across Central Site and LUMI bridge.\n\nSTACK\n\nDask\n\nGFM\n\nView Notebook\n\n\n\nSTACK Service Dask This notebook introduces authentication and multi-cluster management using the DEDL Stack client with OIDC, enabling users to securely spawn, monitor, and scale Dask clusters across Central and LUMI locations within the DestinE Data Lake.\n\nSTACK\n\nDask\n\nGFM\n\nView Notebook","type":"content","url":"/tag-dask#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks DataCube"},"type":"lvl1","url":"/tag-datacube","position":0},{"hierarchy":{"lvl1":"Notebooks DataCube"},"content":"","type":"content","url":"/tag-datacube","position":1},{"hierarchy":{"lvl1":"Notebooks DataCube","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-datacube#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks DataCube","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nInteractive Dashboard for ExtremeDT Weather Forecast Data with xcube This notebook guides users through accessing ExtremeDT weather data cubes, filtering them by region, converting units, and visualizing the results in an interactive dashboard using the xcube viewer.\n\nSTACK\n\nDataCube\n\nDigital Twin\n\nView Notebook\n\n\n\nExtremeDT Weather Data Cubes This notebook demonstrates how to access, explore, and visualize weather forecast data from the ExtremeDT data cubes using xarray and matplotlib, including spatial plots, time series analysis, and interactive dashboard preparation.\n\nSTACK\n\nDataCube\n\nDigital Twin\n\nView Notebook","type":"content","url":"/tag-datacube#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Digital Twin"},"type":"lvl1","url":"/tag-digital-twin","position":0},{"hierarchy":{"lvl1":"Notebooks Digital Twin"},"content":"","type":"content","url":"/tag-digital-twin","position":1},{"hierarchy":{"lvl1":"Notebooks Digital Twin","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-digital-twin#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Digital Twin","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nHDA Climate DT Parameter Plotter - Tutorial This notebook provides an interactive workflow to select, query, download, and visualize Climate Digital Twin parameters from the DestinE Data Lake using the DEDL HDA API.\n\nHDA\n\nECMWF\n\nEarthkit\n\nDigital Twin\n\nView Notebook\n\n\n\nClimate Change Adaptation This notebook authenticates with the DestinE API, queries ECMWF Climate Digital Twin adaptation data based on ScenarioMIP parameters, downloads the selected forecast data using a robust retry mechanism, and visualizes it using EarthKit.\n\nECMWF\n\nAuthentication\n\nDigital Twin\n\nView Notebook\n\n\n\nHDA Extreme DT Parameter Plotter - Tutorial This notebook shows how to select, request, and download Climate Digital Twin Extremes data from the DestinE Data Lake (DEDL HDA), including user-defined parameter, scenario, date, and level selection, followed by secure authentication, API querying, and visualization using EarthKit.\n\nECMWF\n\nEarthkit\n\nDigital Twin\n\nView Notebook\n\n\n\nWeather-Induced Extremes Digital Twin Series his notebook authenticates with the DestinE API, queries ECMWF Extremes Digital Twin forecast data for a user-selected date within the last 14 days, downloads it, and visualizes wind gust and sea-level pressure fields using EarthKit.\n\nHDA\n\nECMWF\n\nAuthentication\n\nDigital Twin\n\nView Notebook\n\n\n\nAviso notification for DT data availability This notebook shows how to check the data availablility for the Weather-Induced Extremes Digital Twin (Extremes DT) using the ECMWF Aviso package.\n\npyaviso\n\nDigital Twin\n\nView Notebook\n\n\n\nClimate Change Adaptation Digital Twin Series This notebook authenticates a user with DestinE services, constructs and submits data requests to the DEDL HDA API for Climate Digital Twin projections, polls for availability, downloads GRIB data for multiple years, and visualizes it using EarthKit.\n\nHDA\n\nECMWF\n\nAuthentication\n\nDigital Twin\n\nView Notebook\n\n\n\nWeather-Induced Extremes - Data Access using DEDL HDA This notebook demonstrates how to access and download sea ice coverage data from the Weather-Induced Extremes Digital Twin using the DestinE Data Lake Harmonised Data Access (DEDL HDA) API, including authentication, filtering, polling, and visualizing the result on a map.\n\nECMWF\n\nEarthkit\n\nDigital Twin\n\nView Notebook\n\n\n\nInteractive Dashboard for ExtremeDT Weather Forecast Data with xcube This notebook guides users through accessing ExtremeDT weather data cubes, filtering them by region, converting units, and visualizing the results in an interactive dashboard using the xcube viewer.\n\nSTACK\n\nDataCube\n\nDigital Twin\n\nView Notebook\n\n\n\nExtremeDT Weather Data Cubes This notebook demonstrates how to access, explore, and visualize weather forecast data from the ExtremeDT data cubes using xarray and matplotlib, including spatial plots, time series analysis, and interactive dashboard preparation.\n\nSTACK\n\nDataCube\n\nDigital Twin\n\nView Notebook","type":"content","url":"/tag-digital-twin#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Earthkit"},"type":"lvl1","url":"/tag-earthkit","position":0},{"hierarchy":{"lvl1":"Notebooks Earthkit"},"content":"","type":"content","url":"/tag-earthkit","position":1},{"hierarchy":{"lvl1":"Notebooks Earthkit","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-earthkit#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Earthkit","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nHDA Climate DT Parameter Plotter - Tutorial This notebook provides an interactive workflow to select, query, download, and visualize Climate Digital Twin parameters from the DestinE Data Lake using the DEDL HDA API.\n\nHDA\n\nECMWF\n\nEarthkit\n\nDigital Twin\n\nView Notebook\n\n\n\nHDA Extreme DT Parameter Plotter - Tutorial This notebook shows how to select, request, and download Climate Digital Twin Extremes data from the DestinE Data Lake (DEDL HDA), including user-defined parameter, scenario, date, and level selection, followed by secure authentication, API querying, and visualization using EarthKit.\n\nECMWF\n\nEarthkit\n\nDigital Twin\n\nView Notebook\n\n\n\nWeather-Induced Extremes - Data Access using DEDL HDA This notebook demonstrates how to access and download sea ice coverage data from the Weather-Induced Extremes Digital Twin using the DestinE Data Lake Harmonised Data Access (DEDL HDA) API, including authentication, filtering, polling, and visualizing the result on a map.\n\nECMWF\n\nEarthkit\n\nDigital Twin\n\nView Notebook","type":"content","url":"/tag-earthkit#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks ECMWF"},"type":"lvl1","url":"/tag-ecmwf","position":0},{"hierarchy":{"lvl1":"Notebooks ECMWF"},"content":"","type":"content","url":"/tag-ecmwf","position":1},{"hierarchy":{"lvl1":"Notebooks ECMWF","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-ecmwf#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks ECMWF","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nERA5 hourly data on single levels from 1940 to present This notebook shows how to authenticate with the DestinE API, queries and downloads ERA5 single-level reanalysis data using the DEDL HDA service, and visualizes the result with EarthKit.\n\nHDA\n\nSTAC\n\nECMWF\n\nView Notebook\n\n\n\nHDA Climate DT Parameter Plotter - Tutorial This notebook provides an interactive workflow to select, query, download, and visualize Climate Digital Twin parameters from the DestinE Data Lake using the DEDL HDA API.\n\nHDA\n\nECMWF\n\nEarthkit\n\nDigital Twin\n\nView Notebook\n\n\n\nClimate Change Adaptation This notebook authenticates with the DestinE API, queries ECMWF Climate Digital Twin adaptation data based on ScenarioMIP parameters, downloads the selected forecast data using a robust retry mechanism, and visualizes it using EarthKit.\n\nECMWF\n\nAuthentication\n\nDigital Twin\n\nView Notebook\n\n\n\nHDA Extreme DT Parameter Plotter - Tutorial This notebook shows how to select, request, and download Climate Digital Twin Extremes data from the DestinE Data Lake (DEDL HDA), including user-defined parameter, scenario, date, and level selection, followed by secure authentication, API querying, and visualization using EarthKit.\n\nECMWF\n\nEarthkit\n\nDigital Twin\n\nView Notebook\n\n\n\nWeather-Induced Extremes Digital Twin Series his notebook authenticates with the DestinE API, queries ECMWF Extremes Digital Twin forecast data for a user-selected date within the last 14 days, downloads it, and visualizes wind gust and sea-level pressure fields using EarthKit.\n\nHDA\n\nECMWF\n\nAuthentication\n\nDigital Twin\n\nView Notebook\n\n\n\nClimate Change Adaptation Digital Twin Series This notebook authenticates a user with DestinE services, constructs and submits data requests to the DEDL HDA API for Climate Digital Twin projections, polls for availability, downloads GRIB data for multiple years, and visualizes it using EarthKit.\n\nHDA\n\nECMWF\n\nAuthentication\n\nDigital Twin\n\nView Notebook\n\n\n\nWeather-Induced Extremes - Data Access using DEDL HDA This notebook demonstrates how to access and download sea ice coverage data from the Weather-Induced Extremes Digital Twin using the DestinE Data Lake Harmonised Data Access (DEDL HDA) API, including authentication, filtering, polling, and visualizing the result on a map.\n\nECMWF\n\nEarthkit\n\nDigital Twin\n\nView Notebook","type":"content","url":"/tag-ecmwf#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks EODAG"},"type":"lvl1","url":"/tag-eodag","position":0},{"hierarchy":{"lvl1":"Notebooks EODAG"},"content":"","type":"content","url":"/tag-eodag","position":1},{"hierarchy":{"lvl1":"Notebooks EODAG","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-eodag#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks EODAG","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nEODAG - A quick start with DEDL This notebook provides a quickstart guide for using the EODAG Python API and CLI to search, discover, and download DEDL data.\n\nHDA\n\nEODAG\n\nView Notebook\n\n\n\nEODAG - DestinE Data Lake Provider This notebook demonstrates how to use the DEDL provider in EODAG.\n\nHDA\n\nEODAG\n\nView Notebook","type":"content","url":"/tag-eodag#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks GFM"},"type":"lvl1","url":"/tag-gfm","position":0},{"hierarchy":{"lvl1":"Notebooks GFM"},"content":"","type":"content","url":"/tag-gfm","position":1},{"hierarchy":{"lvl1":"Notebooks GFM","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-gfm#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks GFM","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nSTACK service - Python Client Dask This notebook demonstrates how to use the DEDL Stack Python client to authenticate, manage, and execute parallel, multi-cloud Dask computations on distributed datasets stored across Central Site and LUMI bridge.\n\nSTACK\n\nDask\n\nGFM\n\nView Notebook\n\n\n\nSTACK Service Dask This notebook introduces authentication and multi-cluster management using the DEDL Stack client with OIDC, enabling users to securely spawn, monitor, and scale Dask clusters across Central and LUMI locations within the DestinE Data Lake.\n\nSTACK\n\nDask\n\nGFM\n\nView Notebook","type":"content","url":"/tag-gfm#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks HDA"},"type":"lvl1","url":"/tag-hda","position":0},{"hierarchy":{"lvl1":"Notebooks HDA"},"content":"","type":"content","url":"/tag-hda","position":1},{"hierarchy":{"lvl1":"Notebooks HDA","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-hda#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks HDA","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nHigh rate SEVIRI Level 1.5 Image Data MSG - Data Access This notebook demonstrates how to access SEVIRI data using HDA and how to read, process and visualize it.\n\nHDA\n\nSEVIRI\n\nView Notebook\n\n\n\nOLCI Level 1B Reduced Resolution - Sentinel-3 This notebook demonstrates how to search and access Sentinel-3 data using HDA and how to read and visualize it using satpy.\n\nHDA\n\nOLCI\n\nSentinel-3\n\nsatpy\n\nView Notebook\n\n\n\nAVHRR Level 1B Metop Global - Data Access This notebook demonstrates how to search and access Metop data using HDA and how to read, process and visualize it using satpy.\n\nHDA\n\nMetop\n\nAVHRR\n\nsatpy\n\nView Notebook\n\n\n\nEODAG - A quick start with DEDL This notebook provides a quickstart guide for using the EODAG Python API and CLI to search, discover, and download DEDL data.\n\nHDA\n\nEODAG\n\nView Notebook\n\n\n\nEODAG - DestinE Data Lake Provider This notebook demonstrates how to use the DEDL provider in EODAG.\n\nHDA\n\nEODAG\n\nView Notebook\n\n\n\nERA5 hourly data on single levels from 1940 to present This notebook shows how to authenticate with the DestinE API, queries and downloads ERA5 single-level reanalysis data using the DEDL HDA service, and visualizes the result with EarthKit.\n\nHDA\n\nSTAC\n\nECMWF\n\nView Notebook\n\n\n\nHDA Tutorial - Queryables This notebook demonstrates how to use the queryables API to filter C3S and DestinE digital twin collections by leveraging variable terms that dynamically adjust based on user selections.\n\nHDA\n\nC3S\n\nDigital twin\n\nAuthentication\n\nView Notebook\n\n\n\nHDA Tutorial This notebook demonstrates the first steps using the Harmonised Data access API.\n\nHDA\n\nSTAC\n\nCore API\n\nView Notebook\n\n\n\nHDA Tutorial - Quick start This notebook demonstrates how to use the HDA (Harmonized Data Access) API by sending a few HTTP requests to the API, using Python code.\n\nHDA\n\nSTAC\n\nHTTP requests\n\nView Notebook\n\n\n\nUsing HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data This notebook demonstrates a simple example of how you can access data from DEDL using HDA and what you can do with it using an example with Sentinel-1 data.\n\nHDA\n\nSTAC\n\nSentinel-1\n\nThresholding techniques\n\nView Notebook\n\n\n\nHow to use HDA to find and download data for conducting monitoring of Śniadrwy lake This notebook demonstrates a simple example of how you can access data from DEDL using HDA and what you can do with it using an example with Sentinel-1 data.\n\nHDA\n\nSTAC\n\nSentinel-2\n\nsatpy\n\nView Notebook\n\n\n\nHDA Climate DT Parameter Plotter - Tutorial This notebook provides an interactive workflow to select, query, download, and visualize Climate Digital Twin parameters from the DestinE Data Lake using the DEDL HDA API.\n\nHDA\n\nECMWF\n\nEarthkit\n\nDigital Twin\n\nView Notebook\n\n\n\nWeather-Induced Extremes Digital Twin Series his notebook authenticates with the DestinE API, queries ECMWF Extremes Digital Twin forecast data for a user-selected date within the last 14 days, downloads it, and visualizes wind gust and sea-level pressure fields using EarthKit.\n\nHDA\n\nECMWF\n\nAuthentication\n\nDigital Twin\n\nView Notebook\n\n\n\nClimate Change Adaptation Digital Twin Series This notebook authenticates a user with DestinE services, constructs and submits data requests to the DEDL HDA API for Climate Digital Twin projections, polls for availability, downloads GRIB data for multiple years, and visualizes it using EarthKit.\n\nHDA\n\nECMWF\n\nAuthentication\n\nDigital Twin\n\nView Notebook","type":"content","url":"/tag-hda#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Hook"},"type":"lvl1","url":"/tag-hook","position":0},{"hierarchy":{"lvl1":"Notebooks Hook"},"content":"","type":"content","url":"/tag-hook","position":1},{"hierarchy":{"lvl1":"Notebooks Hook","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-hook#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Hook","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nHook - Perform data harvesting This Notebook demonstrates how to perform data harvesting with Hook.\n\nHook\n\nAuthentication\n\nToken\n\nView Notebook\n\n\n\nHook Tutorial - Data Harvest This notebook demonstrates how to use the Hook service.\n\nHook\n\nAuthentication\n\nWorkflow\n\nStorage\n\nView Notebook\n\n\n\nAccess to Hook services This Notebook demonstrates the retrieval of a token appropriate for interaction with the OnDemand Processing API (Hook API) and listing of available Hooks (Processors) using the retrieved token.\n\nHook\n\nAuthentication\n\nToken\n\nView Notebook","type":"content","url":"/tag-hook#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks HTTP requests"},"type":"lvl1","url":"/tag-http-requests","position":0},{"hierarchy":{"lvl1":"Notebooks HTTP requests"},"content":"","type":"content","url":"/tag-http-requests","position":1},{"hierarchy":{"lvl1":"Notebooks HTTP requests","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-http-requests#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks HTTP requests","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nHDA Tutorial - Quick start This notebook demonstrates how to use the HDA (Harmonized Data Access) API by sending a few HTTP requests to the API, using Python code.\n\nHDA\n\nSTAC\n\nHTTP requests\n\nView Notebook","type":"content","url":"/tag-http-requests#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Metop"},"type":"lvl1","url":"/tag-metop","position":0},{"hierarchy":{"lvl1":"Notebooks Metop"},"content":"","type":"content","url":"/tag-metop","position":1},{"hierarchy":{"lvl1":"Notebooks Metop","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-metop#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Metop","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nAVHRR Level 1B Metop Global - Data Access This notebook demonstrates how to search and access Metop data using HDA and how to read, process and visualize it using satpy.\n\nHDA\n\nMetop\n\nAVHRR\n\nsatpy\n\nView Notebook","type":"content","url":"/tag-metop#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks OLCI"},"type":"lvl1","url":"/tag-olci","position":0},{"hierarchy":{"lvl1":"Notebooks OLCI"},"content":"","type":"content","url":"/tag-olci","position":1},{"hierarchy":{"lvl1":"Notebooks OLCI","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-olci#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks OLCI","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nOLCI Level 1B Reduced Resolution - Sentinel-3 This notebook demonstrates how to search and access Sentinel-3 data using HDA and how to read and visualize it using satpy.\n\nHDA\n\nOLCI\n\nSentinel-3\n\nsatpy\n\nView Notebook","type":"content","url":"/tag-olci#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks pyaviso"},"type":"lvl1","url":"/tag-pyaviso","position":0},{"hierarchy":{"lvl1":"Notebooks pyaviso"},"content":"","type":"content","url":"/tag-pyaviso","position":1},{"hierarchy":{"lvl1":"Notebooks pyaviso","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-pyaviso#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks pyaviso","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nAviso notification for DT data availability This notebook shows how to check the data availablility for the Weather-Induced Extremes Digital Twin (Extremes DT) using the ECMWF Aviso package.\n\npyaviso\n\nDigital Twin\n\nView Notebook","type":"content","url":"/tag-pyaviso#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks satpy"},"type":"lvl1","url":"/tag-satpy","position":0},{"hierarchy":{"lvl1":"Notebooks satpy"},"content":"","type":"content","url":"/tag-satpy","position":1},{"hierarchy":{"lvl1":"Notebooks satpy","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-satpy#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks satpy","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nOLCI Level 1B Reduced Resolution - Sentinel-3 This notebook demonstrates how to search and access Sentinel-3 data using HDA and how to read and visualize it using satpy.\n\nHDA\n\nOLCI\n\nSentinel-3\n\nsatpy\n\nView Notebook\n\n\n\nAVHRR Level 1B Metop Global - Data Access This notebook demonstrates how to search and access Metop data using HDA and how to read, process and visualize it using satpy.\n\nHDA\n\nMetop\n\nAVHRR\n\nsatpy\n\nView Notebook\n\n\n\nHow to use HDA to find and download data for conducting monitoring of Śniadrwy lake This notebook demonstrates a simple example of how you can access data from DEDL using HDA and what you can do with it using an example with Sentinel-1 data.\n\nHDA\n\nSTAC\n\nSentinel-2\n\nsatpy\n\nView Notebook","type":"content","url":"/tag-satpy#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Sentinel-1"},"type":"lvl1","url":"/tag-sentinel-1","position":0},{"hierarchy":{"lvl1":"Notebooks Sentinel-1"},"content":"","type":"content","url":"/tag-sentinel-1","position":1},{"hierarchy":{"lvl1":"Notebooks Sentinel-1","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-sentinel-1#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Sentinel-1","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nUsing HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data This notebook demonstrates a simple example of how you can access data from DEDL using HDA and what you can do with it using an example with Sentinel-1 data.\n\nHDA\n\nSTAC\n\nSentinel-1\n\nThresholding techniques\n\nView Notebook","type":"content","url":"/tag-sentinel-1#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Sentinel-2"},"type":"lvl1","url":"/tag-sentinel-2","position":0},{"hierarchy":{"lvl1":"Notebooks Sentinel-2"},"content":"","type":"content","url":"/tag-sentinel-2","position":1},{"hierarchy":{"lvl1":"Notebooks Sentinel-2","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-sentinel-2#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Sentinel-2","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nHow to use HDA to find and download data for conducting monitoring of Śniadrwy lake This notebook demonstrates a simple example of how you can access data from DEDL using HDA and what you can do with it using an example with Sentinel-1 data.\n\nHDA\n\nSTAC\n\nSentinel-2\n\nsatpy\n\nView Notebook","type":"content","url":"/tag-sentinel-2#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Sentinel-3"},"type":"lvl1","url":"/tag-sentinel-3","position":0},{"hierarchy":{"lvl1":"Notebooks Sentinel-3"},"content":"","type":"content","url":"/tag-sentinel-3","position":1},{"hierarchy":{"lvl1":"Notebooks Sentinel-3","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-sentinel-3#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Sentinel-3","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nOLCI Level 1B Reduced Resolution - Sentinel-3 This notebook demonstrates how to search and access Sentinel-3 data using HDA and how to read and visualize it using satpy.\n\nHDA\n\nOLCI\n\nSentinel-3\n\nsatpy\n\nView Notebook","type":"content","url":"/tag-sentinel-3#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks SEVIRI"},"type":"lvl1","url":"/tag-seviri","position":0},{"hierarchy":{"lvl1":"Notebooks SEVIRI"},"content":"","type":"content","url":"/tag-seviri","position":1},{"hierarchy":{"lvl1":"Notebooks SEVIRI","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-seviri#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks SEVIRI","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nHigh rate SEVIRI Level 1.5 Image Data MSG - Data Access This notebook demonstrates how to access SEVIRI data using HDA and how to read, process and visualize it.\n\nHDA\n\nSEVIRI\n\nView Notebook","type":"content","url":"/tag-seviri#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks STAC"},"type":"lvl1","url":"/tag-stac","position":0},{"hierarchy":{"lvl1":"Notebooks STAC"},"content":"","type":"content","url":"/tag-stac","position":1},{"hierarchy":{"lvl1":"Notebooks STAC","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-stac#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks STAC","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nERA5 hourly data on single levels from 1940 to present This notebook shows how to authenticate with the DestinE API, queries and downloads ERA5 single-level reanalysis data using the DEDL HDA service, and visualizes the result with EarthKit.\n\nHDA\n\nSTAC\n\nECMWF\n\nView Notebook\n\n\n\nHDA Tutorial This notebook demonstrates the first steps using the Harmonised Data access API.\n\nHDA\n\nSTAC\n\nCore API\n\nView Notebook\n\n\n\nHDA Tutorial - Quick start This notebook demonstrates how to use the HDA (Harmonized Data Access) API by sending a few HTTP requests to the API, using Python code.\n\nHDA\n\nSTAC\n\nHTTP requests\n\nView Notebook\n\n\n\nUsing HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data In this notebook, we will present a simple example of how you can access data from DEDL using HDA and what you can do with it. We will demonstrate how to utilize thresholding techniques and compare the values of VV and VH polarizations to analyze urban areas. As an illustration, we will attempt to download Sentinel-1 images containing data of the urban area of Warsaw (Poland).\n\nSTAC\n\nAuthentication\n\nAccess Token\n\nView Notebook\n\n\n\nHow to use HDA to find and download data for conducting monitoring of Śniadrwy lake In this notebook, we will present a simple example on how you can access data from DEDL using HDA and what you can do with it. As an example, we will try to download Sentinel-2 images containining data of Śniadrwy lake from first week of July 2023. With usage of HDA and few Python packages, you will be able to obtain rasters with NDWI index.\n\nSTAC\n\nAuthentication\n\nAccess Token\n\nView Notebook\n\n\n\nUsing HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data This notebook demonstrates a simple example of how you can access data from DEDL using HDA and what you can do with it using an example with Sentinel-1 data.\n\nHDA\n\nSTAC\n\nSentinel-1\n\nThresholding techniques\n\nView Notebook\n\n\n\nHow to use HDA to find and download data for conducting monitoring of Śniadrwy lake This notebook demonstrates a simple example of how you can access data from DEDL using HDA and what you can do with it using an example with Sentinel-1 data.\n\nHDA\n\nSTAC\n\nSentinel-2\n\nsatpy\n\nView Notebook\n\n\n\nHDA PySTAC-Client Introduction This notebook shows the basic use of DestinE Data Lake Harmonised Data Access using pystac-client.\n\nSTAC\n\nAuthentication\n\nAccess Token\n\nView Notebook","type":"content","url":"/tag-stac#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks STACK"},"type":"lvl1","url":"/tag-stack","position":0},{"hierarchy":{"lvl1":"Notebooks STACK"},"content":"","type":"content","url":"/tag-stack","position":1},{"hierarchy":{"lvl1":"Notebooks STACK","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-stack#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks STACK","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nSTACK service - Dask 101 This notebook introduces Dask's core APIs and demonstrates how to use them for scalable, parallel, and distributed data processing, culminating in deploying and interacting with a Dask cluster on the DestinE Data Lake STACK service.\n\nSTACK\n\nDask\n\nCluster\n\nView Notebook\n\n\n\nInteractive Dashboard for ExtremeDT Weather Forecast Data with xcube This notebook guides users through accessing ExtremeDT weather data cubes, filtering them by region, converting units, and visualizing the results in an interactive dashboard using the xcube viewer.\n\nSTACK\n\nDataCube\n\nDigital Twin\n\nView Notebook\n\n\n\nExtremeDT Weather Data Cubes This notebook demonstrates how to access, explore, and visualize weather forecast data from the ExtremeDT data cubes using xarray and matplotlib, including spatial plots, time series analysis, and interactive dashboard preparation.\n\nSTACK\n\nDataCube\n\nDigital Twin\n\nView Notebook\n\n\n\nSTACK service - Python Client Dask This notebook demonstrates how to use the DEDL Stack Python client to authenticate, manage, and execute parallel, multi-cloud Dask computations on distributed datasets stored across Central Site and LUMI bridge.\n\nSTACK\n\nDask\n\nGFM\n\nView Notebook\n\n\n\nSTACK Service Dask This notebook introduces authentication and multi-cluster management using the DEDL Stack client with OIDC, enabling users to securely spawn, monitor, and scale Dask clusters across Central and LUMI locations within the DestinE Data Lake.\n\nSTACK\n\nDask\n\nGFM\n\nView Notebook","type":"content","url":"/tag-stack#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Storage"},"type":"lvl1","url":"/tag-storage","position":0},{"hierarchy":{"lvl1":"Notebooks Storage"},"content":"","type":"content","url":"/tag-storage","position":1},{"hierarchy":{"lvl1":"Notebooks Storage","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-storage#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Storage","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nHook Tutorial - Data Harvest This notebook demonstrates how to use the Hook service.\n\nHook\n\nAuthentication\n\nWorkflow\n\nStorage\n\nView Notebook","type":"content","url":"/tag-storage#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Template"},"type":"lvl1","url":"/tag-template","position":0},{"hierarchy":{"lvl1":"Notebooks Template"},"content":"","type":"content","url":"/tag-template","position":1},{"hierarchy":{"lvl1":"Notebooks Template","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-template#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Template","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nTemplate 2 Brief description of the notebook.\n\nTemplate\n\nView Notebook\n\n\n\nTemplate 1 Brief description of the notebook.\n\nTemplate\n\nView Notebook","type":"content","url":"/tag-template#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Thresholding techniques"},"type":"lvl1","url":"/tag-thresholding-techniques","position":0},{"hierarchy":{"lvl1":"Notebooks Thresholding techniques"},"content":"","type":"content","url":"/tag-thresholding-techniques","position":1},{"hierarchy":{"lvl1":"Notebooks Thresholding techniques","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-thresholding-techniques#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Thresholding techniques","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nUsing HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data This notebook demonstrates a simple example of how you can access data from DEDL using HDA and what you can do with it using an example with Sentinel-1 data.\n\nHDA\n\nSTAC\n\nSentinel-1\n\nThresholding techniques\n\nView Notebook","type":"content","url":"/tag-thresholding-techniques#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Token"},"type":"lvl1","url":"/tag-token","position":0},{"hierarchy":{"lvl1":"Notebooks Token"},"content":"","type":"content","url":"/tag-token","position":1},{"hierarchy":{"lvl1":"Notebooks Token","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-token#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Token","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nHook - Perform data harvesting This Notebook demonstrates how to perform data harvesting with Hook.\n\nHook\n\nAuthentication\n\nToken\n\nView Notebook\n\n\n\nAccess to Hook services This Notebook demonstrates the retrieval of a token appropriate for interaction with the OnDemand Processing API (Hook API) and listing of available Hooks (Processors) using the retrieved token.\n\nHook\n\nAuthentication\n\nToken\n\nView Notebook","type":"content","url":"/tag-token#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"Notebooks Workflow"},"type":"lvl1","url":"/tag-workflow","position":0},{"hierarchy":{"lvl1":"Notebooks Workflow"},"content":"","type":"content","url":"/tag-workflow","position":1},{"hierarchy":{"lvl1":"Notebooks Workflow","lvl2":"Filter Notebooks by Tags"},"type":"lvl2","url":"/tag-workflow#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"Notebooks Workflow","lvl2":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow\n\n\n\nHook Tutorial - Data Harvest This notebook demonstrates how to use the Hook service.\n\nHook\n\nAuthentication\n\nWorkflow\n\nStorage\n\nView Notebook","type":"content","url":"/tag-workflow#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"DestinE Data Lake Gallery"},"type":"lvl1","url":"/","position":0},{"hierarchy":{"lvl1":"DestinE Data Lake Gallery"},"content":"Welcome to the Destination Earth Data Lake Notebook Gallery! \nExplore our collection of interactive Jupyter Notebooks, designed to help you work with the wide range of services offered by the DestinE Data Lake. Each notebook provides hands-on examples and practical guidance that you can adapt for your own projects.","type":"content","url":"/","position":1},{"hierarchy":{"lvl1":"DestinE Data Lake Gallery","lvl3":"Filter Notebooks by Tags"},"type":"lvl3","url":"/#filter-notebooks-by-tags","position":2},{"hierarchy":{"lvl1":"DestinE Data Lake Gallery","lvl3":"Filter Notebooks by Tags"},"content":"Access Token\n\n\nAuthentication\n\n\nAvhrr\n\n\nC3S\n\n\nCluster\n\n\nCore Api\n\n\nDask\n\n\nDatacube\n\n\nDigital Twin\n\n\nEarthkit\n\n\nEcmwf\n\n\nEodag\n\n\nGfm\n\n\nHda\n\n\nHook\n\n\nHttp Requests\n\n\nMetop\n\n\nOlci\n\n\nPyaviso\n\n\nSatpy\n\n\nSentinel 1\n\n\nSentinel 2\n\n\nSentinel 3\n\n\nSeviri\n\n\nStac\n\n\nStack\n\n\nStorage\n\n\nTemplate\n\n\nThresholding Techniques\n\n\nToken\n\n\nWorkflow","type":"content","url":"/#filter-notebooks-by-tags","position":3},{"hierarchy":{"lvl1":"DestinE Data Lake Gallery","lvl2":"Services Overview"},"type":"lvl2","url":"/#services-overview","position":4},{"hierarchy":{"lvl1":"DestinE Data Lake Gallery","lvl2":"Services Overview"},"content":"Harmonised Data Access (HDA)  – Jupyter notebook examples + Python tools\n\nSTACK service – Use DASK for near-data processing\n\nHOOK service – Create and manage workflows via HOOK","type":"content","url":"/#services-overview","position":5},{"hierarchy":{"lvl1":"DestinE Data Lake Gallery","lvl2":"How to Use Notebooks on the DestinE Platform"},"type":"lvl2","url":"/#how-to-use-notebooks-on-the-destine-platform","position":6},{"hierarchy":{"lvl1":"DestinE Data Lake Gallery","lvl2":"How to Use Notebooks on the DestinE Platform"},"content":"If you’re using the DestinE JupyterLab environment, please follow these steps:# 1. Create a virtual environment\npython -m venv /home/jovyan/my_env\n\n# 2. Activate the environment\nsource /home/jovyan/my_env/bin/activate\n\n# 3. Install dependencies\npip install -r /home/jovyan/datalake-lab/requirements.txt\n\n# 4. Install the kernel\npython -m ipykernel install --name my_env --user\n\nThen select my_env from the Jupyter kernel menu.","type":"content","url":"/#how-to-use-notebooks-on-the-destine-platform","position":7},{"hierarchy":{"lvl1":"DestinE Data Lake Gallery","lvl2":"Want to Contribute?"},"type":"lvl2","url":"/#want-to-contribute","position":8},{"hierarchy":{"lvl1":"DestinE Data Lake Gallery","lvl2":"Want to Contribute?"},"content":"Do you have a useful notebooks to share? Have a look at the \n\nContributing Guide!","type":"content","url":"/#want-to-contribute","position":9},{"hierarchy":{"lvl1":"DestinE Data Lake Gallery","lvl2":"More Information"},"type":"lvl2","url":"/#more-information","position":10},{"hierarchy":{"lvl1":"DestinE Data Lake Gallery","lvl2":"More Information"},"content":"DestinE Documentation\n\nDestinE Data Portfolio\n\nAccess the API (Swagger)","type":"content","url":"/#more-information","position":11},{"hierarchy":{"lvl1":"ERA5 hourly data on single levels from 1940 to present"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level","position":0},{"hierarchy":{"lvl1":"ERA5 hourly data on single levels from 1940 to present"},"content":"🚀 Launch in JupyterHub\n\nCredit: Earthkit and HDA Polytope used in this context are both packages provided by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\nDEDL Harmonised Data Access is used in this example.\n\nDocumentation DestinE Data Lake HDA\n\nDocumentation ERA5\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level","position":1},{"hierarchy":{"lvl1":"ERA5 hourly data on single levels from 1940 to present","lvl2":"Obtain Authentication Token"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#obtain-authentication-token","position":2},{"hierarchy":{"lvl1":"ERA5 hourly data on single levels from 1940 to present","lvl2":"Obtain Authentication Token"},"content":"\n\nimport requests\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\n\nFirst, we get an access token for the API\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#obtain-authentication-token","position":3},{"hierarchy":{"lvl1":"ERA5 hourly data on single levels from 1940 to present","lvl2":"Query using the DEDL HDA API"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#query-using-the-dedl-hda-api","position":4},{"hierarchy":{"lvl1":"ERA5 hourly data on single levels from 1940 to present","lvl2":"Query using the DEDL HDA API"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#query-using-the-dedl-hda-api","position":5},{"hierarchy":{"lvl1":"ERA5 hourly data on single levels from 1940 to present","lvl3":"Filter","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#filter","position":6},{"hierarchy":{"lvl1":"ERA5 hourly data on single levels from 1940 to present","lvl3":"Filter","lvl2":"Query using the DEDL HDA API"},"content":"We have to setup up a filter and define which data to obtain.\n\nFilter Options HDA\n\ndatechoice = \"2020-06-10T10:00:00Z\"\nfilters = {\n    key: {\"eq\": value}\n    for key, value in {\n        \"format\": \"grib\",\n        \"variable\": \"2m_temperature\",\n        \"time\": \"12:00\"\n    }.items()\n}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#filter","position":7},{"hierarchy":{"lvl1":"ERA5 hourly data on single levels from 1940 to present","lvl3":"Make Data Request","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#make-data-request","position":8},{"hierarchy":{"lvl1":"ERA5 hourly data on single levels from 1940 to present","lvl3":"Make Data Request","lvl2":"Query using the DEDL HDA API"},"content":"\n\nresponse = requests.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n \"collections\": [\"EO.ECMWF.DAT.REANALYSIS_ERA5_SINGLE_LEVELS\"],\n    \"datetime\": datechoice,\n    \"query\": filters\n})\n\nif(response.status_code!= 200):\n    (print(response.text))\n# Requests to EO.ECMWF.DAT.REANALYSIS_ERA5_SINGLE_LEVELS always return a single item containing all the requested data\n#print(response.json())\nproduct = response.json()[\"features\"][0]\n#product id\nproduct[\"id\"]\n\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#make-data-request","position":9},{"hierarchy":{"lvl1":"ERA5 hourly data on single levels from 1940 to present","lvl3":"Once our product found, we download the data.","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#once-our-product-found-we-download-the-data","position":10},{"hierarchy":{"lvl1":"ERA5 hourly data on single levels from 1940 to present","lvl3":"Once our product found, we download the data.","lvl2":"Query using the DEDL HDA API"},"content":"\n\nfrom IPython.display import JSON\n\n\n# DownloadLink is an asset representing the whole product\ndownload_url = product[\"assets\"][\"downloadLink\"][\"href\"]\nHTTP_SUCCESS_CODE = 200\nHTTP_ACCEPTED_CODE = 202\n\ndirect_download_url=''\n\nresponse = requests.get(download_url, headers=auth_headers)\nif (response.status_code == HTTP_SUCCESS_CODE):\n    direct_download_url = product['assets']['downloadLink']['href']\nelif (response.status_code != HTTP_ACCEPTED_CODE):\n    print(response.text)\nprint(download_url)\nresponse.raise_for_status()\n\nIf the data is already available in the cache we can directly download it.\nIf the data is not available, we can see that our request is in queuedstatus.We will then poll the API until the data is ready and then download it.\n\nPlease note that the basic HDA quota allows a maximum of 4 requests per second. The following code limits polling to this quota.\n\npip install ratelimit --quiet\n\nfrom tqdm import tqdm\nimport time\nimport re\nfrom ratelimit import limits, sleep_and_retry\n\n# Set limit: max 4 calls per 1 second\nCALLS = 4\nPERIOD = 1  # seconds\n\n@sleep_and_retry\n@limits(calls=CALLS, period=PERIOD)\ndef call_api(url,auth_headers):\n    response = requests.get(url, headers=auth_headers, stream=True)\n    return response\n\n# we poll as long as the data is not ready\nif direct_download_url=='':\n    while url := response.headers.get(\"Location\"):\n        print(f\"order status: {response.json()['status']}\")\n        response = call_api(url,auth_headers)\n\nif (response.status_code not in (HTTP_SUCCESS_CODE,HTTP_ACCEPTED_CODE)):\n     (print(response.text))        \nresponse.raise_for_status()        \n\nfilename = re.findall('filename=\\\"?(.+)\\\"?', response.headers[\"Content-Disposition\"])[0]\ntotal_size = int(response.headers.get(\"content-length\", 0))\n\nprint(f\"downloading {filename}\")\n\nwith tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n    with open(filename, 'wb') as f:\n        for data in response.iter_content(1024):\n            progress_bar.update(len(data))\n            f.write(data)\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#once-our-product-found-we-download-the-data","position":11},{"hierarchy":{"lvl1":"ERA5 hourly data on single levels from 1940 to present","lvl2":"EarthKit"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#earthkit","position":12},{"hierarchy":{"lvl1":"ERA5 hourly data on single levels from 1940 to present","lvl2":"EarthKit"},"content":"Lets plot the result file\n[EarthKit Documentation] \n\nhttps://​earthkit​-data​.readthedocs​.io​/en​/latest​/index​.html\n\nThis section requires that you have ecCodes >= 2.35 installed on your system.You can follow the installation procedure at \n\nhttps://​confluence​.ecmwf​.int​/display​/ECC​/ecCodes+installation\n\nimport earthkit.data\nimport earthkit.maps\nimport earthkit.regrid\n\ndata = earthkit.data.from_source(\"file\", filename)\ndata.ls\nearthkit.maps.quickplot(data,#style=style\n                       )","type":"content","url":"/dedl-hda-eo-ecmwf-dat-reanalysis-era5-single-level#earthkit","position":13},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial"},"type":"lvl1","url":"/climatedt-parameterplotter","position":0},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial"},"content":"🚀 Launch in JupyterHub\n\nCredit: Earthkit and HDA Polytope used in this context are both packages provided by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\nDEDL Harmonised Data Access is used in this example to access and plot Climate DT parameter.\n\nDocumentation DestinE DataLake HDA\n\nDocumentation Digital Twin - Parameter Usage\n\n","type":"content","url":"/climatedt-parameterplotter","position":1},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Import the required packages"},"type":"lvl2","url":"/climatedt-parameterplotter#import-the-required-packages","position":2},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Import the required packages"},"content":"\n\nImport the Climate DT parameter & scenario dictionary\n\nfrom destinelab import climate_dt_dictionary\nimport ipywidgets as widgets\nimport json\nimport datetime\n\nimport importlib.metadata\n\n","type":"content","url":"/climatedt-parameterplotter#import-the-required-packages","position":3},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Climate DT parameter selection (we limit the plotting to one parameter)"},"type":"lvl2","url":"/climatedt-parameterplotter#climate-dt-parameter-selection-we-limit-the-plotting-to-one-parameter","position":4},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Climate DT parameter selection (we limit the plotting to one parameter)"},"content":"\n\n# Create search box\nsearch_box = widgets.Text(placeholder='Search by parameter name', description='Search:', disabled=False)\n\n# Create dropdown to select entry\nentry_dropdown = widgets.Dropdown(\n    options=[(entry['paramName'], i) for i, entry in enumerate(climate_dt_dictionary.climateDT_params)],\n    description='Select Entry:'\n)\n\ndef filter_entries(search_string):\n    return [(entry['paramName'], i) for i, entry in enumerate(climate_dt_dictionary.climateDT_params) if search_string.lower() in entry['paramName'].lower()]\n\ndef on_search_change(change):\n    search_string = change.new\n    if search_string:\n        filtered_options = filter_entries(search_string)\n        entry_dropdown.options = filtered_options\n    else:\n        entry_dropdown.options = [(entry['paramName'], i) for i, entry in enumerate(climate_dt_dictionary.climateDT_params)]\n\nsearch_box.observe(on_search_change, names='value')\n\n# Display widgets\ndisplay(search_box, entry_dropdown)\n\ndef get_selected_entry():\n    return entry_dropdown.value\n\n\n# Print the details of the parameter (Polytope convention):\nselected_index = get_selected_entry()\nselected_entry = climate_dt_dictionary.climateDT_params[selected_index]\nprint(json.dumps(selected_entry,indent=4))\n\n","type":"content","url":"/climatedt-parameterplotter#climate-dt-parameter-selection-we-limit-the-plotting-to-one-parameter","position":5},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Choose now the Scenario from which we want to obtain the Climate Parameter"},"type":"lvl2","url":"/climatedt-parameterplotter#choose-now-the-scenario-from-which-we-want-to-obtain-the-climate-parameter","position":6},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Choose now the Scenario from which we want to obtain the Climate Parameter"},"content":"\n\n# Create dropdown to select scenario\nscenario_dropdown = widgets.Dropdown(\n    options=[(f\"{entry['experiment']} - {entry['model']} - {resolution}\", (i, resolution)) for i, entry in enumerate(climate_dt_dictionary.climateDT_scenario) for resolution in entry['resolution']],\n    description='Scenario:'\n)\n\n# Function to generate hourly slots\ndef generate_hourly_slots():\n    hours = []\n    for hour in range(0, 24):\n        for minute in range(0, 60, 60):  # Step by 60 minutes (1 hour)\n            hours.append(datetime.time(hour, minute))\n    return hours\n\n# Create dropdown to select hour\nhourly_slots = generate_hourly_slots()\nhour_dropdown = widgets.Dropdown(options=[(str(slot), slot) for slot in hourly_slots], description='Select Hour:', disabled=False)\n\n# Create date picker widgets\nstart_date_picker = widgets.DatePicker(description='Start Date:', disabled=False)\n\ndef on_scenario_change(change):\n    selected_index, selected_resolution = change.new\n    selected_sc_entry = climate_dt_dictionary.climateDT_scenario[selected_index]\n    date_from = datetime.datetime.strptime(selected_sc_entry['dateFrom'], '%m/%d/%Y').date()\n    start_date_picker.max = None\n    start_date_picker.min = date_from\n    start_date_picker.max = datetime.datetime.strptime(selected_sc_entry['dateTo'], '%m/%d/%Y').date()\n    start_date_picker.value = date_from\n\nscenario_dropdown.observe(on_scenario_change, names='value')\n\n# Set initial values directly\nselected_sc_entry = climate_dt_dictionary.climateDT_scenario[0]\n# Convert dateFrom string to date object\ndate_from = datetime.datetime.strptime(selected_sc_entry['dateFrom'], '%m/%d/%Y').date()\n\n# Set initial values directly\nstart_date_picker.min = date_from\nstart_date_picker.max = datetime.datetime.strptime(selected_sc_entry['dateTo'], '%m/%d/%Y').date()\nstart_date_picker.value = date_from\n\n# Display widgets\nif selected_entry[\"time\"] == \"Hourly\":\n    display(scenario_dropdown, start_date_picker, hour_dropdown)\nelse:\n    display(scenario_dropdown, start_date_picker)\n\ndef get_selected_values():\n    selected_scenario_index, selected_resolution = scenario_dropdown.value\n    selected_scenario = climate_dt_dictionary.climateDT_scenario[selected_scenario_index]\n    selected_start_date = start_date_picker.value\n    selected_end_date = \"\" # end_date_picker.value\n    selected_hour = \"00:00:00\"\n    if selected_entry[\"time\"] == \"Hourly\":\n        selected_hour = hour_dropdown.value\n        \n    return selected_scenario_index, selected_scenario, selected_resolution, selected_start_date, selected_end_date, selected_hour\n\n# Example usage:\nselected_scenario_index, selected_scenario, selected_resolution, selected_start_date, selected_end_date, selected_hour = get_selected_values()\n\n\nHandle different Levels to be selected (if any)\n\n# Define a global variable\nglobal global_widget\nglobal_widget = None\n\nif selected_entry[\"levelist\"] != \"\":\n    # Convert levelist string to list of integers\n    levelist = list(map(int, selected_entry[\"levelist\"].split('/')))\n    if(selected_scenario['model']=='IFS-NEMO'):\n        levelist = levelist + [73,74,75]\n\n      \n    # Create a function to generate the widget based on the selection mode\n    def generate_widget(selection_mode):\n        global global_widget\n        if selection_mode == 'Single':\n            global_widget = widgets.Dropdown(options=levelist, description='Select level:')\n            return global_widget\n        elif selection_mode == 'Multiple':\n            global_widget = widgets.SelectMultiple(options=levelist, description='Select levels:')\n            return global_widget\n\n    # Create a dropdown widget to choose selection mode\n    selection_mode_dropdown = widgets.Dropdown(options=['Single', 'Multiple'], description='Selection Mode:')\n\n    # Create an output widget to display the selected option(s)\n    output = widgets.Output()\n\n    # Function to display the widget based on the selection mode\n    def display_widget(selection_mode):\n        output.clear_output()\n        with output:\n            display(generate_widget(selection_mode))\n\n    # Define a function to handle the change in selection mode\n    def on_dropdown_change(change):\n        display_widget(change.new)\n\n    # Register the function to handle dropdown changes\n    selection_mode_dropdown.observe(on_dropdown_change, names='value')\n\n    # Display the widgets\n    display(selection_mode_dropdown, output)\n\n    # Display the initial widget based on default selection mode\n    display_widget('Single')\n\n# Function to convert tuple or single integer to string separated by \"/\"\ndef convert_to_string(input):\n    if isinstance(input, tuple):\n        return '/'.join(map(str, input))\n    elif isinstance(input, int):\n        return str(input)\n    else:\n        return None  # Handle other types if needed\n\nlevlInput = \"\"\nif global_widget != None:\n    # Test cases\n    levlInput = convert_to_string(global_widget.value)\n\n\nhourchoice4 = '{shour}00'.format(shour = str(get_selected_values()[5]).split(\":\")[0])\n\nfilter_params = {\n  \"class\": \"d1\",             # fixed \n  \"dataset\": \"climate-dt\",   # fixed climate-dt access\n  \"activity\" : get_selected_values()[1][\"activity\"],\n  \"experiment\" : get_selected_values()[1][\"experiment\"].upper(),\n  \"model\": get_selected_values()[1][\"model\"],\n  \"generation\": \"1\",         # fixed Specifies the generation of the dataset, which can be incremented as required (latest is 1)\n  \"realization\": \"1\",        # fixed Specifies the climate realization. Default 1. Based on perturbations of initial conditions\n  \"resolution\": get_selected_values()[2],      # standard/ high \n  \"expver\": \"0001\",          # fixed experiment version \n  \"stream\": selected_entry[\"stream\"],\n  \"time\": hourchoice4,            # choose the hourly slot(s)\n  \"type\": \"fc\",              # fixed forecasted fields\n  \"levtype\": selected_entry[\"levtype\"],  \n  \"levelist\": str(levlInput),  \n  \"param\": str(selected_entry[\"param\"]),  \n}\n\n# Print the result in JSON format\ndatechoice = \"{fname}T{shour}Z\".format(fname = get_selected_values()[3], shour = get_selected_values()[5] )\nprint(\"datechoice = \", datechoice)\nprint(json.dumps(filter_params, indent=4))\n\n\n","type":"content","url":"/climatedt-parameterplotter#choose-now-the-scenario-from-which-we-want-to-obtain-the-climate-parameter","position":7},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Obtain Authentication Token"},"type":"lvl2","url":"/climatedt-parameterplotter#obtain-authentication-token","position":8},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Obtain Authentication Token"},"content":"\n\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\n\nDESP_USERNAME = input(\"Please input your DESP username: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/climatedt-parameterplotter#obtain-authentication-token","position":9},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl3":"Check if DT access is granted","lvl2":"Obtain Authentication Token"},"type":"lvl3","url":"/climatedt-parameterplotter#check-if-dt-access-is-granted","position":10},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl3":"Check if DT access is granted","lvl2":"Obtain Authentication Token"},"content":"If DT access is not granted, you will not be able to execute the rest of the notebook.\n\nimport importlib\ninstalled_version = importlib.metadata.version(\"destinelab\")\nversion_number = installed_version.split('.')[1]\nif((int(version_number) >= 8 and float(installed_version) < 1) or float(installed_version) >= 1):\n    auth.is_DTaccess_allowed(access_token)\n\n","type":"content","url":"/climatedt-parameterplotter#check-if-dt-access-is-granted","position":11},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Query using the DEDL HDA API"},"type":"lvl2","url":"/climatedt-parameterplotter#query-using-the-dedl-hda-api","position":12},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Query using the DEDL HDA API"},"content":"\n\n","type":"content","url":"/climatedt-parameterplotter#query-using-the-dedl-hda-api","position":13},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl3":"Filter","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/climatedt-parameterplotter#filter","position":14},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl3":"Filter","lvl2":"Query using the DEDL HDA API"},"content":"We have to setup up a filter and define which data to obtain.\n\n# Check if levelist is empty and remove it\nif filter_params.get(\"levelist\") == \"\":\n    del filter_params[\"levelist\"]\n\nif selected_entry[\"time\"] == \"Daily\":\n    del filter_params[\"time\"]\n\n    \nhdaFilters = {\n    key: {\"eq\": value}\n    for key, value in filter_params.items()\n}\n\n#print(hdaFilters)\n\n","type":"content","url":"/climatedt-parameterplotter#filter","position":15},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl3":"Make Data Request","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/climatedt-parameterplotter#make-data-request","position":16},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl3":"Make Data Request","lvl2":"Query using the DEDL HDA API"},"content":"\n\n#Sometimes requests to polytope get timeouts, it is then convenient define a retry strategy\nretry_strategy = Retry(\n    total=5,  # Total number of retries\n    status_forcelist=[500, 502, 503, 504],  # List of 5xx status codes to retry on\n    allowed_methods=[\"GET\",'POST'],  # Methods to retry\n    backoff_factor=1  # Wait time between retries (exponential backoff)\n)\n\n# Create an adapter with the retry strategy\nadapter = HTTPAdapter(max_retries=retry_strategy)\n\n# Create a session and mount the adapter\nsession = requests.Session()\nsession.mount(\"https://\", adapter)\n\nresponse = session.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n \"collections\": [\"EO.ECMWF.DAT.DT_CLIMATE_ADAPTATION\"],\n    \"datetime\": datechoice,\n    \"query\": hdaFilters\n})\n\nif(response.status_code!= 200):\n    (print(response.text))\nresponse.raise_for_status()\n# Requests to EO.ECMWF.DAT.DT_CLIMATE always return a single item containing all the requested data\nproduct = response.json()[\"features\"][0]\nproduct[\"id\"]\n\n","type":"content","url":"/climatedt-parameterplotter#make-data-request","position":17},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl3":"Submission worked ? Once our product found, we download the data.","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/climatedt-parameterplotter#submission-worked-once-our-product-found-we-download-the-data","position":18},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl3":"Submission worked ? Once our product found, we download the data.","lvl2":"Query using the DEDL HDA API"},"content":"\n\nfrom IPython.display import JSON\n\n# DownloadLink is an asset representing the whole product\ndownload_url = product[\"assets\"][\"downloadLink\"][\"href\"]\nHTTP_SUCCESS_CODE = 200\nHTTP_ACCEPTED_CODE = 202\n\ndirect_download_url=''\n\nresponse = session.get(download_url, headers=auth_headers)\nif (response.status_code == HTTP_SUCCESS_CODE):\n    direct_download_url = product['assets']['downloadLink']['href']\nelif (response.status_code != HTTP_ACCEPTED_CODE):\n    print(response.text)\nprint(download_url)\nresponse.raise_for_status()\n    \n\n","type":"content","url":"/climatedt-parameterplotter#submission-worked-once-our-product-found-we-download-the-data","position":19},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Wait until data is there"},"type":"lvl2","url":"/climatedt-parameterplotter#wait-until-data-is-there","position":20},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl2":"Wait until data is there"},"content":"This data is not available at the moment. And we can see that our request is in queuedstatus.We will now poll the API until the data is ready and then download it.\n\nPlease note that the basic HDA quota allows a maximum of 4 requests per second. The following code limits polling to this quota.\n\npip install --user ratelimit --quiet\n\nfrom tqdm import tqdm\nimport time\nimport re\nfrom ratelimit import limits, sleep_and_retry\n\n# Set limit: max 4 calls per 1 seconds\nCALLS = 4\nPERIOD = 1  # seconds\n\n@sleep_and_retry\n@limits(calls=CALLS, period=PERIOD)\ndef call_api(url,auth_headers):\n    response = requests.get(url, headers=auth_headers, stream=True)\n    return response\n\n# we poll as long as the data is not ready\nif direct_download_url=='':\n    while url := response.headers.get(\"Location\"):\n        print(f\"order status: {response.json()['status']}\")\n        response = call_api(url,auth_headers)\n\nif (response.status_code not in (HTTP_SUCCESS_CODE,HTTP_ACCEPTED_CODE)):\n     (print(response.text))\n\n# Check if Content-Disposition header is present\nif \"Content-Disposition\" not in response.headers:\n    print(response)\n    print(response.text)\n    raise Exception(\"Headers: \\n\"+str(response.headers)+\"\\nContent-Disposition header not found in response. Must be something wrong.\")\n        \nfilename = re.findall('filename=\\\"?(.+)\\\"?', response.headers[\"Content-Disposition\"])[0]\ntotal_size = int(response.headers.get(\"content-length\", 0))\n\nprint(f\"downloading {filename}\")\n\nwith tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n    with open(filename, 'wb') as f:\n        for data in response.iter_content(1024):\n            progress_bar.update(len(data))\n            f.write(data)\n\n","type":"content","url":"/climatedt-parameterplotter#wait-until-data-is-there","position":21},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl2":"EarthKit"},"type":"lvl2","url":"/climatedt-parameterplotter#earthkit","position":22},{"hierarchy":{"lvl1":"HDA Climate DT Parameter Plotter - Tutorial","lvl2":"EarthKit"},"content":"Lets plot the result file\n[EarthKit Documentation] \n\nhttps://​earthkit​-data​.readthedocs​.io​/en​/latest​/index​.html\n\nThis section requires that you have ecCodes >= 2.35 installed on your system.You can follow the installation procedure at \n\nhttps://​confluence​.ecmwf​.int​/display​/ECC​/ecCodes+installation\n\nimport earthkit.data\nimport earthkit.maps\nimport earthkit.regrid\n\ndata = earthkit.data.from_source(\"file\", filename)\ndata.ls\nearthkit.maps.quickplot(data)\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action = \"ignore\", category = RuntimeWarning)","type":"content","url":"/climatedt-parameterplotter#earthkit","position":23},{"hierarchy":{"lvl1":"Climate Change Adaptation Digital Twin Series"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series","position":0},{"hierarchy":{"lvl1":"Climate Change Adaptation Digital Twin Series"},"content":"🚀 Launch in JupyterHub\n\nCredit: Earthkit and HDA Polytope used in this context are both packages provided by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\nDEDL Harmonised Data Access is used in this example.\n\nDocumentation DestinE Data Lake HDA\n\nClimate DT data catalogue\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series","position":1},{"hierarchy":{"lvl1":"Climate Change Adaptation Digital Twin Series","lvl2":"Obtain Authentication Token"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#obtain-authentication-token","position":2},{"hierarchy":{"lvl1":"Climate Change Adaptation Digital Twin Series","lvl2":"Obtain Authentication Token"},"content":"\n\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\n\nFirst, we get an access token for the API\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#obtain-authentication-token","position":3},{"hierarchy":{"lvl1":"Climate Change Adaptation Digital Twin Series","lvl4":"Check if DT access is granted","lvl2":"Obtain Authentication Token"},"type":"lvl4","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#check-if-dt-access-is-granted","position":4},{"hierarchy":{"lvl1":"Climate Change Adaptation Digital Twin Series","lvl4":"Check if DT access is granted","lvl2":"Obtain Authentication Token"},"content":"If DT access is not granted, you will not be able to execute the rest of the notebook.\n\nimport importlib\ninstalled_version = importlib.metadata.version(\"destinelab\")\nversion_number = installed_version.split('.')[1]\nif((int(version_number) >= 8 and float(installed_version) < 1) or float(installed_version) >= 1):\n    auth.is_DTaccess_allowed(access_token)\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#check-if-dt-access-is-granted","position":5},{"hierarchy":{"lvl1":"Climate Change Adaptation Digital Twin Series","lvl2":"Query using the DEDL HDA API"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#query-using-the-dedl-hda-api","position":6},{"hierarchy":{"lvl1":"Climate Change Adaptation Digital Twin Series","lvl2":"Query using the DEDL HDA API"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#query-using-the-dedl-hda-api","position":7},{"hierarchy":{"lvl1":"Climate Change Adaptation Digital Twin Series","lvl3":"Filter","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#filter","position":8},{"hierarchy":{"lvl1":"Climate Change Adaptation Digital Twin Series","lvl3":"Filter","lvl2":"Query using the DEDL HDA API"},"content":"We have to setup up a filter and define which data to obtain.\n\nChoose a valid combination of => Activity + Experiment + Model (based on the year of interest)\n\nFollowing activities/experiment/model & dates are possible:\n\nScenarioMIP/ssp3-7.0/ICON: Start date 20200101, 40 years\nScenarioMIP/ssp3-7.0/IFS-NEMO: Start date 20200101, 40 years\n\ndatechoice = \"2028-06-10T00:00:00Z\"\nfilters = {\n    key: {\"eq\": value}\n    for key, value in {\n        \"class\": \"d1\",             # fixed \n        \"dataset\": \"climate-dt\",   # fixed climate-dt access\n        \"activity\": \"ScenarioMIP\", # activity + experiment + model (go together)\n        \"experiment\": \"SSP3-7.0\",  # activity + experiment + model (go together)\n        \"model\": \"IFS-NEMO\",       # activity + experiment + model (go together)\n        \"generation\": \"1\",         # fixed Specifies the generation of the dataset, which can be incremented as required (latest is 1)\n        \"realization\": \"1\",        # fixed Specifies the climate realization. Default 1. Based on perturbations of initial conditions\n        \"resolution\": \"high\",      # standard/ high \n        \"expver\": \"0001\",          # fixed experiment version \n        \"stream\": \"clte\",          # fixed climate\n        \"time\": \"0000\",            # choose the hourly slot(s)\n        \"type\": \"fc\",              # fixed forecasted fields\n        \"levtype\": \"sfc\",          # Surface fields (levtype=sfc), Height level fields (levtype=hl), Pressure level fields (levtype=pl), Model Level (Levtype=ml)\n#        \"levelist\": \"1/2/3/...\",  # for ml/pl/sol type data\n        \"param\": \"167\"             # 2m Temperature parameter\n    }.items()\n}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#filter","position":9},{"hierarchy":{"lvl1":"Climate Change Adaptation Digital Twin Series","lvl3":"Make Data Request","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#make-data-request","position":10},{"hierarchy":{"lvl1":"Climate Change Adaptation Digital Twin Series","lvl3":"Make Data Request","lvl2":"Query using the DEDL HDA API"},"content":"We request data, it is not available at the moment. We can see that our request is in queuedstatus.We will poll the API until the data is ready and then download it.\n\nPlease note that the basic HDA quota allows a maximum of 4 requests per second. The following code limits polling to this quota.\n\npip install ratelimit --quiet\n\nfrom tqdm import tqdm\nimport time\nimport re\nfrom datetime import datetime\nfrom IPython.display import JSON\nfrom ratelimit import limits, sleep_and_retry\n\n\n#Sometimes requests to polytope get timeouts, it is then convenient define a retry strategy\n###\nretry_strategy = Retry(\n    total=5,  # Total number of retries\n    status_forcelist=[500, 502, 503, 504],  # List of 5xx status codes to retry on\n    allowed_methods=[\"GET\",'POST'],  # Methods to retry\n    backoff_factor=1  # Wait time between retries (exponential backoff)\n)\n\n# Create an adapter with the retry strategy\nadapter = HTTPAdapter(max_retries=retry_strategy)\n\n# Create a session and mount the adapter\nsession = requests.Session()\nsession.mount(\"https://\", adapter)\n\n\n# Set limit: max 4 calls per 1 seconds\n###\nCALLS = 4\nPERIOD = 1  # seconds\n@sleep_and_retry\n@limits(calls=CALLS, period=PERIOD)\ndef call_api(url,auth_headers):\n    response = session.get(url, headers=auth_headers, stream=True)\n    return response\n\n# Define date choice and filters if needed\ndatechoice = \"2024-07-01\"\n\n# Initialize a list to store filenames\nfilenames = []\n\n# Define start and end years\nstart_year = 2024\nstart_month = 7\nend_year = 2028\n\n# Loop \nfor year in range(start_year, end_year + 1):\n    # Create a datetime object \n    obsdate = datetime(year, start_month, 1)\n    datechoice = obsdate.strftime(\"%Y-%m-%dT12:00:00Z\")\n    response = session.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n        \"collections\": [\"EO.ECMWF.DAT.DT_CLIMATE_ADAPTATION\"],\n        \"datetime\": datechoice,\n        \"query\": filters\n    })\n\n    # Requests to EO.ECMWF.DAT.DT_CLIMATE always return a single item containing all the requested data\n    # print(response.json())\n    product = response.json()[\"features\"][0]\n\n    # DownloadLink is an asset representing the whole product\n    download_url = product[\"assets\"][\"downloadLink\"][\"href\"]\n    print(download_url)\n    HTTP_SUCCESS_CODE = 200\n    HTTP_ACCEPTED_CODE = 202\n\n    direct_download_url = ''\n\n    response =session.get(download_url, headers=auth_headers)\n    response.raise_for_status()\n    if (response.status_code == HTTP_SUCCESS_CODE):\n        direct_download_url = product['assets']['downloadLink']['href']\n    elif (response.status_code != HTTP_ACCEPTED_CODE):\n        JSON(response.json(), expanded=True)\n\n    # we poll as long as the data is not ready\n    if direct_download_url=='':\n        while url := response.headers.get(\"Location\"):\n            print(f\"order status: {response.json()['status']}\")\n            response = call_api(url,auth_headers)  \n            response.raise_for_status()\n            \n            \n    # Check if Content-Disposition header is present\n    if \"Content-Disposition\" not in response.headers:\n        print(response)\n        print(response.json())\n        raise Exception(\"Headers: \\n\"+str(response.headers)+\"\\nContent-Disposition header not found in response. Must be something wrong.\")\n    filename = re.findall('filename=\\\"?(.+)\\\"?', response.headers[\"Content-Disposition\"])[0]\n    total_size = int(response.headers.get(\"content-length\", 0))\n\n    print(f\"downloading {filename}\")\n\n    with tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n        with open(filename, 'wb') as f:\n            for data in response.iter_content(1024):\n                progress_bar.update(len(data))\n                f.write(data)\n    \n    # Add the filename to the list\n    filenames.append(filename)\n\n\n    \n    \n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#make-data-request","position":11},{"hierarchy":{"lvl1":"Climate Change Adaptation Digital Twin Series","lvl2":"EarthKit"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#earthkit","position":12},{"hierarchy":{"lvl1":"Climate Change Adaptation Digital Twin Series","lvl2":"EarthKit"},"content":"Lets plot the result file\n[EarthKit Documentation] \n\nhttps://​earthkit​-data​.readthedocs​.io​/en​/latest​/index​.html\n\nThis section requires that you have ecCodes >= 2.35 installed on your system.You can follow the installation procedure at \n\nhttps://​confluence​.ecmwf​.int​/display​/ECC​/ecCodes+installation\n\nimport earthkit.data\nimport earthkit.maps\nimport earthkit.regrid\n\n# Iterate over filenames\nfor filename in filenames:\n    print(filename)  # For example, print each filename\n    data = earthkit.data.from_source(\"file\", filename)\n    data.ls\n    earthkit.maps.quickplot(data,#style=style\n                       )\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate-series#earthkit","position":13},{"hierarchy":{"lvl1":"Climate Change Adaptation"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-climate","position":0},{"hierarchy":{"lvl1":"Climate Change Adaptation"},"content":"🚀 Launch in JupyterHub\n\nCredit: Earthkit and HDA Polytope used in this context are both packages provided by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\nDEDL Harmonised Data Access is used in this example.\n\nDocumentation DestinE Data Lake HDA\n\nDocumentation Digital Twin - Parameter Usage\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate","position":1},{"hierarchy":{"lvl1":"Climate Change Adaptation","lvl2":"Obtain Authentication Token"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#obtain-authentication-token","position":2},{"hierarchy":{"lvl1":"Climate Change Adaptation","lvl2":"Obtain Authentication Token"},"content":"\n\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\n\nimport importlib.metadata\n\nFirst, we get an access token for the API\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#obtain-authentication-token","position":3},{"hierarchy":{"lvl1":"Climate Change Adaptation","lvl4":"Check if DT access is granted","lvl2":"Obtain Authentication Token"},"type":"lvl4","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#check-if-dt-access-is-granted","position":4},{"hierarchy":{"lvl1":"Climate Change Adaptation","lvl4":"Check if DT access is granted","lvl2":"Obtain Authentication Token"},"content":"If DT access is not granted, you will not be able to execute the rest of the notebook.\n\nimport importlib\ninstalled_version = importlib.metadata.version(\"destinelab\")\nversion_number = installed_version.split('.')[1]\nif((int(version_number) >= 8 and float(installed_version) < 1) or float(installed_version) >= 1):\n    auth.is_DTaccess_allowed(access_token)\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#check-if-dt-access-is-granted","position":5},{"hierarchy":{"lvl1":"Climate Change Adaptation","lvl2":"Query using the DEDL HDA API"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#query-using-the-dedl-hda-api","position":6},{"hierarchy":{"lvl1":"Climate Change Adaptation","lvl2":"Query using the DEDL HDA API"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#query-using-the-dedl-hda-api","position":7},{"hierarchy":{"lvl1":"Climate Change Adaptation","lvl3":"Filter","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#filter","position":8},{"hierarchy":{"lvl1":"Climate Change Adaptation","lvl3":"Filter","lvl2":"Query using the DEDL HDA API"},"content":"We have to setup up a filter and define which data to obtain.\n\nChoose a valid combination of => Activity + Experiment + Model (based on the year of interest)\n\nFollowing activities/experiment/model & dates are possible:\n\nScenarioMIP/ssp3-7.0/ICON: Start date 20200101, 40 years\nScenarioMIP/ssp3-7.0/IFS-NEMO: Start date 20200101, 40 years\n\ndatechoice = \"2028-06-10T00:00:00Z\"\nfilters = {\n    key: {\"eq\": value}\n    for key, value in {\n        \"class\": \"d1\",             # fixed \n        \"dataset\": \"climate-dt\",   # fixed climate-dt access\n        \"activity\": \"ScenarioMIP\", # activity + experiment + model (go together)\n        \"experiment\": \"SSP3-7.0\",  # activity + experiment + model (go together)\n        \"model\": \"IFS-NEMO\",       # activity + experiment + model (go together)\n        \"generation\": \"1\",         # fixed Specifies the generation of the dataset, which can be incremented as required (latest is 1)\n        \"realization\": \"1\",        # fixed Specifies the climate realization. Default 1. Based on perturbations of initial conditions\n        \"resolution\": \"high\",      # standard/ high \n        \"expver\": \"0001\",          # fixed experiment version \n        \"stream\": \"clte\",          # fixed climate\n        \"time\": \"0000\",            # choose the hourly slot(s)\n        \"type\": \"fc\",              # fixed forecasted fields\n        \"levtype\": \"sfc\",          # Surface fields (levtype=sfc), Height level fields (levtype=hl), Pressure level fields (levtype=pl), Model Level (Levtype=ml)\n#        \"levelist\": \"1/2/3/...\",  # for ml/pl/sol type data\n        \"param\": \"134\"             # Surface Pressure parameter\n    }.items()\n}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#filter","position":9},{"hierarchy":{"lvl1":"Climate Change Adaptation","lvl3":"Make Data Request","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#make-data-request","position":10},{"hierarchy":{"lvl1":"Climate Change Adaptation","lvl3":"Make Data Request","lvl2":"Query using the DEDL HDA API"},"content":"\n\n#Sometimes requests to polytope get timeouts, it is then convenient define a retry strategy\nretry_strategy = Retry(\n    total=5,  # Total number of retries\n    status_forcelist=[500, 502, 503, 504],  # List of 5xx status codes to retry on\n    allowed_methods=[\"GET\",'POST'],  # Methods to retry\n    backoff_factor=1  # Wait time between retries (exponential backoff)\n)\n\n# Create an adapter with the retry strategy\nadapter = HTTPAdapter(max_retries=retry_strategy)\n\n# Create a session and mount the adapter\nsession = requests.Session()\nsession.mount(\"https://\", adapter)\n\nresponse = session.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n \"collections\": [\"EO.ECMWF.DAT.DT_CLIMATE_ADAPTATION\"],\n    \"datetime\": datechoice,\n    \"query\": filters\n})\n\nif(response.status_code!= 200):\n    (print(response.text))\nresponse.raise_for_status()\n\n# Requests to EO.ECMWF.DAT.DT_CLIMATE always return a single item containing all the requested data\nproduct = response.json()[\"features\"][0]\nproduct[\"id\"]\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#make-data-request","position":11},{"hierarchy":{"lvl1":"Climate Change Adaptation","lvl3":"Submission worked ? Once our product found, we download the data.","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#submission-worked-once-our-product-found-we-download-the-data","position":12},{"hierarchy":{"lvl1":"Climate Change Adaptation","lvl3":"Submission worked ? Once our product found, we download the data.","lvl2":"Query using the DEDL HDA API"},"content":"\n\nfrom IPython.display import JSON\n\n# DownloadLink is an asset representing the whole product\ndownload_url = product[\"assets\"][\"downloadLink\"][\"href\"]\nHTTP_SUCCESS_CODE = 200\nHTTP_ACCEPTED_CODE = 202\n\ndirect_download_url=''\n\nresponse = session.get(download_url, headers=auth_headers)\nif (response.status_code == HTTP_SUCCESS_CODE):\n    direct_download_url = product['assets']['downloadLink']['href']\nelif (response.status_code != HTTP_ACCEPTED_CODE):\n    print(response.text)\nprint(download_url)\nresponse.raise_for_status()    \n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#submission-worked-once-our-product-found-we-download-the-data","position":13},{"hierarchy":{"lvl1":"Climate Change Adaptation","lvl3":"Wait until data is there","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#wait-until-data-is-there","position":14},{"hierarchy":{"lvl1":"Climate Change Adaptation","lvl3":"Wait until data is there","lvl2":"Query using the DEDL HDA API"},"content":"This data is not available at the moment. And we can see that our request is in queuedstatus.We will now poll the API until the data is ready and then download it.\n\nPlease note that the basic HDA quota allows a maximum of 4 requests per second. The following code limits polling to this quota.\n\npip install ratelimit --quiet\n\nfrom tqdm import tqdm\nimport time\nimport re\nfrom ratelimit import limits, sleep_and_retry\n\n# Set limit: max 4 calls per 1 seconds\nCALLS = 4\nPERIOD = 1  # seconds\n\n@sleep_and_retry\n@limits(calls=CALLS, period=PERIOD)\ndef call_api(url,auth_headers):\n    response = session.get(url, headers=auth_headers, stream=True)\n    return response\n\n# we poll as long as the data is not ready\nif direct_download_url=='':\n    while url := response.headers.get(\"Location\"):\n        print(f\"order status: {response.json()['status']}\")\n        response = call_api(url,auth_headers)  \n        \nif (response.status_code not in (HTTP_SUCCESS_CODE,HTTP_ACCEPTED_CODE)):\n     (print(response.text))\n\n# Check if Content-Disposition header is present\nif \"Content-Disposition\" not in response.headers:\n    print(response)\n    print(response.text)\n    raise Exception(\"Headers: \\n\"+str(response.headers)+\"\\nContent-Disposition header not found in response. Must be something wrong.\")\n        \nfilename = re.findall('filename=\\\"?(.+)\\\"?', response.headers[\"Content-Disposition\"])[0]\ntotal_size = int(response.headers.get(\"content-length\", 0))\n\nprint(f\"downloading {filename}\")\n\nwith tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n    with open(filename, 'wb') as f:\n        for data in response.iter_content(1024):\n            progress_bar.update(len(data))\n            f.write(data)\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#wait-until-data-is-there","position":15},{"hierarchy":{"lvl1":"Climate Change Adaptation","lvl2":"EarthKit"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#earthkit","position":16},{"hierarchy":{"lvl1":"Climate Change Adaptation","lvl2":"EarthKit"},"content":"Lets plot the result file\n[EarthKit Documentation] \n\nhttps://​earthkit​-data​.readthedocs​.io​/en​/latest​/index​.html\n\nThis section requires that you have ecCodes >= 2.35 installed on your system.You can follow the installation procedure at \n\nhttps://​confluence​.ecmwf​.int​/display​/ECC​/ecCodes+installation\n\nimport earthkit.data\nimport earthkit.maps\nimport earthkit.regrid\n\ndata = earthkit.data.from_source(\"file\", filename)\ndata.ls\nearthkit.maps.quickplot(data,#style=style\n                       )","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-climate#earthkit","position":17},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series","position":0},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series"},"content":"🚀 Launch in JupyterHub\n\nCredit: Earthkit and HDA Polytope used in this context are both packages provided by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\nDEDL Harmonised Data Access is used in this example.\n\nDocumentation DestinE Data Lake HDA\n\nDocumentation Digital Twin - Parameter Usage\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series","position":1},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series","lvl2":"Obtain Authentication Token"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#obtain-authentication-token","position":2},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series","lvl2":"Obtain Authentication Token"},"content":"\n\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nimport ipywidgets as widgets\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\nfrom datetime import datetime, timedelta\n\nFirst, we get an access token for the API\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#obtain-authentication-token","position":3},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series","lvl4":"Check if DT access is granted","lvl2":"Obtain Authentication Token"},"type":"lvl4","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#check-if-dt-access-is-granted","position":4},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series","lvl4":"Check if DT access is granted","lvl2":"Obtain Authentication Token"},"content":"If DT access is not granted, you will not be able to execute the rest of the notebook.\n\nimport importlib\ninstalled_version = importlib.metadata.version(\"destinelab\")\nversion_number = installed_version.split('.')[1]\nif((int(version_number) >= 8 and float(installed_version) < 1) or float(installed_version) >= 1):\n    auth.is_DTaccess_allowed(access_token)\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#check-if-dt-access-is-granted","position":5},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series","lvl2":"Query using the DEDL HDA API"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#query-using-the-dedl-hda-api","position":6},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series","lvl2":"Query using the DEDL HDA API"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#query-using-the-dedl-hda-api","position":7},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series","lvl3":"Filter","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#filter","position":8},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series","lvl3":"Filter","lvl2":"Query using the DEDL HDA API"},"content":"We have to setup up a filter and define which data to obtain.\n\nExtreme DT data is available for specific time ranges (last 14 days) around the current date.\n\nIt is possible to use the ECMWF Aviso package to check data availability in the last 14 days (see \n\nhttps://github.com/destination-earth/DestinE-DataLake-Lab/blob/main/HDA/DestinE Digital Twins/ExtremeDT-dataAvailability.ipynb or \n\nextremes​-dt​/extremes​-dt​.py) and request data accordingly.\n\nfrom datetime import datetime, timedelta\n\n# Get the current date and time in UTC\ncurrent_date = datetime.utcnow()\n\n# Calculate the date 15 days before the current date\ndate_14_days_ago = current_date - timedelta(days=14)\n\n# Format the date as YYYYMMDD and set the time to 0000 UTC\nformatted_date = date_14_days_ago.strftime('%Y%m%d') + '0000'\n# Convert the formatted date back to a datetime object\ndate_from = datetime.strptime(formatted_date, '%Y%m%d%H%M%S').date()\n\n# Format the date as YYYYMMDD and set the time to 0000 UTC\nformatted_date = current_date.strftime('%Y%m%d') + '0000'\n# Convert the formatted date back to a datetime object\ndate_to = datetime.strptime(formatted_date, '%Y%m%d%H%M%S').date()\n\nstart_date_picker = widgets.DatePicker(description='Start Date:', disabled=False)\n\n# Set initial values directly\nstart_date_picker.min = date_from\nstart_date_picker.max = date_to\nstart_date_picker.value = date_from\n\ndef get_selected_values():\n    selected_start_date = start_date_picker.value\n    return selected_start_date\n\n# Display widgets\ndisplay(start_date_picker)\n\n\n# Example usage:\ndatechoice = get_selected_values().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\n#print(datechoice)\nfilters = {\n    key: {\"eq\": value}\n    for key, value in {\n        \"class\": \"d1\",             # fixed (rd or d1)\n        \"dataset\": \"extremes-dt\",  # fixed extreme dt \n        \"expver\": \"0001\",          # fixed experiment version \n        \"stream\": \"oper\",          # fixed oper\n        \"step\": \"0/6/12/18/24\",    # Forcast step hourly (1..96)\n        \"type\": \"fc\",              # fixed forecasted fields\n        \"levtype\": \"sfc\",          # Surface fields (levtype=sfc), Height level fields (levtype=hl), Pressure level fields (levtype=pl), Model Level (Levtype=ml)\n        \"param\": \"151/228029\"      # 10m Wind gust & Mean Sea Level Pressure\n    }.items()\n}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#filter","position":9},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series","lvl3":"Make Data Request","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#make-data-request","position":10},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series","lvl3":"Make Data Request","lvl2":"Query using the DEDL HDA API"},"content":"\n\n#Sometimes requests to polytope get timeouts, it is then convenient define a retry strategy\nretry_strategy = Retry(\n    total=5,  # Total number of retries\n    status_forcelist=[500, 502, 503, 504],  # List of 5xx status codes to retry on\n    allowed_methods=[\"GET\",'POST'],  # Methods to retry\n    backoff_factor=1  # Wait time between retries (exponential backoff)\n)\n\n# Create an adapter with the retry strategy\nadapter = HTTPAdapter(max_retries=retry_strategy)\n\n# Create a session and mount the adapter\nsession = requests.Session()\nsession.mount(\"https://\", adapter)\n\nresponse = session.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n    \"collections\": [\"EO.ECMWF.DAT.DT_EXTREMES\"],\n    \"datetime\": datechoice,\n    \"query\": filters\n})\n\n# Requests to EO.ECMWF.DAT.DT_EXTREMES always return a single item containing all the requested data\nif(response.status_code!= 200):\n    (print(response.text))\nresponse.raise_for_status()\n\nproduct = response.json()[\"features\"][0]\nproduct[\"id\"]\n#product\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#make-data-request","position":11},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series","lvl3":"Submission worked ? Once our product found, we download the data.","lvl2":"Query using the DEDL HDA API"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#submission-worked-once-our-product-found-we-download-the-data","position":12},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series","lvl3":"Submission worked ? Once our product found, we download the data.","lvl2":"Query using the DEDL HDA API"},"content":"\n\nfrom IPython.display import JSON\n\n# DownloadLink is an asset representing the whole product\ndownload_url = product[\"assets\"][\"downloadLink\"][\"href\"]\n\nHTTP_SUCCESS_CODE = 200\nHTTP_ACCEPTED_CODE = 202\n\ndirect_download_url=''\n\nresponse = session.get(download_url, headers=auth_headers)\nif (response.status_code == HTTP_SUCCESS_CODE):\n    direct_download_url = product['assets']['downloadLink']['href']\nelif (response.status_code != HTTP_ACCEPTED_CODE):\n    print(response.text)\n\nresponse.raise_for_status()    \nprint(download_url)   \n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#submission-worked-once-our-product-found-we-download-the-data","position":13},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series","lvl2":"Wait until data is there"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#wait-until-data-is-there","position":14},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series","lvl2":"Wait until data is there"},"content":"This data is not available at the moment. And we can see that our request is in queuedstatus.We will now poll the API until the data is ready and then download it.\n\nPlease note that the basic HDA quota allows a maximum of 4 requests per second. The following code limits polling to this quota.\n\npip install ratelimit --quiet\n\nfrom tqdm import tqdm\nimport time\nimport re\nfrom tqdm import tqdm\nimport time\nimport re\nfrom ratelimit import limits, sleep_and_retry\n\n# Set limit: max 4 calls per 1 seconds\nCALLS = 4\nPERIOD = 1  # seconds\n\n@sleep_and_retry\n@limits(calls=CALLS, period=PERIOD)\ndef call_api(url,auth_headers):\n    response = session.get(url, headers=auth_headers, stream=True)\n    return response\n\n# we poll as long as the data is not ready\nif direct_download_url=='':\n    while url := response.headers.get(\"Location\"):\n        print(f\"order status: {response.json()['status']}\")\n        print(f\"order status: {response.text}\")\n        response = call_api(url,auth_headers)  \n        \nif (response.status_code not in (HTTP_SUCCESS_CODE,HTTP_ACCEPTED_CODE)):\n    (print(response.text))\n\n        \n# Check if Content-Disposition header is present\nif \"Content-Disposition\" not in response.headers:\n    print(response)\n    print(response.text)\n    raise Exception(\"Headers: \\n\"+str(response.headers)+\"\\nContent-Disposition header not found in response. Must be something wrong.\")\n        \nfilename = re.findall('filename=\\\"?(.+)\\\"?', response.headers[\"Content-Disposition\"])[0]\ntotal_size = int(response.headers.get(\"content-length\", 0))\n\nprint(f\"downloading {filename}\")\n\nwith tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n    with open(filename, 'wb') as f:\n        for data in response.iter_content(1024):\n            progress_bar.update(len(data))\n            f.write(data)\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#wait-until-data-is-there","position":15},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series","lvl2":"Render the sea ice coverage on a map"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#render-the-sea-ice-coverage-on-a-map","position":16},{"hierarchy":{"lvl1":"Weather-Induced Extremes Digital Twin Series","lvl2":"Render the sea ice coverage on a map"},"content":"Lets plot the result file\n\nThis section requires that you have ecCodes >= 2.35 installed on your system.You can follow the installation procedure at \n\nhttps://​confluence​.ecmwf​.int​/display​/ECC​/ecCodes+installation\n\nimport earthkit.data\nimport earthkit.maps\n\ndata = earthkit.data.from_source(\"file\", filename)\ndata.ls()\n\nimport matplotlib.pyplot as plt\n\nchart = earthkit.maps.Chart(domain=[-5, 23, 40, 58], rows=3, columns=4)\n\ngust_style = earthkit.maps.Style(\n    colors=[\"#85AAEE\", \"#208EFC\", \"#6CA632\", \"#FFB000\", \"#FF0000\", \"#7A11B1\"],\n    levels=[12, 15, 20, 25, 30, 35, 50],\n    units=\"m s-1\",\n)\n\nchart.add_subplot(row=0, column=3)\nfor i in range(4):\n    chart.add_subplot(row=1+i//4, column=i%4)\n\nchart.plot(data.sel(shortName=\"i10fg\"), style=gust_style)\nchart.plot(data.sel(shortName=\"msl\"), units=\"hPa\")\n\nchart.land(color=\"lightgrey\")\nchart.coastlines()\n\nax = plt.axes((0.05, 0.8, 0.65, 0.025))\nchart.legend(ax=ax)\n\nchart.subplot_titles(\"{time:%Y-%m-%d %H} UTC (+{lead_time}h)\")\nchart.title(\n    \"ECMWF HRES Run: {base_time:%Y-%m-%d %H} UTC\\n{variable_name}\",\n    fontsize=15, horizontalalignment=\"left\", x=0, y=0.96,\n)\n\nchart.show()","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes-series#render-the-sea-ice-coverage-on-a-map","position":17},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA"},"type":"lvl1","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes","position":0},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA"},"content":"🚀 Launch in JupyterHub\n\nCredit: Earthkit and HDA Polytope used in this context are both packages provided by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\nDocumentation DestinE Data Lake HDA\n\nDocumentation Digital Twin - Parameter Usage\n\nDEDL Harmonised Data Access is used in this example.\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes","position":1},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA","lvl2":"Obtain Authentication Token"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#obtain-authentication-token","position":2},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA","lvl2":"Obtain Authentication Token"},"content":"\n\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nimport ipywidgets as widgets\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#obtain-authentication-token","position":3},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA","lvl4":"Check if DT access is granted","lvl2":"Obtain Authentication Token"},"type":"lvl4","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#check-if-dt-access-is-granted","position":4},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA","lvl4":"Check if DT access is granted","lvl2":"Obtain Authentication Token"},"content":"If DT access is not granted, you will not be able to execute the rest of the notebook.\n\nimport importlib\ninstalled_version = importlib.metadata.version(\"destinelab\")\nversion_number = installed_version.split('.')[1]\nif((int(version_number) >= 8 and float(installed_version) < 1) or float(installed_version) >= 1):\n    auth.is_DTaccess_allowed(access_token)\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#check-if-dt-access-is-granted","position":5},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA","lvl3":"Query using the DEDL HDA API","lvl2":"Obtain Authentication Token"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#query-using-the-dedl-hda-api","position":6},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA","lvl3":"Query using the DEDL HDA API","lvl2":"Obtain Authentication Token"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#query-using-the-dedl-hda-api","position":7},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA","lvl3":"Filter","lvl2":"Obtain Authentication Token"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#filter","position":8},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA","lvl3":"Filter","lvl2":"Obtain Authentication Token"},"content":"We have to setup up a filter and define which data to obtain.\n\nExtreme DT data is available for specific time ranges (last 14 days) around the current date.\n\nIt is possible to use the ECMWF Aviso package to check data availability in the last 14 days (see \n\nhttps://github.com/destination-earth/DestinE-DataLake-Lab/blob/main/HDA/DestinE Digital Twins/ExtremeDT-dataAvailability.ipynb or \n\nextremes​-dt​/extremes​-dt​.py) and request data accordingly.\n\nfrom datetime import datetime, timedelta\n\n# Get the current date and time in UTC\ncurrent_date = datetime.utcnow()\n\n# Calculate the date 14 days before the current date\ndate_14_days_ago = current_date - timedelta(days=14)\n\n# Format the date as YYYYMMDD and set the time to 0000 UTC\nformatted_date = date_14_days_ago.strftime('%Y%m%d') + '0000'\n# Convert the formatted date back to a datetime object\ndate_from = datetime.strptime(formatted_date, '%Y%m%d%H%M%S').date()\n\n# Format the date as YYYYMMDD and set the time to 0000 UTC\nformatted_date = current_date.strftime('%Y%m%d') + '0000'\n# Convert the formatted date back to a datetime object\ndate_to = datetime.strptime(formatted_date, '%Y%m%d%H%M%S').date()\n\n\n\nstart_date_picker = widgets.DatePicker(description='Start Date:', disabled=False)\n\n# Set initial values directly\nstart_date_picker.min = date_from\nstart_date_picker.max = date_to\nstart_date_picker.value = date_from\n\ndef get_selected_values():\n    selected_start_date = start_date_picker.value\n    return selected_start_date\n\n# Display widgets\ndisplay(start_date_picker)\n\ndatechoice =  get_selected_values().strftime(\"%Y-%m-%dT%H:%M:%SZ\")\nfilters = {\n    key: {\"eq\": value}\n    for key, value in {\n        \"class\": \"d1\",             # fixed (rd or d1)\n        \"dataset\": \"extremes-dt\",  # fixed extreme dt \n        \"expver\": \"0001\",          # fixed experiment version \n        \"stream\": \"oper\",          # fixed oper\n        \"step\": \"0\",               # Forcast step hourly (1..96)\n        \"type\": \"fc\",              # fixed forecasted fields\n        \"levtype\": \"sfc\",          # Surface fields (levtype=sfc), Height level fields (levtype=hl), Pressure level fields (levtype=pl), Model Level (Levtype=ml)\n        \"param\": \"31\"             # Sea ice area fraction\n    }.items()\n}\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#filter","position":9},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA","lvl3":"Make Data Request","lvl2":"Obtain Authentication Token"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#make-data-request","position":10},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA","lvl3":"Make Data Request","lvl2":"Obtain Authentication Token"},"content":"\n\n#Sometimes requests to polytope get timeouts, it is then convenient define a retry strategy\nretry_strategy = Retry(\n    total=5,  # Total number of retries\n    status_forcelist=[500, 502, 503, 504],  # List of 5xx status codes to retry on\n    allowed_methods=[\"GET\",'POST'],  # Methods to retry\n    backoff_factor=1  # Wait time between retries (exponential backoff)\n)\n\n# Create an adapter with the retry strategy\nadapter = HTTPAdapter(max_retries=retry_strategy)\n\n# Create a session and mount the adapter\nsession = requests.Session()\nsession.mount(\"https://\", adapter)\nresponse = session.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n    \"collections\": [\"EO.ECMWF.DAT.DT_EXTREMES\"],\n    \"datetime\": datechoice,\n    \"query\": filters\n})\n\n# Requests to EO.ECMWF.DAT.DT_EXTREMES always return a single item containing all the requested data\nif(response.status_code!= 200):\n    (print(response.text))\nresponse.raise_for_status()\n\nproduct = response.json()[\"features\"][0]\nproduct[\"id\"]\n#product\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#make-data-request","position":11},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA","lvl3":"Submission worked ? Once our product found, we download the data.","lvl2":"Obtain Authentication Token"},"type":"lvl3","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#submission-worked-once-our-product-found-we-download-the-data","position":12},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA","lvl3":"Submission worked ? Once our product found, we download the data.","lvl2":"Obtain Authentication Token"},"content":"\n\nfrom IPython.display import JSON\n\n# DownloadLink is an asset representing the whole product\ndownload_url = product[\"assets\"][\"downloadLink\"][\"href\"]\n\nHTTP_SUCCESS_CODE = 200\nHTTP_ACCEPTED_CODE = 202\n\ndirect_download_url=''\n\nresponse = session.get(download_url, headers=auth_headers)\nif (response.status_code == HTTP_SUCCESS_CODE):\n    direct_download_url = product['assets']['downloadLink']['href']\nelif (response.status_code != HTTP_ACCEPTED_CODE):\n    print(response.text)\n\nresponse.raise_for_status()    \nprint(download_url)  \n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#submission-worked-once-our-product-found-we-download-the-data","position":13},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA","lvl2":"Wait until data is there"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#wait-until-data-is-there","position":14},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA","lvl2":"Wait until data is there"},"content":"This data is not available at the moment. And we can see that our request is in queuedstatus.We will now poll the API until the data is ready and then download it.\n\nPlease note that the basic HDA quota allows a maximum of 4 requests per second. The following code limits polling to this quota.\n\npip install ratelimit --quiet\n\nfrom tqdm import tqdm\nimport time\nimport re\nfrom ratelimit import limits, sleep_and_retry\n\n# Set limit: max 4 calls per 1 seconds\nCALLS = 4\nPERIOD = 1  # seconds\n\n@sleep_and_retry\n@limits(calls=CALLS, period=PERIOD)\ndef call_api(url,auth_headers):\n    response = session.get(url, headers=auth_headers, stream=True)\n    return response\n\n\n# we poll as long as the data is not ready\nif direct_download_url=='':\n    while url := response.headers.get(\"Location\"):\n        print(f\"order status: {response.json()['status']}\")\n        response = call_api(url,auth_headers)\n\nif (response.status_code not in (HTTP_SUCCESS_CODE,HTTP_ACCEPTED_CODE)):\n     (print(response.text))\n        \n# Check if Content-Disposition header is present\nif \"Content-Disposition\" not in response.headers:\n    print(response)\n    print(response.text)\n    raise Exception(\"Headers: \\n\"+str(response.headers)+\"\\nContent-Disposition header not found in response. Must be something wrong.\")\n        \nfilename = re.findall('filename=\\\"?(.+)\\\"?', response.headers[\"Content-Disposition\"])[0]\ntotal_size = int(response.headers.get(\"content-length\", 0))\n\nprint(f\"downloading {filename}\")\n\nwith tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n    with open(filename, 'wb') as f:\n        for data in response.iter_content(1024):\n            progress_bar.update(len(data))\n            f.write(data)\n\n","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#wait-until-data-is-there","position":15},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA","lvl2":"Render the sea ice coverage on a map"},"type":"lvl2","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#render-the-sea-ice-coverage-on-a-map","position":16},{"hierarchy":{"lvl1":"Weather-Induced Extremes - Data Access using DEDL HDA","lvl2":"Render the sea ice coverage on a map"},"content":"Lets plot the result file\n\nThis section requires that you have ecCodes >= 2.35 installed on your system.You can follow the installation procedure at \n\nhttps://​confluence​.ecmwf​.int​/display​/ECC​/ecCodes+installation\n\nimport xarray as xr\nimport cfgrib\nimport matplotlib.pyplot as plt\nimport numpy as np\n\nds = xr.load_dataset(filename, engine=\"cfgrib\")\n\nds\n\nimport cartopy.crs as crs\nimport cartopy.feature as cfeature\n\nfig = plt.figure(figsize=[10, 10])\n\n#ax = fig.add_subplot(1,1,1, projection=crs.Robinson())\ncrs_epsg=crs.NorthPolarStereo(central_longitude=0)\nax = fig.add_subplot(1,1,1, projection=crs_epsg)\n\nax.set_extent([-3850000.0, 3750000.0, -5350000, 5850000.0],crs_epsg)\n\nax.add_feature(cfeature.COASTLINE)\nax.gridlines()\n\ncs = plt.scatter(x=ds.longitude[::10], y=ds.latitude.data[::10], c=ds.siconc[::10], cmap=\"Blues\",\n            s=1,\n            transform=crs.PlateCarree())\n\nfig.colorbar(cs, ax=ax, location='right', shrink =0.8)\nplt.show()","type":"content","url":"/dedl-hda-eo-ecmwf-dat-dt-extremes#render-the-sea-ice-coverage-on-a-map","position":17},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial"},"type":"lvl1","url":"/extremedt-parameterplotter","position":0},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial"},"content":"🚀 Launch in JupyterHub\n\nCredit: Earthkit and HDA Polytope used in this context are both packages provided by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\nDEDL Harmonised Data Access is used in this example to access and plot Extreme DT parameter.\n\nDocumentation DestinE DataLake HDA\n\nDocumentation Digital Twin - Parameter Usage\n\n","type":"content","url":"/extremedt-parameterplotter","position":1},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl3":"Import the required packages"},"type":"lvl3","url":"/extremedt-parameterplotter#import-the-required-packages","position":2},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl3":"Import the required packages"},"content":"\n\nImport the Climate DT parameter & scenario dictionary\n\nfrom destinelab import extreme_dt_dictionary\nimport ipywidgets as widgets\nimport json\nfrom datetime import datetime, timedelta\n\n","type":"content","url":"/extremedt-parameterplotter#import-the-required-packages","position":3},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl3":"Extreme DT parameter selection (we limit the plotting to one parameter)"},"type":"lvl3","url":"/extremedt-parameterplotter#extreme-dt-parameter-selection-we-limit-the-plotting-to-one-parameter","position":4},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl3":"Extreme DT parameter selection (we limit the plotting to one parameter)"},"content":"\n\n# Create search box\nsearch_box = widgets.Text(placeholder='Search by parameter name', description='Search:', disabled=False)\n\n# Create dropdown to select entry\nentry_dropdown = widgets.Dropdown(\n    options=[(entry['paramName'], i) for i, entry in enumerate(extreme_dt_dictionary.extremeDT_params)],\n    description='Select Entry:'\n)\n\ndef filter_entries(search_string):\n    return [(entry['paramName'], i) for i, entry in enumerate(extreme_dt_dictionary.extremeDT_params) if search_string.lower() in entry['paramName'].lower()]\n\ndef on_search_change(change):\n    search_string = change.new\n    if search_string:\n        filtered_options = filter_entries(search_string)\n        entry_dropdown.options = filtered_options\n    else:\n        entry_dropdown.options = [(entry['paramName'], i) for i, entry in enumerate(extreme_dt_dictionary.extremeDT_params)]\n\nsearch_box.observe(on_search_change, names='value')\n\n# Display widgets\ndisplay(search_box, entry_dropdown)\n\ndef get_selected_entry():\n    return entry_dropdown.value\n\n\n","type":"content","url":"/extremedt-parameterplotter#extreme-dt-parameter-selection-we-limit-the-plotting-to-one-parameter","position":5},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl2":"Print the details of the parameter (Polytope convention):"},"type":"lvl2","url":"/extremedt-parameterplotter#print-the-details-of-the-parameter-polytope-convention","position":6},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl2":"Print the details of the parameter (Polytope convention):"},"content":"\n\n\nselected_index = get_selected_entry()\nselected_entry = extreme_dt_dictionary.extremeDT_params[selected_index]\nprint(json.dumps(selected_entry,indent=4))\n\n","type":"content","url":"/extremedt-parameterplotter#print-the-details-of-the-parameter-polytope-convention","position":7},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl3":"Select the Date of Observation to be selected","lvl2":"Print the details of the parameter (Polytope convention):"},"type":"lvl3","url":"/extremedt-parameterplotter#select-the-date-of-observation-to-be-selected","position":8},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl3":"Select the Date of Observation to be selected","lvl2":"Print the details of the parameter (Polytope convention):"},"content":"\n\nExtremes DT data that is older than 15 days is archived, below it is then possible to choose only dates in the last 15 days.\n\nTo choose your date, please consider also the current Extremes DT data availability. Extremes DT data availability can be find out using the ExtremeDT-dataAvailability.ipynb in this folder.\n\nfrom datetime import datetime, timedelta\n\n# Get the current date and time in UTC\ncurrent_date = datetime.utcnow()\n\n# Calculate the date 15 days before the current date\ndate_15_days_ago = current_date - timedelta(days=15)\n\n# Format the date as YYYYMMDD and set the time to 0000 UTC\nformatted_date = date_15_days_ago.strftime('%Y%m%d') + '0000'\n# Convert the formatted date back to a datetime object\ndate_from = datetime.strptime(formatted_date, '%Y%m%d%H%M%S').date()\n\n# Format the date as YYYYMMDD and set the time to 0000 UTC\nformatted_date = current_date.strftime('%Y%m%d') + '0000'\n# Convert the formatted date back to a datetime object\ndate_to = datetime.strptime(formatted_date, '%Y%m%d%H%M%S').date()\n\nfrom ipywidgets import Label\n# Create dropdown to select scenario\nscenario_dropdown = widgets.Dropdown(\n    options=[(f\"{entry['model']}\", (i)) for i, entry in enumerate(extreme_dt_dictionary.extremeDT_scenario)],\n    description='Scenario:'\n)\n\n# Create date picker widgets\nstart_date_picker = widgets.DatePicker(description='Start Date:', disabled=False)\n\ndef on_scenario_change(change):\n    print(\"scenario_change\")\n    selected_index = change.new\n    selected_entry = extreme_dt_dictionary.extremeDT_scenario[selected_index]\n    start_date_picker.min = date_from\n    start_date_picker.max = date_to\n    # Set the initial date of the start_date_picker to the scenario's start date\n    start_date_picker.value = date_to\n    selected_start_date = start_date_picker.value\n    \nscenario_dropdown.observe(on_scenario_change, names='value')\n\n# Set initial values directly\nselected_entry = extreme_dt_dictionary.extremeDT_scenario[0]\n\n# Set initial values directly\nstart_date_picker.min = date_from\nstart_date_picker.max = date_to\nstart_date_picker.value = date_to\n\n# Display widgets\n\ndef get_selected_values():\n    selected_scenario_index = scenario_dropdown.value\n    selected_scenario = extreme_dt_dictionary.extremeDT_scenario[selected_scenario_index]\n    selected_start_date = start_date_picker.value\n    return selected_scenario_index, selected_scenario, selected_start_date\n\n# Display widgets\ndisplay(Label(\"To choose your date, please consider the current Extremes DT data availability (ExtremeDT-dataAvailability.ipynb in this folder).\"),scenario_dropdown,  start_date_picker)\n\n# Example usage:\nselected_scenario_index, selected_scenario, selected_start_date = get_selected_values()\n\n\n","type":"content","url":"/extremedt-parameterplotter#select-the-date-of-observation-to-be-selected","position":9},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl3":"Choose now the Steps within the observation to be retrieved (one step usually one hour)","lvl2":"Print the details of the parameter (Polytope convention):"},"type":"lvl3","url":"/extremedt-parameterplotter#choose-now-the-steps-within-the-observation-to-be-retrieved-one-step-usually-one-hour","position":10},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl3":"Choose now the Steps within the observation to be retrieved (one step usually one hour)","lvl2":"Print the details of the parameter (Polytope convention):"},"content":"\n\nselected_entry = extreme_dt_dictionary.extremeDT_params[selected_index]\ninput_string = selected_entry[\"step\"]\n\ndef parse_input_string(input_string):\n    ranges = input_string.split('/')\n    step_start = \"\"\n    step_end = \"\"\n    step_width = 1\n    hypen = \"\"\n    options = []\n    for rng in ranges:\n        if rng:\n            if '-' in rng:\n                hypen = \"-\"\n                start, end = rng.split('-')\n                step_width = int(end) - int(start)\n                if step_start == \"\":\n                    step_start = int(start.strip())\n                step_end = int(end.strip())\n            elif 'to' not in rng:\n                if step_start == \"\":\n                    step_start = int(rng.strip())\n                step_end = int(rng.strip())\n                #options.append(option)\n    #print(str(step_start) + \":\" + str(step_end))\n    if hypen != \"\":\n        options.extend([f\"{i}-{i+step_width}\" for i in range(step_start, step_end, step_width)])\n    else:\n        options.extend([f\"{i}\" for i in range(step_start, step_end+1, step_width)])\n    return options\n\ndef get_selected_step_values():\n    selected_values = multi_select.value\n    selected_values_string = \"/\".join(selected_values)\n    return selected_values_string\n\noptions = parse_input_string(input_string)\n\nmulti_select = widgets.SelectMultiple(\n    options=options,\n    description='Select (Steps):',\n    disabled=False\n)\n\ndisplay(multi_select)\n\n\n","type":"content","url":"/extremedt-parameterplotter#choose-now-the-steps-within-the-observation-to-be-retrieved-one-step-usually-one-hour","position":11},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl3":"Handle different Levels to be selected (if any)","lvl2":"Print the details of the parameter (Polytope convention):"},"type":"lvl3","url":"/extremedt-parameterplotter#handle-different-levels-to-be-selected-if-any","position":12},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl3":"Handle different Levels to be selected (if any)","lvl2":"Print the details of the parameter (Polytope convention):"},"content":"\n\n# Define a global variable\nglobal global_widget\nglobal_widget = None\n\nif selected_entry[\"levelist\"] != \"\":\n    # Convert levelist string to list of integers\n    levelist = list(map(int, selected_entry[\"levelist\"].split('/')))\n\n      \n    # Create a function to generate the widget based on the selection mode\n    def generate_widget(selection_mode):\n        global global_widget\n        if selection_mode == 'Single':\n            global_widget = widgets.Dropdown(options=levelist, description='Select level:')\n            return global_widget\n        elif selection_mode == 'Multiple':\n            global_widget = widgets.SelectMultiple(options=levelist, description='Select levels:')\n            return global_widget\n\n    # Create a dropdown widget to choose selection mode\n    selection_mode_dropdown = widgets.Dropdown(options=['Single', 'Multiple'], description='Selection Mode:')\n\n    # Create an output widget to display the selected option(s)\n    output = widgets.Output()\n\n    # Function to display the widget based on the selection mode\n    def display_widget(selection_mode):\n        output.clear_output()\n        with output:\n            display(generate_widget(selection_mode))\n\n    # Define a function to handle the change in selection mode\n    def on_dropdown_change(change):\n        display_widget(change.new)\n\n    # Register the function to handle dropdown changes\n    selection_mode_dropdown.observe(on_dropdown_change, names='value')\n\n    # Display the widgets\n    display(selection_mode_dropdown, output)\n\n    # Display the initial widget based on default selection mode\n    display_widget('Single')\n\n# Function to convert tuple or single integer to string separated by \"/\"\ndef convert_to_string(input):\n    if isinstance(input, tuple):\n        return '/'.join(map(str, input))\n    elif isinstance(input, int):\n        return str(input)\n    else:\n        return None  # Handle other types if needed\n\nlevlInput = \"\"\nif global_widget != None:\n    # Test cases\n    levlInput = convert_to_string(global_widget.value)\n\n\n# Call get_selected_values after the display is finished\nselected_step_values = get_selected_step_values()\n\n# Print the result in JSON format\n#datechoice = get_selected_values()[2].strftime('%Y%m%d')\ndatechoice = \"{fname}T00:00:00Z\".format(fname = get_selected_values()[2])\n\nfilter_params = {\n  \"class\": \"d1\",                       # fixed \n  \"dataset\": \"extremes-dt\",             # fixed extreme-dt access\n  \"expver\": \"0001\",                    # fixed experiment version \n  \"stream\": selected_entry[\"stream\"],\n  \"type\": \"fc\",                        # fixed forecasted fields\n#  \"date\": datechoice,                  # choose the date\n  \"time\": \"0000\",                      # fixed \n  \"step\": selected_step_values,        # step choice \n  \"levtype\": selected_entry[\"levtype\"],  \n  \"levelist\": str(levlInput),  \n  \"param\": str(selected_entry[\"param\"]),  \n}\n\n# Print the result in JSON format\nprint(datechoice)\nprint(json.dumps(filter_params, indent=4))\n\n","type":"content","url":"/extremedt-parameterplotter#handle-different-levels-to-be-selected-if-any","position":13},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl3":"Obtain Authentication Token","lvl2":"Print the details of the parameter (Polytope convention):"},"type":"lvl3","url":"/extremedt-parameterplotter#obtain-authentication-token","position":14},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl3":"Obtain Authentication Token","lvl2":"Print the details of the parameter (Polytope convention):"},"content":"\n\nimport requests\nfrom requests.adapters import HTTPAdapter\nfrom urllib3.util.retry import Retry\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\n\nDESP_USERNAME = input(\"Please input your DESP username: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/extremedt-parameterplotter#obtain-authentication-token","position":15},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl5":"Check if DT access is granted","lvl3":"Obtain Authentication Token","lvl2":"Print the details of the parameter (Polytope convention):"},"type":"lvl5","url":"/extremedt-parameterplotter#check-if-dt-access-is-granted","position":16},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl5":"Check if DT access is granted","lvl3":"Obtain Authentication Token","lvl2":"Print the details of the parameter (Polytope convention):"},"content":"If DT access is not granted, you will not be able to execute the rest of the notebook.\n\nimport importlib\ninstalled_version = importlib.metadata.version(\"destinelab\")\nversion_number = installed_version.split('.')[1]\nif((int(version_number) >= 8 and float(installed_version) < 1) or float(installed_version) >= 1):\n    auth.is_DTaccess_allowed(access_token)\n\n","type":"content","url":"/extremedt-parameterplotter#check-if-dt-access-is-granted","position":17},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl3":"Query using the DEDL HDA API","lvl2":"Print the details of the parameter (Polytope convention):"},"type":"lvl3","url":"/extremedt-parameterplotter#query-using-the-dedl-hda-api","position":18},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl3":"Query using the DEDL HDA API","lvl2":"Print the details of the parameter (Polytope convention):"},"content":"\n\n","type":"content","url":"/extremedt-parameterplotter#query-using-the-dedl-hda-api","position":19},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl4":"Filter","lvl3":"Query using the DEDL HDA API","lvl2":"Print the details of the parameter (Polytope convention):"},"type":"lvl4","url":"/extremedt-parameterplotter#filter","position":20},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl4":"Filter","lvl3":"Query using the DEDL HDA API","lvl2":"Print the details of the parameter (Polytope convention):"},"content":"We have to setup up a filter and define which data to obtain.\n\n# Check if levelist is empty and remove it\nif filter_params.get(\"levelist\") == \"\":\n    del filter_params[\"levelist\"]\n   \nhdaFilters = {\n    key: {\"eq\": value}\n    for key, value in filter_params.items()\n}\n\n#print(hdaFilters)\n\n\n\n","type":"content","url":"/extremedt-parameterplotter#filter","position":21},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl4":"Make Data Request","lvl3":"Query using the DEDL HDA API","lvl2":"Print the details of the parameter (Polytope convention):"},"type":"lvl4","url":"/extremedt-parameterplotter#make-data-request","position":22},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl4":"Make Data Request","lvl3":"Query using the DEDL HDA API","lvl2":"Print the details of the parameter (Polytope convention):"},"content":"Please repeat this call is you have a timeout failure (under investion in the moment)\n\n#Sometimes requests to polytope get timeouts, it is then convenient define a retry strategy\nretry_strategy = Retry(\n    total=5,  # Total number of retries\n    status_forcelist=[500, 502, 503, 504],  # List of 5xx status codes to retry on\n    allowed_methods=[\"GET\",'POST'],  # Methods to retry\n    backoff_factor=1  # Wait time between retries (exponential backoff)\n)\n\n# Create an adapter with the retry strategy\nadapter = HTTPAdapter(max_retries=retry_strategy)\n\n# Create a session and mount the adapter\nsession = requests.Session()\nsession.mount(\"https://\", adapter)\n\nresponse = session.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n \"collections\": [\"EO.ECMWF.DAT.DT_EXTREMES\"],\n    \"datetime\": datechoice,\n    \"query\": hdaFilters\n})\n\n# Requests to EO.ECMWF.DAT.DT_EXTREMES always return a single item containing all the requested data\n#print(response.json())\nproduct = response.json()[\"features\"][0]\nproduct[\"id\"]\n\n","type":"content","url":"/extremedt-parameterplotter#make-data-request","position":23},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl4":"Submission worked ? Once our product found, we download the data.","lvl3":"Query using the DEDL HDA API","lvl2":"Print the details of the parameter (Polytope convention):"},"type":"lvl4","url":"/extremedt-parameterplotter#submission-worked-once-our-product-found-we-download-the-data","position":24},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl4":"Submission worked ? Once our product found, we download the data.","lvl3":"Query using the DEDL HDA API","lvl2":"Print the details of the parameter (Polytope convention):"},"content":"\n\n# DownloadLink is an asset representing the whole product\ndownload_url = product[\"assets\"][\"downloadLink\"][\"href\"]\nHTTP_SUCCESS_CODE = 200\nHTTP_ACCEPTED_CODE = 202\n\ndirect_download_url=''\n\nresponse = session.get(download_url, headers=auth_headers)\nif (response.status_code == HTTP_SUCCESS_CODE):\n    direct_download_url = product['assets']['downloadLink']['href']\nelif (response.status_code != HTTP_ACCEPTED_CODE):\n    print(response.text)\nprint(download_url)\nresponse.raise_for_status()\n    \n\n","type":"content","url":"/extremedt-parameterplotter#submission-worked-once-our-product-found-we-download-the-data","position":25},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl4":"Wait until data is there","lvl3":"Query using the DEDL HDA API","lvl2":"Print the details of the parameter (Polytope convention):"},"type":"lvl4","url":"/extremedt-parameterplotter#wait-until-data-is-there","position":26},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl4":"Wait until data is there","lvl3":"Query using the DEDL HDA API","lvl2":"Print the details of the parameter (Polytope convention):"},"content":"This data is not available at the moment. And we can see that our request is in queuedstatus.We will now poll the API until the data is ready and then download it.\n\nPlease note that the basic HDA quota allows a maximum of 4 requests per second. The following code limits polling to this quota.\n\npip install ratelimit --quiet\n\nfrom tqdm import tqdm\nimport time\nimport re\nfrom ratelimit import limits, sleep_and_retry\n\n# Set limit: max 4 calls per 1 seconds\nCALLS = 4\nPERIOD = 1  # seconds\n\n@sleep_and_retry\n@limits(calls=CALLS, period=PERIOD)\ndef call_api(url,auth_headers):\n    response = session.get(url, headers=auth_headers, stream=True)\n    return response\n\n# we poll as long as the data is not ready\nif direct_download_url=='':\n    while url := response.headers.get(\"Location\"):\n        print(f\"order status: {response.json()['status']}\")\n        response = call_api(url,auth_headers)\n\nif (response.status_code not in (HTTP_SUCCESS_CODE,HTTP_ACCEPTED_CODE)):\n     (print(response.text))\n\n# Check if Content-Disposition header is present\nif \"Content-Disposition\" not in response.headers:\n    print(response)\n    print(response.text)\n    raise Exception(\"Headers: \\n\"+str(response.headers)+\"\\nContent-Disposition header not found in response. Must be something wrong.\")\n        \nfilename = re.findall('filename=\\\"?(.+)\\\"?', response.headers[\"Content-Disposition\"])[0]\ntotal_size = int(response.headers.get(\"content-length\", 0))\n\nprint(f\"downloading {filename}\")\n\nwith tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n    with open(filename, 'wb') as f:\n        for data in response.iter_content(1024):\n            progress_bar.update(len(data))\n            f.write(data)\n\n","type":"content","url":"/extremedt-parameterplotter#wait-until-data-is-there","position":27},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl3":"EarthKit","lvl2":"Print the details of the parameter (Polytope convention):"},"type":"lvl3","url":"/extremedt-parameterplotter#earthkit","position":28},{"hierarchy":{"lvl1":"HDA Extreme DT Parameter Plotter - Tutorial","lvl3":"EarthKit","lvl2":"Print the details of the parameter (Polytope convention):"},"content":"Lets plot the result file\n[EarthKit Documentation] \n\nhttps://​earthkit​-data​.readthedocs​.io​/en​/latest​/index​.html\n\nThis section requires that you have ecCodes >= 2.35 installed on your system.You can follow the installation procedure at \n\nhttps://​confluence​.ecmwf​.int​/display​/ECC​/ecCodes+installation\n\nimport earthkit.data\nimport earthkit.maps\nimport earthkit.regrid\n\ndata = earthkit.data.from_source(\"file\", filename)\ndata.ls\n\nearthkit.maps.quickplot(data, #style=style\n                       )","type":"content","url":"/extremedt-parameterplotter#earthkit","position":29},{"hierarchy":{"lvl1":"Aviso notification for DT data availability"},"type":"lvl1","url":"/extremedt-dataavailability","position":0},{"hierarchy":{"lvl1":"Aviso notification for DT data availability"},"content":"🚀 Launch in JupyterHub\n\nCredit: The pyaviso package is provided by the European Centre for Medium-Range Weather Forecasts (ECMWF).\n\n","type":"content","url":"/extremedt-dataavailability","position":1},{"hierarchy":{"lvl1":"Aviso notification for DT data availability","lvl2":"Install the pyaviso package"},"type":"lvl2","url":"/extremedt-dataavailability#install-the-pyaviso-package","position":2},{"hierarchy":{"lvl1":"Aviso notification for DT data availability","lvl2":"Install the pyaviso package"},"content":"\n\n!pip  install  pyaviso --quiet\n\n","type":"content","url":"/extremedt-dataavailability#install-the-pyaviso-package","position":3},{"hierarchy":{"lvl1":"Aviso notification for DT data availability","lvl2":"Import pyaviso"},"type":"lvl2","url":"/extremedt-dataavailability#import-pyaviso","position":4},{"hierarchy":{"lvl1":"Aviso notification for DT data availability","lvl2":"Import pyaviso"},"content":"Import pyaviso and other useful libraries. Defining constants and functions.\n\nfrom datetime import datetime\nfrom pprint import pprint as pp\n\nfrom pyaviso import NotificationManager, user_config\n\nLISTENER_EVENT = \"data\"  # Event for the listener, options are mars and dissemination\nTRIGGER_TYPE = \"function\"  # Type of trigger for the listener\n\n","type":"content","url":"/extremedt-dataavailability#import-pyaviso","position":5},{"hierarchy":{"lvl1":"Aviso notification for DT data availability","lvl2":"Defining the data to be notified"},"type":"lvl2","url":"/extremedt-dataavailability#defining-the-data-to-be-notified","position":6},{"hierarchy":{"lvl1":"Aviso notification for DT data availability","lvl2":"Defining the data to be notified"},"content":"The following request describes the data whose availability we want to be notified\n\nREQUEST = {\n    \"class\": \"d1\",\n#    \"dataset\": \"extremes-dt\",\n    \"expver\": \"0001\",\n    \"stream\": \"wave\",\n    \"type\": \"fc\",\n    \"time\": \"00\",\n    \"step\": \"0\",\n    \"levtype\": \"sfc\",\n#    \"levelist\": \"\",\n#    \"param\": \"168\"   \n}  # Request configuration for the listener\n\n\n","type":"content","url":"/extremedt-dataavailability#defining-the-data-to-be-notified","position":7},{"hierarchy":{"lvl1":"Aviso notification for DT data availability","lvl2":"Aviso configuration"},"type":"lvl2","url":"/extremedt-dataavailability#aviso-configuration","position":8},{"hierarchy":{"lvl1":"Aviso notification for DT data availability","lvl2":"Aviso configuration"},"content":"\n\nCONFIG = {\n    \"notification_engine\": {\n        \"host\": \"aviso.lumi.apps.dte.destination-earth.eu\",\n        \"port\": 443,\n        \"https\": True,\n    },\n    \"configuration_engine\": {\n        \"host\": \"aviso.lumi.apps.dte.destination-earth.eu\",\n        \"port\": 443,\n        \"https\": True,\n    },\n    \"schema_parser\": \"generic\",\n    \"remote_schema\": True,\n    \"auth_type\": \"none\",\n    \"quiet\" : True\n}  # manually defined configuration\n\n\n","type":"content","url":"/extremedt-dataavailability#aviso-configuration","position":9},{"hierarchy":{"lvl1":"Aviso notification for DT data availability","lvl2":"Searching for old notifications"},"type":"lvl2","url":"/extremedt-dataavailability#searching-for-old-notifications","position":10},{"hierarchy":{"lvl1":"Aviso notification for DT data availability","lvl2":"Searching for old notifications"},"content":"Ssearching for old notifications where available. This way users can explicitly replay past notifications and executes triggers.\n\nimport sys\nSTART_DATE = datetime(2020, 12, 12)  # Start date for the notification listener\n\ndef triggered_function(notification):\n    \"\"\"\n    Function for the listener to trigger.\n    \"\"\"\n    \n    #pp(notification)\n    # Access the date field\n    date_str = notification['request']['date']    \n\n    # Convert the date string to a datetime object\n    date_obj = datetime.strptime(date_str, '%Y%m%d')\n    formatted_date = date_obj.strftime('%Y-%m-%d')\n    pp(\"ExtremeDT data available=>\" + formatted_date)\n    \n    \n\ndef create_hist_listener():\n    \"\"\"\n    Creates and returns a listener configuration.\n    \"\"\"\n\n    trigger = {\n        \"type\": TRIGGER_TYPE,\n        \"function\": triggered_function,\n    }  # Define the trigger for the listener\n    \n    # Return the complete listener configuration\n    return {\"event\": LISTENER_EVENT, \"request\": REQUEST, \"triggers\": [trigger]}\n\n\ntry:\n    listener = create_hist_listener()  # Create listener configuration\n    listeners_config = {\"listeners\": [listener]}  # Define listeners configuration\n    config = user_config.UserConfig(**CONFIG)\n    print(\"loaded config:\")\n    pp(CONFIG)\n    nmh = NotificationManager()  # Initialize the NotificationManager\n\n    nmh.listen(\n        listeners=listeners_config, from_date=START_DATE, config=config\n    )  # Start listening\nexcept Exception as e:\n    print(f\"Failed to initialize the Notification Manager: {e}\")","type":"content","url":"/extremedt-dataavailability#searching-for-old-notifications","position":11},{"hierarchy":{"lvl1":"EODAG - DestinE Data Lake Provider"},"type":"lvl1","url":"/hda-eodag-full-version","position":0},{"hierarchy":{"lvl1":"EODAG - DestinE Data Lake Provider"},"content":"🚀 Launch in JupyterHub\n\n","type":"content","url":"/hda-eodag-full-version","position":1},{"hierarchy":{"lvl1":"EODAG - DestinE Data Lake Provider","lvl3":"How to use EODAG to search and access DEDL data"},"type":"lvl3","url":"/hda-eodag-full-version#how-to-use-eodag-to-search-and-access-dedl-data","position":2},{"hierarchy":{"lvl1":"EODAG - DestinE Data Lake Provider","lvl3":"How to use EODAG to search and access DEDL data"},"content":"EODAG is a command line tool and a Python package for searching and downloading earth observation data via a unified API regardless of the data provider. Detailed information about the usage of EODAG can be found on the \n\nproject documentation page.\n\nThis notebook demonstrates how to use the DEDL provider in EODAG, using Python code.\n\nSetup: EODAG configuration to use the provider DEDL .\n\nSearch: search DEDL data, we search for Sentinel-3 data.\n\nFilter: filter DEDL data.\n\nDownload: download DEDL data.\n\nThe complete guide on how to use EODAG Python API is available via \n\nhttps://​eodag​.readthedocs​.io​/en​/stable​/api​_user​_guide​.html.\n\nPrequisites: For search and download dedl products : \n\nDestinE user account\n\nNote:\n\nPlease note that the two factor authentication (2FA) is still not implemented in EODAG. The users who have enabled 2FA on DESP will not be able to run this notebook.\n\n","type":"content","url":"/hda-eodag-full-version#how-to-use-eodag-to-search-and-access-dedl-data","position":3},{"hierarchy":{"lvl1":"EODAG - DestinE Data Lake Provider","lvl2":"Setup"},"type":"lvl2","url":"/hda-eodag-full-version#setup","position":4},{"hierarchy":{"lvl1":"EODAG - DestinE Data Lake Provider","lvl2":"Setup"},"content":"In this section, we set:\n\nThe output_dir, the directory where to store downloaded products.\n\nThe DEDL credentials, you’ll be asked to enter your DEDL credentials.\n\nThe search timeout, it is of 60 seconds to avoid any unexpected errors because of long running search queries.\n\nimport os\nfrom getpass import getpass\n\nworkspace = 'eodag_workspace'\nif not os.path.isdir(workspace):\n    os.mkdir(workspace)\n    \nos.environ[\"EODAG__DEDL__DOWNLOAD__OUTPUT_DIR\"] = os.path.abspath(workspace)\n#os.environ[\"EODAG__DEDL__DOWNLOAD__OUTPUTS_PREFIX\"] = os.path.abspath(workspace)\n\nos.environ[\"EODAG__DEDL__PRIORITY\"]=\"10\"\nos.environ[\"EODAG__DEDL__SEARCH__TIMEOUT\"]=\"60\"\n\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nos.environ[\"EODAG__DEDL__AUTH__CREDENTIALS__USERNAME\"]=DESP_USERNAME\nos.environ[\"EODAG__DEDL__AUTH__CREDENTIALS__PASSWORD\"]=DESP_PASSWORD\n\n\n","type":"content","url":"/hda-eodag-full-version#setup","position":5},{"hierarchy":{"lvl1":"EODAG - DestinE Data Lake Provider","lvl3":"Import EODAG and list available products on DEDL","lvl2":"Setup"},"type":"lvl3","url":"/hda-eodag-full-version#import-eodag-and-list-available-products-on-dedl","position":6},{"hierarchy":{"lvl1":"EODAG - DestinE Data Lake Provider","lvl3":"Import EODAG and list available products on DEDL","lvl2":"Setup"},"content":"We now need to import the EODataAccessGateway class. The class is going to take care of  all the following operations.\n\nWe can start listing the products available using dedl as provider.\n\n\nfrom eodag import EODataAccessGateway\ndag = EODataAccessGateway()\n\n[product_type[\"ID\"] + \", \" + product_type[\"title\"] for product_type in dag.list_product_types(\"dedl\")]\n\n","type":"content","url":"/hda-eodag-full-version#import-eodag-and-list-available-products-on-dedl","position":7},{"hierarchy":{"lvl1":"EODAG - DestinE Data Lake Provider","lvl2":"Search"},"type":"lvl2","url":"/hda-eodag-full-version#search","position":8},{"hierarchy":{"lvl1":"EODAG - DestinE Data Lake Provider","lvl2":"Search"},"content":"To search we use the search method passing the ID of our dataset of interest and a geo-time filter.\n\nThe search method returns a SearchResult object that stores the products obtained from a given page (default: page=1) and a given maximum number of items per page (default: items_per_page=20). The search_all() method can be used instead.\n\nIn the following cell, we change the default value of items_per_page and define the search criteria to retrieve Sentinel-2 MSI Level-2 images over Sicily, first days of July 2024. Our goal is to check whether any effects of Mount Etna’s eruptions during that period are visible in the Sentinel-2 imagery.\n\nsearch_criteria = {\n    \"provider\":\"dedl\",\n    \"productType\": \"EO.ESA.DAT.SENTINEL-2.MSI.L2A\",\n    \"start\": \"2024-07-04T07:00:00.00Z\",\n    \"end\": \"2024-07-08T07:00:00.00Z\",\n    \"geom\": {\"lonmin\": 12, \"latmin\": 37, \"lonmax\": 16, \"latmax\": 39},\n    \"count\": True,\n    \"items_per_page\": 50\n}\n\nproducts_first_page = dag.search(**search_criteria)\n\nResults are stored in a ‘SearchResult’ object that contains the details on the single search result.\n\nproducts_first_page\n\nIt is possible to list the metadata associated with a certain product, we choose the first one returned [0], and look into it.\n\none_product = products_first_page[0]\none_product.properties.keys()\n\none_product.properties['cloudCover']\n\n","type":"content","url":"/hda-eodag-full-version#search","position":9},{"hierarchy":{"lvl1":"EODAG - DestinE Data Lake Provider","lvl2":"Filter"},"type":"lvl2","url":"/hda-eodag-full-version#filter","position":10},{"hierarchy":{"lvl1":"EODAG - DestinE Data Lake Provider","lvl2":"Filter"},"content":"EODAG can filter the search result. We can then refine our initial search without asking the provider again.\nProducts can be filtered according to their properties or also with finer geometry filters.\n\nThe following example shows how to filter products to keep only those whose cloud coverage is less than 20%. And then restrict the results to products containing a smaller area over the mount Etna.\n\nLet’s define now a smaller area around the mount Etna and a function to see the area on a map together with the results\n\nfrom eodag.crunch import FilterProperty\nfrom eodag.crunch import FilterOverlap\nimport shapely\nimport folium\nfrom shapely.geometry import Polygon\n\nsmall_geom = Polygon([[15.1, 37.7], [15.5, 37.7], [15.1, 37.75], [15.1, 37.75], [15.1, 37.7]])\n\nsmaller_area = {\"lonmin\": 15.1, \"latmin\": 37.7, \"lonmax\": 15.5, \"latmax\": 37.75}\n\nsearch_geometry = shapely.geometry.box(\n    smaller_area[\"lonmin\"],\n    smaller_area[\"latmin\"],\n    smaller_area[\"lonmax\"],\n    smaller_area[\"latmax\"],\n)\n\n\ndef create_search_result_map(search_results, extent):\n    \"\"\"Small utility to create an interactive map with folium\n    that displays an extent in red and EO Producs in blue\"\"\"\n    fmap = folium.Map([38, 14], zoom_start=7)\n    folium.GeoJson(\n        extent,\n        style_function=lambda x: dict(color=\"red\")\n    ).add_to(fmap)\n    folium.GeoJson(\n        search_results\n    ).add_to(fmap)\n    return fmap\n\n# Crunch the results\nfiltered_results = products_first_page.crunch(FilterProperty({\"cloudCover\": 20, \"operator\" : \"lt\"}))\n\nprint(f\"Got now {len(filtered_results)} products after filtering by cloudCover.\")\n\nfiltered_products = filtered_results.crunch(\n    FilterOverlap(dict(contains=True)),\n    geometry=small_geom\n)\nprint(f\"Got now {len(filtered_products)} products after filtering by geometry.\")\n\nLet’s use the function defined to see the area defined on a map (red) together with the initial results (blue) filtered by cloud coverage and geometry (green).\n\nfmap = create_search_result_map(products_first_page, search_geometry)\n# Create a layer that represents the filtered products in green\nfolium.GeoJson(\n    filtered_products,\n    style_function=lambda x: dict(color=\"green\")\n).add_to(fmap)\nfmap\n\n\n","type":"content","url":"/hda-eodag-full-version#filter","position":11},{"hierarchy":{"lvl1":"EODAG - DestinE Data Lake Provider","lvl2":"Download"},"type":"lvl2","url":"/hda-eodag-full-version#download","position":12},{"hierarchy":{"lvl1":"EODAG - DestinE Data Lake Provider","lvl2":"Download"},"content":"Before downloading any product, it can be useful to have a quick look at them.\n\nimport matplotlib.pyplot as plt\nimport matplotlib.image as mpimg\n\nquicklooks_dir = os.path.join(workspace, \"quicklooks\")\nif not os.path.isdir(quicklooks_dir):\n    os.mkdir(quicklooks_dir)\n\n\n\nfig = plt.figure(figsize=(20, 40))\nfor i, product in enumerate(filtered_products, start=1):\n\n    # This line takes care of downloading the quicklook\n    quicklook_path = product.get_quicklook()\n    \n    img = mpimg.imread(quicklook_path)\n    ax = fig.add_subplot(8, 2, i)\n    ax.set_title(product.properties['dedl:beginningDateTime'] + \"TILE: \" +product.properties['dedl:tileIdentifier'])\n    plt.imshow(img)\nplt.tight_layout()\n\nThe quicklook shows effectively the ash plume caused by the eruptions.\n\nEOProducts can be downloaded individually. The last image is going to be downloaded.\n\nproduct_to_download = filtered_products[-1]\nproduct_path = dag.download(product_to_download)\nproduct_path\n\nThe location property of this product now points to a local path.\n\nproduct_to_download.location","type":"content","url":"/hda-eodag-full-version#download","position":13},{"hierarchy":{"lvl1":"EODAG - A quick start with DEDL"},"type":"lvl1","url":"/hda-eodag-quick-start","position":0},{"hierarchy":{"lvl1":"EODAG - A quick start with DEDL"},"content":"🚀 Launch in JupyterHub\n\n","type":"content","url":"/hda-eodag-quick-start","position":1},{"hierarchy":{"lvl1":"EODAG - A quick start with DEDL","lvl3":"How to use EODAG to search and access DEDL data - quick start"},"type":"lvl3","url":"/hda-eodag-quick-start#how-to-use-eodag-to-search-and-access-dedl-data-quick-start","position":2},{"hierarchy":{"lvl1":"EODAG - A quick start with DEDL","lvl3":"How to use EODAG to search and access DEDL data - quick start"},"content":"EODAG is a command line tool and a Python package for searching and downloading earth observation data via a unified API.\n\nThis quickstart is to help get DEDL data using EODAG. Detailed information about the usage of EODAG can be found on the \n\nproject documentation page.\n\nThroughout this quickstart notebook, you will learn:\n\nSetup: How to configure EODAG to use the provider DEDL.\n\nDiscover: How to discover DEDL datasets through EODAG.\n\nSearch products:  How to search DEDL data through EODAG.\n\nDownload products: How to download DEDL data through EODAG.\n\nThe complete guide on how to use EODAG Python API is available via \n\nhttps://​eodag​.readthedocs​.io​/en​/stable​/api​_user​_guide​.html.\n\nPrequisites: Search and download dedl products : \n\nDestinE user account\n\nNote:\n\nPlease note that the two factor authentication (2FA) is still not implemented in EODAG. The users who have enabled 2FA on DESP will not be able to run this notebook.\n\n","type":"content","url":"/hda-eodag-quick-start#how-to-use-eodag-to-search-and-access-dedl-data-quick-start","position":3},{"hierarchy":{"lvl1":"EODAG - A quick start with DEDL","lvl2":"Setup"},"type":"lvl2","url":"/hda-eodag-quick-start#setup","position":4},{"hierarchy":{"lvl1":"EODAG - A quick start with DEDL","lvl2":"Setup"},"content":"In this section, we set:\n\nThe output_dir, the directory where to store downloaded products.\n\nThe DEDL credentials, you’ll be asked to enter your DEDL credentials.\n\nThe search timeout, it is of 60 seconds to avoid any unexpected errors because of long running search queries.\n\nimport os\nfrom getpass import getpass\n\nworkspace = 'eodag_workspace'\nif not os.path.isdir(workspace):\n    os.mkdir(workspace)\n    \nos.environ[\"EODAG__DEDL__DOWNLOAD__OUTPUTS_PREFIX\"] = os.path.abspath(workspace)\n\nos.environ[\"EODAG__DEDL__SEARCH__TIMEOUT\"]=\"60\"\nos.environ[\"DEFAULT_STREAM_REQUESTS_TIMEOUT\"] = \"15\"\nos.environ[\"EODAG__DEDL__PRIORITY\"]=\"10\"\n\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nos.environ[\"EODAG__DEDL__AUTH__CREDENTIALS__USERNAME\"]=DESP_USERNAME\nos.environ[\"EODAG__DEDL__AUTH__CREDENTIALS__PASSWORD\"]=DESP_PASSWORD\n\n\n","type":"content","url":"/hda-eodag-quick-start#setup","position":5},{"hierarchy":{"lvl1":"EODAG - A quick start with DEDL","lvl2":"EODiscover"},"type":"lvl2","url":"/hda-eodag-quick-start#eodiscover","position":6},{"hierarchy":{"lvl1":"EODAG - A quick start with DEDL","lvl2":"EODiscover"},"content":"We now need to import the EODataAccessGateway class in order to discover the available DEDL collections.\n\nCollections are presented in a dropdown menu, selecting a collection its description will be prompted.\n\n\nfrom eodag import EODataAccessGateway, setup_logging\ndag = EODataAccessGateway()\n\nsetup_logging(1)\n\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output, HTML\nfrom ipywidgets import Layout, Box\nimport json\n\n#default values\n\nDATASET_ID = 'EO.EUM.DAT.SENTINEL-3.OL_2_WFR___'\n\n\n# Event listeners\ndef on_change(change):\n    with output_area:\n        clear_output()\n        print(f'Selected: {change[\"new\"]}')\n        print('---------------------------------------------')\n        delimiter=''\n        global DATASET_ID\n        DATASET_ID = delimiter.join(change[\"new\"])\n        product_types=dag.list_product_types(\"dedl\")\n        index = next((i for i, d in enumerate(product_types) if d.get('ID') == DATASET_ID), None)\n        \n        print(\"TITLE: \"+product_types[index]['title'])\n        print(\"ABSTRACT: \"+product_types[index]['abstract'])\n        print(\"KEYWORDS: \"+product_types[index]['abstract'])\n\noptions=[product_type[\"ID\"] for product_type in dag.list_product_types(\"dedl\")]\n\n# Widgets\noutput_area = widgets.Output()\n\ndropdown = widgets.Dropdown(\n    options=options,\n    value=options[0],\n    description=\"Datasets:\",\n    disabled=False,\n) \n\ndropdown.observe(on_change, names='value')\n\n\n# Layout\n# Define the layout for the dropdown\ndropdown_layout = Layout(display='space-between', justify_content='center', width='80%')\n# Create a box to hold the dropdown with the specified layout\nbox = Box([dropdown, output_area], layout=dropdown_layout)\ndisplay( box)  \n\n\n","type":"content","url":"/hda-eodag-quick-start#eodiscover","position":7},{"hierarchy":{"lvl1":"EODAG - A quick start with DEDL","lvl2":"EOSearch"},"type":"lvl2","url":"/hda-eodag-quick-start#eosearch","position":8},{"hierarchy":{"lvl1":"EODAG - A quick start with DEDL","lvl2":"EOSearch"},"content":"Once we selected our dataset of interest, we can define the search criteria to find data inside the chosen dataset.\n\nSelect bbox, start and end date to use in the search:\n\n# Import necessary libraries\nimport folium, datetime\nfrom ipywidgets import interactive, HBox, VBox, FloatText, Button, Label\n\n#default values\n\nSTART_DATE = '2024-08-11'\nEND_DATE = '2024-08-13'\n\nsw_lat = 37.0\nsw_lng = 14.0\nne_lat = 38.0\nne_lng = 16.0\n\n#functions to handle the chosen values\ndef on_date_change(change):\n    with output_area:\n        clear_output()\n        global START_DATE\n        global END_DATE\n        t1 = start_date.value\n        t2 = end_date.value\n        START_DATE = t1.strftime('%Y-%m-%d')\n        END_DATE = t2.strftime('%Y-%m-%d')\n\n        \ndef update_map(sw_lat, sw_lng, ne_lat, ne_lng):\n    # Clear the map\n    m = folium.Map(location=[(sw_lat + ne_lat) / 2, (sw_lng + ne_lng) / 2], zoom_start=4, tiles=None)\n\n    nasa_wms = folium.WmsTileLayer(\n        url='https://gibs.earthdata.nasa.gov/wms/epsg4326/best/wms.cgi',\n        name='NASA Blue Marble',\n        layers='BlueMarble_ShadedRelief',\n        format='image/png',\n        transparent=True,\n        attr='NASA'\n    )\n    nasa_wms.add_to(m)\n    \n    # Add the bounding box rectangle\n    folium.Rectangle(\n        bounds=[[sw_lat, sw_lng], [ne_lat, ne_lng]],\n        color=\"#ff7800\",\n        fill=True,\n        fill_opacity=0.3\n    ).add_to(m)\n    \n    # Display the map\n    display(m)\n    return (sw_lat, sw_lng, ne_lat, ne_lng)\n\ndef save_bbox(button):\n    global sw_lat \n    global sw_lng  \n    global ne_lat \n    global ne_lng \n\n    sw_lat = sw_lat_input.value\n    sw_lng = sw_lng_input.value\n    ne_lat = ne_lat_input.value\n    ne_lng = ne_lng_input.value\n    \n    with output:\n        clear_output()\n        print(f\"BBox: sw_lat {sw_lat} sw_lng {sw_lng} ne_lat {ne_lat} ne_lng {ne_lng}\")\n\n\n# Create a base map\nm = folium.Map(location=[40, 20], zoom_start=4)\n        \n        \n# Widgets\noutput = widgets.Output()\n\n# Widgets for bbox coordinates input\nsw_lat_input = FloatText(value=sw_lat, description='SW Latitude:')\nsw_lng_input = FloatText(value=sw_lng, description='SW Longitude:')\nne_lat_input = FloatText(value=ne_lat, description='NE Latitude:')\nne_lng_input = FloatText(value=ne_lng, description='NE Longitude:')\n\nsave_button = Button(description=\"Save BBox\")\n\n\n# Create DatePicker widgets\nstart_date = widgets.DatePicker(\n    description='Start Date',value = datetime.date(2024,8,11),\n    disabled=False\n)\n\nend_date = widgets.DatePicker(\n    description='End Date',value = datetime.date(2024,8,13),\n    disabled=False\n)\n\n\nui = VBox([\n   # HBox([sw_lat_input, sw_lng_input]),\n  #  HBox([ne_lat_input, ne_lng_input]),\n    save_button,\n    output,\n    start_date, \n    end_date\n])\n\n# Display the interactive map and UI\ninteractive_map = interactive(update_map, sw_lat=sw_lat_input, sw_lng=sw_lng_input,\n                              ne_lat=ne_lat_input, ne_lng=ne_lng_input)\n\n\ndisplay(interactive_map, ui)\n\n     \n#Events\n\nstart_date.observe(on_date_change, names='value')\nend_date.observe(on_date_change, names='value')\nsave_button.on_click(save_bbox)\n\nSelected criteria:\n\nsearch_criteria = {\n    \"productType\": DATASET_ID,\n    \"start\": START_DATE,\n    \"end\": END_DATE,\n    \"geom\": {\"lonmin\": sw_lng, \"latmin\": sw_lat, \"lonmax\": ne_lng, \"latmax\": ne_lat},\n    \"count\": True\n}\n\nprint(json.dumps(search_criteria, indent=2))\n\nUse selected criteria to search data:\n\nproducts_first_page = dag.search(**search_criteria)\nprint(f\"Got {len(products_first_page)} products and an estimated total number of {products_first_page.number_matched} products.\")\nproducts_first_page\n\nSee the available metadata:\n\n\nif(len(products_first_page)>0):\n    one_product = products_first_page[0]\n    print(one_product.properties.keys())\n\n","type":"content","url":"/hda-eodag-quick-start#eosearch","position":9},{"hierarchy":{"lvl1":"EODAG - A quick start with DEDL","lvl2":"EODownload"},"type":"lvl2","url":"/hda-eodag-quick-start#eodownload","position":10},{"hierarchy":{"lvl1":"EODAG - A quick start with DEDL","lvl2":"EODownload"},"content":"EOProducts can be downloaded individually. The last image is going to be downloaded.\n\nif(len(products_first_page)>0):\n    product_to_download = one_product\n    product_path = dag.download(product_to_download)\n    print(product_path)","type":"content","url":"/hda-eodag-quick-start#eodownload","position":11},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access"},"type":"lvl1","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1","position":0},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access"},"content":"🚀 Launch in JupyterHub\n\nThe Advanced Very High Resolution Radiometer (AVHRR) operates at 5 different channels simultaneously in the visible and infrared bands. Channel 3 switches between 3a and 3b for daytime and nighttime. As a high-resolution imager (about 1.1 km near nadir) its main purpose is to provide cloud and surface information such as cloud coverage, cloud top temperature, surface temperature over land and sea, and vegetation or snow/ice.\n\nDestinE Data Lake HDA\n\nAVHRR Level 1B - Metop - Global\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1","position":1},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl3":"How to access and visualize AVHRR Level 1B Metop Global"},"type":"lvl3","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#how-to-access-and-visualize-avhrr-level-1b-metop-global","position":2},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl3":"How to access and visualize AVHRR Level 1B Metop Global"},"content":"This notebook demonstrates how to search and access Metop data using HDA and how to read, process and visualize it using satpy\n\nThroughout this notebook, you will learn:\n\nAuthenticate: How to authenticate for searching and access DEDL collections.\n\nSearch Metop AVHRR data:  How to search DEDL data using datetime and bbox filters.\n\nDownload Metop AVHRR data: How to download DEDL data through HDA.\n\nRead and visualize Metop AVHRR data: How to load process and visualize Metop AVHRR data using Satpy.\n\nPrerequisites:\n\nFor filtering data inside collections : \n\nDestinE user account\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#how-to-access-and-visualize-avhrr-level-1b-metop-global","position":3},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl2":"Authenticate"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#authenticate","position":4},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl2":"Authenticate"},"content":"\n\nimport destinelab as deauth\n\nimport requests\nimport json\nimport os\nfrom getpass import getpass\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#authenticate","position":5},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl2":"Search Metop AVHRR data"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#search-metop-avhrr-data","position":6},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl2":"Search Metop AVHRR data"},"content":"\n\nsearch_response = requests.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n        \"BBox\":  [-5 ,31,20,51],\n    \"collections\": [\"EO.EUM.DAT.METOP.AVHRRL1\"],\n    \"datetime\": \"2024-07-04T11:00:00Z/2024-07-04T13:00:00Z\"\n})\n\n\nThe first item in the search results\n\nfrom IPython.display import JSON\n\nJSON(search_response.json()[\"features\"][0])\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#search-metop-avhrr-data","position":7},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl2":"Download Metop AVHRR data"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#download-metop-avhrr-data","position":8},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl2":"Download Metop AVHRR data"},"content":"We can download now the returned data.\n\nfrom tqdm import tqdm\nimport time\nimport zipfile\n\n#number of products to download:\nnptd=1\n\n# Define a list of assets to download\nfor i in range(0,nptd,1):\n    product=search_response.json()[\"features\"][i]\n    download_url = product[\"assets\"][\"downloadLink\"][\"href\"]\n    print(download_url)\n    filename = \"downloadLink\"\n    response = requests.get(download_url, headers=auth_headers)\n    total_size = int(response.headers.get(\"content-length\", 0))\n\n    print(f\"downloading {filename}\")\n\n    with tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n        with open(filename, 'wb') as f:\n            for data in response.iter_content(1024):\n                progress_bar.update(len(data))\n                f.write(data)\n        \n    zf=zipfile.ZipFile(filename)\n    with zipfile.ZipFile(filename, 'r') as zip_ref:\n        zip_ref.extractall('.')\n\ndel response\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#download-metop-avhrr-data","position":9},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#read-and-visualize-metop-avhrr-data-using-satpy","position":10},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"content":"\n\nThe Python package satpy supports reading and loading data from many input files.\nFor Metop data in the native format, we can use the satpy reader ‘avhrr_l1b_eps’.\n\npip install --quiet satpy pyspectral\n\nimport os\nfrom glob import glob\n\nimport xarray as xr\nimport numpy as np\n\nimport matplotlib.pyplot as plt\nimport matplotlib.colors\nfrom matplotlib.axes import Axes\n\nimport satpy\nfrom satpy.scene import Scene\nfrom satpy.composites import GenericCompositor\nfrom satpy.writers import to_image\nfrom satpy.resample import get_area_def\nfrom satpy import available_readers\nfrom satpy import MultiScene\n\nimport pyresample\nimport pyspectral\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(action = \"ignore\", category = RuntimeWarning)\n\nsatpy_installation_path=satpy.__path__\ndelimiter = \"\" \nsatpy_installation_path = delimiter.join(satpy_installation_path)\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#read-and-visualize-metop-avhrr-data-using-satpy","position":11},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl3":"Read and load data","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"type":"lvl3","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#read-and-load-data","position":12},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl3":"Read and load data","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#read-and-load-data","position":13},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl4":"Single scene","lvl3":"Read and load data","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"type":"lvl4","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#single-scene","position":14},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl4":"Single scene","lvl3":"Read and load data","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"content":"We can use the Scene constructor from the satpy library, a Scene object represents a single geographic region of data.\nOnce loaded we can list all the available bands (spectral channel) for that scene.\n\nfilenames = glob('./AVHR_xxx_1B_M0*.nat')\n#len(filenames)\n\n# read the last file in filenames\nscn = Scene(reader='avhrr_l1b_eps', filenames=[filenames[-1]])\n# print available datasets\nscn.available_dataset_names()\n\nWe can then load the first and the second spectral channels and have a look to some info\n\n# load  \nscn.load(['1','2'])\nscn['1']\n\nscn['1'].attrs['wavelength']\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#single-scene","position":15},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl4":"Do some calculation","lvl3":"Read and load data","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"type":"lvl4","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#do-some-calculation","position":16},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl4":"Do some calculation","lvl3":"Read and load data","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"content":"Calculations based on loaded datasets/channels can easily be assigned to a new dataset.\n\nWe resample the scene in a smaller area over the Spain and use the 2 loaded datasets to calculate a new dataset.\n\nnewscn = scn.resample('spain')\n\nnewscn[\"ndvi\"] = (newscn['2'] - newscn['1']) / (newscn['2'] + newscn['1'])\n#scn.show(\"ndvi\")\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#do-some-calculation","position":17},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl3":"Visualize datasets","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"type":"lvl3","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#visualize-datasets","position":18},{"hierarchy":{"lvl1":"AVHRR Level 1B Metop Global - Data Access","lvl3":"Visualize datasets","lvl2":"Read and visualize Metop AVHRR data using Satpy"},"content":"\n\nimport matplotlib.pyplot as plt\nfrom pyresample.kd_tree import resample_nearest\nfrom pyresample.geometry import AreaDefinition\nfrom pyresample import load_area\n\n\n\n\narea_def = load_area(satpy_installation_path+'/etc/areas.yaml', 'spain') \n#scene \nlons, lats = newscn[\"1\"].area.get_lonlats()\nswath_def = pyresample.geometry.SwathDefinition(lons, lats)\nndvi = newscn[\"ndvi\"].data.compute()\nresult = resample_nearest(swath_def, ndvi, area_def, radius_of_influence=20000, fill_value=None)\n\n#cartopy\ncrs = area_def.to_cartopy_crs()\nfig, ax = plt.subplots(subplot_kw=dict(projection=crs))\ncoastlines = ax.coastlines()  \nax.set_global()\n\n#plot\nimg = plt.imshow(result, transform=crs, extent=crs.bounds, origin='upper')\ncbar = plt.colorbar()","type":"content","url":"/dedl-hda-eo-eum-dat-metop-avhrrl1#visualize-datasets","position":19},{"hierarchy":{"lvl1":"title: “High rate SEVIRI Level 1.5 Image Data MSG - Data Access”\nsubtitle: “This notebook demonstrates how to access SEVIRI data using HDA and how to read, process and visualize it.”\nauthor: “Author: Eumetsat”\ntags: [HDA, SEVIRI]\nthumbnail: ../../img/EUMETSAT-icon.png\nlicense: MIT\ncopyright: “© 2024 EUMETSAT”"},"type":"lvl1","url":"/dedl-hda-eo-eum-dat-msg-1-5","position":0},{"hierarchy":{"lvl1":"title: “High rate SEVIRI Level 1.5 Image Data MSG - Data Access”\nsubtitle: “This notebook demonstrates how to access SEVIRI data using HDA and how to read, process and visualize it.”\nauthor: “Author: Eumetsat”\ntags: [HDA, SEVIRI]\nthumbnail: ../../img/EUMETSAT-icon.png\nlicense: MIT\ncopyright: “© 2024 EUMETSAT”"},"content":"","type":"content","url":"/dedl-hda-eo-eum-dat-msg-1-5","position":1},{"hierarchy":{"lvl1":"title: “High rate SEVIRI Level 1.5 Image Data MSG - Data Access”\nsubtitle: “This notebook demonstrates how to access SEVIRI data using HDA and how to read, process and visualize it.”\nauthor: “Author: Eumetsat”\ntags: [HDA, SEVIRI]\nthumbnail: ../../img/EUMETSAT-icon.png\nlicense: MIT\ncopyright: “© 2024 EUMETSAT”"},"type":"lvl1","url":"/dedl-hda-eo-eum-dat-msg-1-5#title-high-rate-seviri-level-1-5-image-data-msg-data-access-subtitle-this-notebook-demonstrates-how-to-access-seviri-data-using-hda-and-how-to-read-process-and-visualize-it-author-author-eumetsat-tags-hda-seviri-thumbnail-img-eumetsat-icon-png-license-mit-copyright-2024-eumetsat","position":2},{"hierarchy":{"lvl1":"title: “High rate SEVIRI Level 1.5 Image Data MSG - Data Access”\nsubtitle: “This notebook demonstrates how to access SEVIRI data using HDA and how to read, process and visualize it.”\nauthor: “Author: Eumetsat”\ntags: [HDA, SEVIRI]\nthumbnail: ../../img/EUMETSAT-icon.png\nlicense: MIT\ncopyright: “© 2024 EUMETSAT”"},"content":"🚀 Launch in JupyterHub\n\nThe Spinning Enhanced Visible and InfraRed Imager (SEVIRI) is MSG’s primary instrument and has the capacity to observe the Earth in 12 spectral channels.\n\nDocumentation DestinE Data Lake HDA\ns\n\n\nHigh Rate SEVIRI Level 1.5 Image Data - MSG - 0 degree\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-msg-1-5#title-high-rate-seviri-level-1-5-image-data-msg-data-access-subtitle-this-notebook-demonstrates-how-to-access-seviri-data-using-hda-and-how-to-read-process-and-visualize-it-author-author-eumetsat-tags-hda-seviri-thumbnail-img-eumetsat-icon-png-license-mit-copyright-2024-eumetsat","position":3},{"hierarchy":{"lvl1":"Authenticate on DESP"},"type":"lvl1","url":"/dedl-hda-eo-eum-dat-msg-1-5#authenticate-on-desp","position":4},{"hierarchy":{"lvl1":"Authenticate on DESP"},"content":"\n\nimport destinelab as deauth\n\nimport requests\nimport json\nimport os\nimport zipfile\nimport datetime\nimport shutil\nfrom getpass import getpass\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-msg-1-5#authenticate-on-desp","position":5},{"hierarchy":{"lvl1":"Search MSG data"},"type":"lvl1","url":"/dedl-hda-eo-eum-dat-msg-1-5#search-msg-data","position":6},{"hierarchy":{"lvl1":"Search MSG data"},"content":"\n\nresponse = requests.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n    \"collections\": [\"EO.EUM.DAT.MSG.HRSEVIRI\"],\n    \"datetime\": \"2023-08-06T08:00:00Z/2023-08-07T00:00:00Z\"\n})\n\n\nfrom IPython.display import JSON\n\nproduct = response.json()[\"features\"][0]\nJSON(product)\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-msg-1-5#search-msg-data","position":7},{"hierarchy":{"lvl1":"Search MSG data","lvl2":"Download MSG"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-msg-1-5#download-msg","position":8},{"hierarchy":{"lvl1":"Search MSG data","lvl2":"Download MSG"},"content":"\n\nfrom tqdm import tqdm\nimport time\n\n# Define a list of assets to download\n#assets = [\"Oa08_radiance.nc\", \"Oa06_radiance.nc\", \"Oa02_radiance.nc\"]\nassets = [\"downloadLink\"]\n\nfor asset in assets:\n    download_url = product[\"assets\"][asset][\"href\"]\n    print(download_url)\n    filename = asset\n    print(filename)\n    response = requests.get(download_url, headers=auth_headers)\n    total_size = int(response.headers.get(\"content-length\", 0))\n\n    print(f\"downloading {filename}\")\n\n    with tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n        with open(filename, 'wb') as f:\n            for data in response.iter_content(1024):\n                progress_bar.update(len(data))\n                f.write(data)\n\nzf=zipfile.ZipFile(filename)\nwith zipfile.ZipFile(filename, 'r') as zip_ref:\n    zip_ref.extractall('.')\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-msg-1-5#download-msg","position":9},{"hierarchy":{"lvl1":"satpy"},"type":"lvl1","url":"/dedl-hda-eo-eum-dat-msg-1-5#satpy","position":10},{"hierarchy":{"lvl1":"satpy"},"content":"\n\nThe Python package satpy supports reading and loading data from many input files.\nFor MSG data and the Native format, we can use the satpy reader 'seviri_l1b_native.\n\npip install --quiet satpy\n\nImport required libraries\n\nfrom satpy.scene import Scene\nfrom satpy.composites import GenericCompositor\nfrom satpy.writers import to_image\nfrom satpy.resample import get_area_def\nfrom satpy import available_readers\n\nimport warnings\nwarnings.filterwarnings('ignore')\nwarnings.simplefilter(action = \"ignore\", category = RuntimeWarning)\n\nfile='MSG3-SEVI-MSG15-0100-NA-20230806081241.839000000Z-NA.nat'\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-msg-1-5#satpy","position":11},{"hierarchy":{"lvl1":"satpy","lvl2":"Read and load data"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-msg-1-5#read-and-load-data","position":12},{"hierarchy":{"lvl1":"satpy","lvl2":"Read and load data"},"content":"\n\nWe use the Scene constructor from the satpy library, a Scene object represents a single geographic region of data.\nOnce loaded we can list all the available bands (spectral channel) for that scene.\n\n# read the file\nscn = Scene(reader='seviri_l1b_native', filenames=[file])\n# print available datasets\nscn.available_dataset_names()\n\nWith the function load(), you can specify an individual band by name. If you then select the loaded band, you see the xarray.DataArray band object\n\n# load bands \nscn.load(['HRV','IR_108'])\nscn['IR_108']\n\nscn['IR_108'].attrs.keys()\n\nscn['IR_108'].attrs['wavelength']\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-msg-1-5#read-and-load-data","position":13},{"hierarchy":{"lvl1":"satpy","lvl2":"Visualize data combining bands"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-msg-1-5#visualize-data-combining-bands","position":14},{"hierarchy":{"lvl1":"satpy","lvl2":"Visualize data combining bands"},"content":"\n\nThe Satpy function available_composite_ids()  returns a list of available composite IDs.\n\nscn = Scene(reader=\"seviri_l1b_native\", \n             filenames=[file])\n\nscn.available_composite_ids()\n\ncomposite_id = [\"natural_color\"]\nscn.load(composite_id, upper_right_corner=\"NE\")\n\n\nscn_cropped = scn.crop(ll_bbox=(-5, 31, 20, 51))\nscn_cropped.show(\"natural_color\")\n\n\ncomposite_id = [\"colorized_ir_clouds\"]\nscn.load(composite_id, upper_right_corner=\"NE\")\n\nscn_cropped = scn.crop(ll_bbox=(-5, 31, 20, 51))\nscn_cropped.show(\"colorized_ir_clouds\")","type":"content","url":"/dedl-hda-eo-eum-dat-msg-1-5#visualize-data-combining-bands","position":15},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3"},"type":"lvl1","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err","position":0},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3"},"content":"🚀 Launch in JupyterHub\n\nDocumentation DestinE Data Lake HDA\n\nOLCI Level 1B Reduced Resolution - Sentinel-3\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err","position":1},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3","lvl3":"How to access and visualize OLCI Level 1B Reduced Resolution - Sentinel-3"},"type":"lvl3","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#how-to-access-and-visualize-olci-level-1b-reduced-resolution-sentinel-3","position":2},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3","lvl3":"How to access and visualize OLCI Level 1B Reduced Resolution - Sentinel-3"},"content":"This notebook demonstrates how to search and access Sentinel-3 data using HDA and how to read and visualize it using satpy\n\nThroughout this notebook, you will learn:\n\nAuthenticate: How to authenticate for searching and access DEDL collections.\n\nSearch OLCI data:  How to search DEDL data using datetime and bbox filters.\n\nDownload OLCI data: How to download DEDL data through HDA.\n\nRead and visualize OLCI data: How to load process and visualize OlCI data using Satpy.\n\nPrerequisites:\n\nFor filtering data inside collections : \n\nDestinE user account\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#how-to-access-and-visualize-olci-level-1b-reduced-resolution-sentinel-3","position":3},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3","lvl2":"Authenticate"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#authenticate","position":4},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3","lvl2":"Authenticate"},"content":"We start off by importing the relevant modules for DestnE authentication, HTTP requests, json handling.\nThen we authenticate in DestinE.\n\nimport destinelab as deauth\n\nimport requests\nimport json\nimport os\nfrom getpass import getpass\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#authenticate","position":5},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3","lvl2":"Search"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#search","position":6},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3","lvl2":"Search"},"content":"\n\nOnce authenticated, we search a product matching our filters.\n\nFor this example, we search data for the \n\nOLCI Level 1B Reduced Resolution - Sentinel-3 dataset.\n\nThe corresponding collection ID in HDA for this dataset is: EO.EUM.DAT.SENTINEL-3.OL_1_ERR___.\n\nresponse = requests.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json={\n    \"collections\": [\"EO.EUM.DAT.SENTINEL-3.OL_1_ERR___\"],\n    \"datetime\": \"2024-06-25T00:00:00Z/2024-06-30T00:00:00Z\",\n    \"bbox\":  [10,53,30,66]\n})\nif(response.status_code!= 200):\n    (print(response.text))\nresponse.raise_for_status()\n\nWe can have a look at the metadata of the first products returned by the search.\n\nfrom IPython.display import JSON\n\nproduct = response.json()[\"features\"][0]\nJSON(product)\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#search","position":7},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3","lvl2":"Download"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#download","position":8},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3","lvl2":"Download"},"content":"The product metadata contains the link to download it. We can use that link to download the selected product.\nIn this case we download the first product returned by our search.\n\nfrom tqdm import tqdm\nimport time\n\nassets = [\"downloadLink\"]\n\nfor asset in assets:\n    download_url = product[\"assets\"][asset][\"href\"]\n    print(download_url)\n    filename = product[\"id\"]\n    print(filename)\n    response = requests.get(download_url, headers=auth_headers)\n    total_size = int(response.headers.get(\"content-length\", 0))\n\n    print(f\"downloading {filename}\")\n\n    with tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n        with open(filename, 'wb') as f:\n            for data in response.iter_content(1024):\n                progress_bar.update(len(data))\n                f.write(data)\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#download","position":9},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3","lvl3":"Unfold the product","lvl2":"Download"},"type":"lvl3","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#unfold-the-product","position":10},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3","lvl3":"Unfold the product","lvl2":"Download"},"content":"\n\ndel response\nimport os\nimport zipfile\n\nzf=zipfile.ZipFile(filename)\nwith zipfile.ZipFile(filename, 'r') as zip_ref:\n    zip_ref.extractall('.')\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#unfold-the-product","position":11},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3","lvl2":"Read and visualize OLCI data using Satpy"},"type":"lvl2","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#read-and-visualize-olci-data-using-satpy","position":12},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3","lvl2":"Read and visualize OLCI data using Satpy"},"content":"\n\nThe Python package satpy supports reading and loading data from many input files.\n\nBelow the installation and import of useful modules and packages.\n\npip install --quiet satpy pyspectral\n\nfrom datetime import datetime\nfrom satpy import find_files_and_readers\nfrom satpy.scene import Scene\nfrom satpy.composites import GenericCompositor\nfrom satpy.writers import to_image\nfrom satpy.resample import get_area_def\nfrom satpy import available_readers\n\nimport pyresample\nimport pyspectral\n\nimport warnings\nwarnings.filterwarnings(\"ignore\")\nwarnings.simplefilter(action = \"ignore\", category = RuntimeWarning)\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#read-and-visualize-olci-data-using-satpy","position":13},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3","lvl3":"Read data","lvl2":"Read and visualize OLCI data using Satpy"},"type":"lvl3","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#read-data","position":14},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3","lvl3":"Read data","lvl2":"Read and visualize OLCI data using Satpy"},"content":"We can read the downloaded data using the “olci_l1b” satpy reader\n\n\n\nfiles = find_files_and_readers(sensor=\"olci\",\n                               start_time=datetime(2024, 6, 25, 0, 0),\n                               end_time=datetime(2024, 6, 30, 0, 0),\n                               base_dir=\".\",\n                               reader=\"olci_l1b\")\n\nscn = Scene(filenames=files)\n# print available datasets\nscn.available_dataset_names()\n\nWe can print the available datasets for the loaded scene.\n\nWith the function load(), you can specify an individual band by name. If you then select the loaded band, you see the xarray.DataArray band object\n\n# load bands \nscn.load(['humidity','total_ozone'])\nscn['humidity']\n\n","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#read-data","position":15},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3","lvl3":"Visualize data","lvl2":"Read and visualize OLCI data using Satpy"},"type":"lvl3","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#visualize-data","position":16},{"hierarchy":{"lvl1":"OLCI Level 1B Reduced Resolution - Sentinel-3","lvl3":"Visualize data","lvl2":"Read and visualize OLCI data using Satpy"},"content":"We can visualize the available datasets on a map.\n\nimport matplotlib.pyplot as plt\nfrom pyresample.kd_tree import resample_nearest\nfrom pyresample.geometry import AreaDefinition\n\n\n#area definition\narea_id = 'worldeqc30km'\ndescription = 'World in 3km, platecarree'\nproj_id = 'eqc'\nprojection = {'proj': 'eqc', 'ellps': 'WGS84'}\nwidth = 820\nheight = 410\narea_extent = (-20037508.3428, -10018754.1714, 20037508.3428, 10018754.1714)\narea_def = AreaDefinition(area_id, description, proj_id, projection,\n                          width, height, area_extent)\n\n#scene \nlons, lats = scn[\"total_ozone\"].area.get_lonlats()\nswath_def = pyresample.geometry.SwathDefinition(lons, lats)\ntotal_ozone = scn[\"total_ozone\"].data.compute()\nresult = resample_nearest(swath_def, total_ozone, area_def, radius_of_influence=20000, fill_value=None)\n\n#cartopy\nplt.rcParams['figure.figsize'] = [15, 15]\ncrs = area_def.to_cartopy_crs()\nfig, ax = plt.subplots(subplot_kw=dict(projection=crs))\ncoastlines = ax.coastlines()  \nax.set_global()\n\n#plot\nimg = plt.imshow(result, transform=crs, extent=crs.bounds, origin='upper')\n# Calculate (height_of_image / width_of_image)\nim_ratio = result.shape[0]/result.shape[1]\n \n# Plot vertical colorbar\nplt.colorbar(fraction=0.047*im_ratio)\nplt.show()","type":"content","url":"/dedl-hda-eo-eum-dat-sentinel-3-ol-1-err#visualize-data","position":17},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1","position":0},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data"},"content":"In this notebook, we will present a simple example of how you can access data from DEDL using HDA and what you can do with it. We will demonstrate how to utilize thresholding techniques and compare the values of VV and VH polarizations to analyze urban areas. As an illustration, we will attempt to download Sentinel-1 images containing data of the urban area of Warsaw (Poland).\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1","position":1},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl2":"1. Prerequisites"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-1-prerequisites","position":2},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl2":"1. Prerequisites"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-1-prerequisites","position":3},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"1.1 DestinE account","lvl2":"1. Prerequisites"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-1-1-destine-account","position":4},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"1.1 DestinE account","lvl2":"1. Prerequisites"},"content":"Firstly, to work with HDA we will need account on DestinE Core Service Platfrom website. You can register under this url: \n\nhttps://​platform​.destine​.eu/\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-1-1-destine-account","position":5},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"1.2 Libraries","lvl2":"1. Prerequisites"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-1-2-libraries","position":6},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"1.2 Libraries","lvl2":"1. Prerequisites"},"content":"\n\nimport datetime\nimport requests\nimport numpy as np\nimport rasterio\nimport matplotlib.pyplot as plt\nimport requests\nimport zipfile\nfrom getpass import getpass\nimport io\nfrom rasterio.mask import mask\nimport os\nimport destinelab as deauth\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-1-2-libraries","position":7},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"1.3 Prerequisites data","lvl2":"1. Prerequisites"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-1-3-prerequisites-data","position":8},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"1.3 Prerequisites data","lvl2":"1. Prerequisites"},"content":"Before reuqesting some data from DEDL, let’s specify what data we want to obtain. We will define 3 variables:\n\nStart date and end date,\n\nOutput directory,\n\nGeometry of interesting us area\n\nStart date and end date will define our timerange in reuqest. HDA will search only for products that were obtained between those two dates.\n\nOutput directory will define directory for downloaded products.\n\nGeometry wll define our area of interest. It will be passed as BBOX (Bounding Box), as a list of coordinates - Xmin, Ymin, Xmax, Ymax. All coordinates will be defined in EPSG:4326.\n\n# Timerange of data that we want to recieve\nstart_date = '2023-07-01'\nend_date = '2023-07-07'\n# Output directory of our desired data\noutput_dir = 'sen_1/'\n# Geometry in form of a BBOX\nbbox = [20.8510, 52.0976, 21.2712, 52.3345,]\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-1-3-prerequisites-data","position":9},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl2":"2. Work with HDA"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-2-work-with-hda","position":10},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl2":"2. Work with HDA"},"content":"HDA (Harmonized Data Access) uses STAC protocol, that allows its user access the Earth Observation data, stored in various provides. Thanks to that, HDA serves as an one stream of data, allowing for comfortable work with sattelite imagery.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-2-work-with-hda","position":11},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl2":"2.1 API URLs"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-2-1-api-urls","position":12},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl2":"2.1 API URLs"},"content":"HDA, as all API, is build upon many endpoints. In this notebook we will use only one for collections and searching. Below there are definitions of those endpoints. We will be using the common one site, but you can change it to central, lumi or leonardo if you want.\n\nCOLLECTIONS_URL = 'https://hda.data.destination-earth.eu/stac/collections'\nSEARCH_URL = 'https://hda.data.destination-earth.eu/stac/search'\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-2-1-api-urls","position":13},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl2":"2.2 Listing available collections"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-2-2-listing-available-collections","position":14},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl2":"2.2 Listing available collections"},"content":"Firstly lets see to what collections we can get access, while using HDA.\n\ndef get_stac_collections(api_url):\n    response = requests.get(api_url)\n    if response.status_code == 200:\n        stac_data = response.json()['collections']\n        collections = [x['id'] for x in stac_data]\n        return collections\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n    \nget_stac_collections(COLLECTIONS_URL)\n\nAs you can see, there are many dataset, that can be access using just one single tool - HDA. In this notebook we will use only Sentinel-1 (GRDH) images, so our collection will be EO.ESA.DAT.SENTINEL-1.L1_GRD.\n\ncollections = ['EO.ESA.DAT.SENTINEL-1.L1_GRD']\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-2-2-listing-available-collections","position":15},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"2.3 Authorization","lvl2":"2.2 Listing available collections"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-2-3-authorization","position":16},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"2.3 Authorization","lvl2":"2.2 Listing available collections"},"content":"As stated before, to use HDA you will need an account on DestinE. Using your credentials, you will be able to generate access token, that will be needed in upcoming requests. In listing collections’ cell you didn’t have to create token, because only more advanced requests (like listing, searching and downloading items) need it.\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-2-3-authorization","position":17},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"2.4 Find newest product","lvl2":"2.2 Listing available collections"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-2-4-find-newest-product","position":18},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"2.4 Find newest product","lvl2":"2.2 Listing available collections"},"content":"After defining all prerequired data and obtaining access token, we can start searching for interesting us products. To do that, we will firstly create body of a POST request with ours parameters. Then, we will send it to HDA and, if request is successful, we will read from response download link.\n\ndef search_items(access_token: str, search_url: str, collection: str, \n                 bbox: list[float | int], start_date: str, end_date: str):\n    body = {\n        'datetime': f'{start_date}T00:00:00Z/{end_date}T23:59:59Z',\n        'collections': [collection],\n        'bbox': bbox\n    }\n    response = requests.post(search_url, json=body, headers={'Authorization': 'Bearer {}'.format(access_token)})\n    if response.status_code != 200:\n        print(f'Error in search request: {response.status_code} - {response.text}')\n        return None\n    else:\n        print(\"Request successful! Reading data...\")\n        products_list = [(feature.get('assets').get('downloadLink').get('href'), feature.get('links')[0].get('title')) for feature in response.json().get('features', [])]\n        return products_list\n\ncollections_items = []\nfor c in collections:\n    collections_items.append(search_items(access_token, SEARCH_URL, c, bbox, start_date, end_date))\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-2-4-find-newest-product","position":19},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"2.5 Download founded images","lvl2":"2.2 Listing available collections"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-2-5-download-founded-images","position":20},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"2.5 Download founded images","lvl2":"2.2 Listing available collections"},"content":"After obtaining URLs for each interesting us product, we can download it with one request. Product will be downloaded compressed in zip format.\n\ndef hda_download(access_token: str, url: str, output: str):\n    response = requests.get(url,stream=True,headers={'Authorization': 'Bearer {}'.format(access_token), 'Accept-Encoding': None})\n    if response.status_code == 200:\n        print('Downloading dataset...')\n        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n            z.extractall(output)\n        print('The dataset has been downloaded to: {}'.format(output))\n    else:\n        print('Request Unsuccessful! Error-Code: {}'.format(response.status_code))\n\nfor collection in collections_items:\n    for item in collection:\n        url = item[0]\n        product_id = item[1]\n        download_path = output_dir + product_id\n        hda_download(access_token, url, download_path)\n        break\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-2-5-download-founded-images","position":21},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl2":"4. Urban areas extraction by tresholding"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-4-urban-areas-extraction-by-tresholding","position":22},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl2":"4. Urban areas extraction by tresholding"},"content":"The analysis of Synthetic Aperture Radar (SAR) images enables the detection of variations in radar wave reflections, which is crucial for identifying urbanized areas. SAR images exhibit distinct reflection patterns that allow for the delineation of urban areas, including agglomerations and smaller urban settlements. Urbanized areas stand out in these images primarily due to the intense reflection of radar waves by man-made structures, roads, and other artificial elements of infrastructure, facilitating their clear identification contrasted with natural terrain. Such analysis of areas like the Warsaw agglomeration can serve for monitoring city growth and development, urban planning, assessment of natural and anthropogenic threats, and environmental management. The image clearly depicts the Warsaw agglomeration area, adjacent to the region between the Warsaw and Łódź agglomerations. This space is currently planned for development to connect both agglomerations. In the central part of the frame, there are plans for establishing a large airport complex.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-4-urban-areas-extraction-by-tresholding","position":23},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"4.1 Reading and preprocessing data","lvl2":"4. Urban areas extraction by tresholding"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-4-1-reading-and-preprocessing-data","position":24},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"4.1 Reading and preprocessing data","lvl2":"4. Urban areas extraction by tresholding"},"content":"The first step in our analysis will be data reading. The data will be read using the rasterio library from the output folder path provided in the previous section.\n\n# Function to find files containing specific substrings (\"vv\" and \"vh\") in their names\ndef find_files_with_names(folder_path, *names):\n    vv_path = None  \n    vh_path = None  \n    \n    # Traverse the directory tree starting at folder_path\n    for root, dirs, files in os.walk(folder_path):\n        if dirs:  # Check if there are subdirectories\n            first_subfolder = dirs[0]\n            measurement_folder = os.path.join(root, first_subfolder, \"measurement\")\n            \n            if os.path.isdir(measurement_folder):\n                # Look through the files in the \"measurement\" folder\n                for file_name in os.listdir(measurement_folder):\n                    if \"vv\" in file_name:\n                        vv_path = os.path.join(measurement_folder, file_name)\n                    elif \"vh\" in file_name:\n                        vh_path = os.path.join(measurement_folder, file_name)\n                break  # Exit after finding the files\n    return vv_path, vh_path\n\n# Find the \"vv\" and \"vh\" files in the specified directory\ninput_file_vv, input_file_vh = find_files_with_names(output_dir, \"vv\", \"vh\")\n\n# Open the \"vv\" file and read its data and profile\nwith rasterio.open(input_file_vv) as src_vv:\n    vv_band = src_vv.read(1)\n    profile_vv = src_vv.profile\n\n# Open the \"vh\" file and read its data and profile\nwith rasterio.open(input_file_vh) as src_vh:\n    vh_band = src_vh.read(1)\n    profile_vh = src_vh.profile\n\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-4-1-reading-and-preprocessing-data","position":25},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"4.2 Pixel value covertion to decibels","lvl2":"4. Urban areas extraction by tresholding"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-4-2-pixel-value-covertion-to-decibels","position":26},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"4.2 Pixel value covertion to decibels","lvl2":"4. Urban areas extraction by tresholding"},"content":"After reading the data, we can convert the pixel values to decibels. Data from Sentinel-1 is converted to the decibel scale (dB) to better visualize differences in the intensity of radar wave reflection, which aids in the analysis of urban areas due to their specific reflection characteristics.\n\ndef convert_to_db(image):\n    # Replace zeros with a small positive value to avoid taking log of zero\n    image[image == 0] = np.finfo(float).eps  \n\n    # Calculate dB values\n    image_db = 10 * np.log10(image)\n\n    # Replace infinite values with the maximum finite value\n    max_value = np.nanmax(image_db[np.isfinite(image_db)])  \n    image_db[np.isinf(image_db)] = max_value  \n\n    return image_db\n\nvv_band_db = convert_to_db(vv_band)\nvh_band_db = convert_to_db(vh_band)\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-4-2-pixel-value-covertion-to-decibels","position":27},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"4.3 Data analysis and tresholding","lvl2":"4. Urban areas extraction by tresholding"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-4-3-data-analysis-and-tresholding","position":28},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"4.3 Data analysis and tresholding","lvl2":"4. Urban areas extraction by tresholding"},"content":"The next step involves plotting histograms of the decibel pixel values for both the VV and VH bands. These histograms provide insights into the distribution of radar reflection intensities, aiding in understanding the characteristics of the observed area. The chosen thresholds, 23.5 dB for VV and 21.5 dB for VH, are selected based on prior knowledge or analysis requirements specific to urban areas to segment regions of interest in the images. The thresholding method applied here converts pixel values above the threshold to 1 and those below to 0, aiding in the delineation of features or areas with specific radar reflection characteristics typical of urban environments.\n\nfig, axs = plt.subplots(1, 2, figsize=(18, 5))\n\n# Histogram of VV Image in dB\naxs[0].hist(vv_band_db.flatten(), bins=50, color='blue', alpha=0.7, label='VV dB')\naxs[0].set_title('Histogram of VV Image in dB')\naxs[0].set_xlabel('Radar Reflection Value (dB)')\naxs[0].set_ylabel('Number of Pixels')\naxs[0].legend()\naxs[0].grid(True, linestyle='--', alpha=0.5)\n\n# Histogram of VH Image in dB\naxs[1].hist(vh_band_db.flatten(), bins=50, color='red', alpha=0.7, label='VH dB')\naxs[1].set_title('Histogram of VH Image in dB')\naxs[1].set_xlabel('Radar Reflection Value (dB)')\naxs[1].set_ylabel('Number of Pixels')\naxs[1].legend()\naxs[1].grid(True, linestyle='--', alpha=0.5)\n\nplt.show()\n\n# Function to apply a threshold to an image\n# Pixels greater than the threshold are set to 1, others to 0\ndef thresholding(image, threshold):\n    return (image > threshold).astype(np.uint8)\n\n# Define threshold values for VV and VH bands\nthreshold_value_vv = 23.5\nthreshold_value_vh = 21.5\n\n# Apply thresholding to the VV and VH band\nvv_band_threshold = thresholding(vv_band_db, threshold_value_vv)\nvh_band_threshold = thresholding(vh_band_db, threshold_value_vh)\n\n# Combine the thresholded images using a logical AND operation\n# The result is 1 where both VV and VH are above their respective thresholds\ncombined_threshold = np.logical_and(vv_band_threshold, vh_band_threshold).astype(np.uint8)\n\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-4-3-data-analysis-and-tresholding","position":29},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"4.4 Images ploting","lvl2":"4. Urban areas extraction by tresholding"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-4-4-images-ploting","position":30},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"4.4 Images ploting","lvl2":"4. Urban areas extraction by tresholding"},"content":"Next on the subplots we can visualize the differences in radio signal reflection for various polarization types. By displaying the original and decibel-transformed images for both VV and VH polarizations, we can observe distinct features and intensities in urban areas. Furthermore, by combining information from both polarizations through thresholding, we enhance our ability to mask urbanized regions, providing a more comprehensive understanding of the observed area’s urban characteristics.\n\n# Create a figure with a 3x2 grid of subplots, setting the overall figure size\nfig, ax = plt.subplots(3, 2, figsize=(18, 18)) \n\n# Display the original VV image\nax[0, 1].imshow(vv_band, cmap='viridis')\nax[0, 1].set_title('Original VV Image')\n\n# Display the VV image in decibels\nax[1, 1].imshow(vv_band_db, cmap='viridis')\nax[1, 1].set_title('VV Image in dB')\n\n# Display the VV image after thresholding\nax[2, 1].imshow(vv_band_threshold, cmap='inferno')\nax[2, 1].set_title('VV Image after Thresholding')\n\n# Display the original VH image\nax[0, 0].imshow(vh_band, cmap='viridis')\nax[0, 0].set_title('Original VH Image')\n\n# Display the VH image in decibels\nax[1, 0].imshow(vh_band_db, cmap='viridis')\nax[1, 0].set_title('VH Image in dB')\n\n# Display the VH image after thresholding\nax[2, 0].imshow(vh_band_threshold, cmap='inferno')\nax[2, 0].set_title('VH Image after Thresholding')\n\n# Turn off the axes for all subplots to improve the visual presentation\nfor i in range(2):\n    for j in range(3):\n        ax[j, i].axis('off')\n\n# Show the complete figure\nplt.show()\n\n# Display the combined threshold image using a specific colormap\nplt.figure(figsize=(18, 9))  \nplt.imshow(combined_threshold, cmap='twilight_shifted')\nplt.title('Combined Threshold Image')  \nplt.axis('off')  \nplt.show()\n\nThis method of data analysis facilitates straightforward masking of urbanized areas. In the processed image, the Warsaw agglomeration is clearly visible on the right side. A star-shaped accumulation of population is apparent in Warsaw along major transportation corridors such as railways and expressways. Smaller urban centers are visible in the image. Furthermore, it can be observed that the number of masked pixels accumulates as we approach the city center, indicating denser urban development in the downtown area compared to the suburbs.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-4-4-images-ploting","position":31},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"4.5 Cuantitive analysis","lvl2":"4. Urban areas extraction by tresholding"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-4-5-cuantitive-analysis","position":32},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"4.5 Cuantitive analysis","lvl2":"4. Urban areas extraction by tresholding"},"content":"After processing our satellite images, we can conduct quantitative analysis. The characteristics of the satellite data enable us to analyze individual pixel values, which is crucial for detailed and statistical studies. In our case, having the data thresholded and the size of each pixel being 10x10 meters, we can calculate the percentage and the adequate size of the urbanized area.\n\n# Calculate the number of urban pixels by summing up the binary values in the combined threshold image\nurban_pixels = np.sum(combined_threshold)\n\n# Define the area covered by a single pixel (in square meters)\npixel_area = 10 * 10  # Assuming each pixel represents a 10m x 10m area\n\n# Calculate the total urban area in square meters\nurban_area = urban_pixels * pixel_area\n\n# Calculate the total number of pixels in the combined threshold image\ntotal_pixels = combined_threshold.size\n\n# Calculate the percentage of urbanized area in the image\nurban_percentage = (urban_pixels / total_pixels) * 100\n\n# Convert urban area from square meters to hectares\nurban_area_hectares = urban_area / 10_000  \n\n# Convert total area from square meters to hectares\ntotal_area_hectares = (total_pixels * pixel_area) / 10_000\n\nprint(f\"Percentage of Urbanized Area: {urban_percentage:.2f}%\")\nprint(f\"Urbanized Area: {urban_area_hectares:.2f} ha\")\nprint(f\"Total Image Area: {total_area_hectares:.2f} ha\")\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-4-5-cuantitive-analysis","position":33},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"4.6 Results download","lvl2":"4. Urban areas extraction by tresholding"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-4-6-results-download","position":34},{"hierarchy":{"lvl1":"Using HDA to find and download data for Urban Area Monitoring with Sentinel-1 Data","lvl3":"4.6 Results download","lvl2":"4. Urban areas extraction by tresholding"},"content":"\n\n# Define output file names\noutput_file_db_vv = 'vv_db_urban_warsaw.tif'\noutput_file_db_vh = 'vh_db_urban_warsaw.tif'\noutput_file_threshold_combined = 'warsaw_urban_threshold_combined.tif'\n\n# Update the data type in the profile to float32 for dB conversion\nprofile_vv.update(dtype=rasterio.float32)\nprofile_vh.update(dtype=rasterio.float32)\n\n# Write the result data in dB to the output file\nwith rasterio.open(output_file_db_vv, 'w', **profile_vv) as dst:\n    dst.write(vv_band_db.astype(np.float32), 1)\n    \nwith rasterio.open(output_file_db_vh, 'w', **profile_vh) as dst:\n    dst.write(vh_band_db.astype(np.float32), 1)\n\n# Update the data type in the VV profile to uint8 for the combined thresholded image\nprofile_vv.update(dtype=rasterio.uint8)\n\n# Write the combined thresholded image data to the output file\nwith rasterio.open(output_file_threshold_combined, 'w', **profile_vv) as dst:\n    dst.write(combined_threshold, 1)","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd-1#id-4-6-results-download","position":35},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1","position":0},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake"},"content":"In this notebook, we will present a simple example on how you can access data from DEDL using HDA and what you can do with it. As an example, we will try to download Sentinel-2 images containining data of Śniadrwy lake from first week of July 2023. With usage of HDA and few Python packages, you will be able to obtain rasters with NDWI index.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1","position":1},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"1. Prerequisites"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-1-prerequisites","position":2},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"1. Prerequisites"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-1-prerequisites","position":3},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"1.1 DestinE account","lvl2":"1. Prerequisites"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-1-1-destine-account","position":4},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"1.1 DestinE account","lvl2":"1. Prerequisites"},"content":"Firstly, to work with HDA we will need account on DestinE Core Service Platfrom website. You can register under this url: \n\nhttps://​platform​.destine​.eu/\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-1-1-destine-account","position":5},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"1.2 Python’s packages","lvl2":"1. Prerequisites"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-1-2-pythons-packages","position":6},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"1.2 Python’s packages","lvl2":"1. Prerequisites"},"content":"\n\nimport requests\nimport zipfile\nimport io\nimport destinelab as deauth\nfrom getpass import getpass\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-1-2-pythons-packages","position":7},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"1.3 Prerequiared data","lvl2":"1. Prerequisites"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-1-3-prerequiared-data","position":8},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"1.3 Prerequiared data","lvl2":"1. Prerequisites"},"content":"Before reuqesting some data from DEDL HDA, let’s specify what data we want to obtain. We will define 3 variables:\n\nStart date and end date,\n\nOutput directory,\n\nGeometry of interesting us area\n\nStart date and end date will define our timerange in reuqest. HDA will search only for products that were obtained between those two dates.\n\nOutput directory will define directory for downloaded products.\n\nGeometry wll define our area of interest. It will be passed as BBOX (Bounding Box), as a list of coordinates - Xmin, Ymin, Xmax, Ymax. All coordinates will be defined in EPSG:4326.\n\n# Timerange of data that we want to recieve\nstart_date = '2023-07-01'\nend_date = '2023-07-07'\n# Output directory of our desired data\noutput_dir = 'output/'\n# Geometry in form of a BBOX\nbbox = [21.61868,53.66627,21.90926,53.82351]\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-1-3-prerequiared-data","position":9},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"2. Work with HDA"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-2-work-with-hda","position":10},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"2. Work with HDA"},"content":"HDA (Harmonized Data Access) uses STAC protocol, that allows its user access the Earth Observation data, stored in various provides. Thanks to that, HDA serves as an one stream of data, allowing for comfortable work with sattelite imagery.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-2-work-with-hda","position":11},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"2.1 API URLs"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-2-1-api-urls","position":12},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"2.1 API URLs"},"content":"HDA, as all API, is build upon many endpoints. In this notebook we will use only one for collections and searching. Below there are definitions of those endpoints. We will be using the common one site, but you can change it to central, lumi or leonardo if you want.\n\nCOLLECTIONS_URL = 'https://hda.data.destination-earth.eu/stac/collections'\nSEARCH_URL = 'https://hda.data.destination-earth.eu/stac/search'\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-2-1-api-urls","position":13},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"2.2 Listing available collections"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-2-2-listing-available-collections","position":14},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"2.2 Listing available collections"},"content":"Firstly lets see to which collections we can get access, while using HDA.\n\ndef get_stac_collections(api_url):\n    response = requests.get(api_url)\n    if response.status_code == 200:\n        stac_data = response.json()['collections']\n        collections = [x['id'] for x in stac_data]\n        return collections\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n    \nget_stac_collections(COLLECTIONS_URL)\n\nAs you can see, there are many dataset, that can be access using just one single tool - HDA. In this notebook we will use only Sentinel-2 images, so our collections will be EO.ESA.DAT.SENTINEL-2.MSI.L1C and EO.ESA.DAT.SENTINEL-2.MSI.L2A.\n\ncollections = ['EO.ESA.DAT.SENTINEL-2.MSI.L1C', 'EO.ESA.DAT.SENTINEL-2.MSI.L2A']\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-2-2-listing-available-collections","position":15},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"2.3 Authorization","lvl2":"2.2 Listing available collections"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-2-3-authorization","position":16},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"2.3 Authorization","lvl2":"2.2 Listing available collections"},"content":"As stated before, to use HDA you will need an account on DestinE. Using your credentials, you will be able to generate access token, that will be needed in upcoming requests. In listing collections’ cell you didn’t have to create token, because only more advanced requests (like listing, searching and downloading items) need it.\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-2-3-authorization","position":17},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"2.4 Find newest product","lvl2":"2.2 Listing available collections"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-2-4-find-newest-product","position":18},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"2.4 Find newest product","lvl2":"2.2 Listing available collections"},"content":"After defining all prerequired data and obtaining access token, we can start searching for interesting us products. To do that, we will firstly create body of a POST request with ours parameters. Then, we will send it to HDA and, if request is successful, we will read from response download link.\n\ndef search_items(access_token: str, search_url: str, collection: str, \n                 bbox: list[float | int], start_date: str, end_date: str):\n    body = {\n        'datetime': f'{start_date}T00:00:00Z/{end_date}T23:59:59Z',\n        'collections': [collection],\n        'bbox': bbox\n    }\n    response = requests.post(search_url, json=body, headers={'Authorization': 'Bearer {}'.format(access_token)})\n    if response.status_code != 200:\n        print(f'Error in search request: {response.status_code} - {response.text}')\n        return None\n    else:\n        print(\"Request successful! Reading data...\")\n        products_list = [(feature.get('assets').get('downloadLink').get('href'), feature.get('links')[0].get('title')) for feature in response.json().get('features', [])]\n        return products_list\n\nTo obtain products from two levels of Sentinel-2 - L2A and L1C, we will use loop, iterating over every single collection.\n\ncollections_items = []\nfor c in collections:\n    collections_items.append(search_items(access_token, SEARCH_URL, c, bbox, start_date, end_date))\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-2-4-find-newest-product","position":19},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"2.5 Download founded images","lvl2":"2.2 Listing available collections"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-2-5-download-founded-images","position":20},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"2.5 Download founded images","lvl2":"2.2 Listing available collections"},"content":"After obtaining download links for each of interesting us product, we can finally download it. With single request, we will download compressed product in zip format to provided directory. Function will set filename as product’s id. Mind that Sentinel-2 products might be over 1 GB, so it may take a few minutes to download them, based on your internet connection.\n\ndef hda_download(access_token: str, url: str, output: str):\n    response = requests.get(url,stream=True,headers={'Authorization': 'Bearer {}'.format(access_token), 'Accept-Encoding': None})\n    if response.status_code == 200:\n        print('Downloading dataset...')\n        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n            z.extractall(output)\n        print('The dataset has been downloaded to: {}'.format(output))\n    else:\n        print('Request Unsuccessful! Error-Code: {}'.format(response.status_code))\n\nFrom previous section, we obtained 2D list - with one dimension being collection and second being one item (single product). Becouse of that, we will use two loops to iterate over single products.\n\nfor collection in collections_items:\n    for item in collection:\n        url = item[0]\n        product_id = item[1]\n        download_path = output_dir + product_id\n        hda_download(access_token, url, download_path)\n        break\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-2-5-download-founded-images","position":21},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"4. Simple data computing - obtaining NDWI"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-4-simple-data-computing-obtaining-ndwi","position":22},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"4. Simple data computing - obtaining NDWI"},"content":"In this chapter we will conduct simple data computing. As stated before, this notebook concentrate on monitoring of Śniadrwy lake, so we will try to calculate NDWI index for each pixel and create raster from it. Using all downloaded items, we will be able to monitor lake status from entire month.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-4-simple-data-computing-obtaining-ndwi","position":23},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"4.1 Libraries","lvl2":"4. Simple data computing - obtaining NDWI"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-4-1-libraries","position":24},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"4.1 Libraries","lvl2":"4. Simple data computing - obtaining NDWI"},"content":"In this chapter we will try to compute obtained by us imagery data, with usage of Python and its spatial-oriented packages.\n\nimport rasterio\nfrom osgeo import gdal, gdal_array, osr\nimport numpy as np\nimport os\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-4-1-libraries","position":25},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"4.2 Functions for reading, calculating and saving raster data","lvl2":"4. Simple data computing - obtaining NDWI"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-4-2-functions-for-reading-calculating-and-saving-raster-data","position":26},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"4.2 Functions for reading, calculating and saving raster data","lvl2":"4. Simple data computing - obtaining NDWI"},"content":"Here we present you some functions for reading raster data into Numpy matrix, calculating NDWI with NIR and GREEN matrixes and saving result as a new raster. We will conduct such calculation for each downloaded item. In the end, we will obtain NDWI data on Śniadrwy Lake from whole week.\n\ndef getFullPath(dir: str, resolution: int, band: str):\n    if not os.path.isdir(dir):\n        raise ValueError(f\"Provided path does not exist: {dir}\")\n    elif resolution not in [10,20,60]:\n        raise ValueError(f\"Provided resolution does not exist: R{resolution}m\")\n    else:\n        full_path = dir\n        while True:\n            content = os.listdir(full_path)\n            if len(content) == 0:\n                raise ValueError(f\"Directory empty: {full_path}\")\n            elif len(content) == 1:\n                if full_path[-1] != '/':\n                    full_path = full_path + '/' + content[0]\n                else:\n                    full_path = full_path + content[0]\n            else:\n                if 'GRANULE' in content:\n                    full_path = full_path + '/' + 'GRANULE'\n                    break\n                else:\n                    raise ValueError(f\"Unsupported dir architecture: {full_path}\")\n        full_path = full_path + '/' + os.listdir(full_path)[0]\n        full_path = full_path + '/' + \"IMG_DATA\"\n        if len(os.listdir(full_path)) == 3:\n            full_path = full_path + '/' + f'R{resolution}m'\n            images = os.listdir(full_path)\n            for img in images:\n                if band in img:\n                    return full_path + '/' + img\n            raise ValueError(f'No such band {band} in directory: {full_path}')\n        else:\n            images = os.listdir(full_path)\n            for img in images:\n                if band in img:\n                    return full_path + '/' + img\n            raise ValueError(f'No such band {band} in directory: {full_path}')\n\n# Get transformation matrix from raster\ndef getTransform(pathToRaster):\n    dataset = gdal.Open(pathToRaster)\n    transformation = dataset.GetGeoTransform()\n    return transformation\n\n# Read raster and return pixels' values matrix as int16, new transformation matrix, crs\ndef readRaster(path, resolution, band):\n    path = getFullPath(path, resolution, band)\n    trans = getTransform(path) # trzeba zdefiniować który kanał\n    raster, crs = rasterToMatrix(path)\n    return raster.astype(np.int16), crs, trans\n\ndef rasterToMatrix(pathToRaster):\n    with rasterio.open(pathToRaster) as src:\n        matrix = src.read(1)\n    return matrix, src.crs.to_epsg()\n\n# Transform numpy's matrix to geotiff; pass new raster's filepath, matrix with pixels' values, gdal file type, transformation matrix, projection, nodata value\ndef npMatrixToGeotiff(filepath, matrix, gdalType, projection, transformMatrix, nodata = None):\n    driver = gdal.GetDriverByName('Gtiff')\n    if len(matrix.shape) > 2:\n        (bandNr, yRes, xRes) = matrix.shape\n        image = driver.Create(filepath, xRes, yRes, bandNr, gdalType)\n        for b in range(bandNr):\n            b = b + 1\n            band = image.GetRasterBand(b)\n            if nodata is not None:\n                band.SetNoDataValue(nodata)\n            band.WriteArray(matrix[b-1,:,:])\n            band.FlushCache\n    else:\n        bandNr = 1\n        (yRes, xRes) = matrix.shape\n        image = driver.Create(filepath, xRes, yRes, bandNr, gdalType)\n        print(type(image))\n        band = image.GetRasterBand(bandNr)\n        if nodata is not None:\n            band.SetNoDataValue(nodata)\n        band.WriteArray(matrix)\n        band.FlushCache\n    image.SetGeoTransform(transformMatrix)\n    image.SetProjection(projection)\n    del driver, image, band\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-4-2-functions-for-reading-calculating-and-saving-raster-data","position":27},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"4.3 Computing","lvl2":"4. Simple data computing - obtaining NDWI"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-4-3-computing","position":28},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"4.3 Computing","lvl2":"4. Simple data computing - obtaining NDWI"},"content":"With usage of defined functions, we will now generate NWDI rasters. Only data that will be needed in this step is a list with paths to our products (extracted from zip archive). Function readRaster will choose specified band from specified path.\n\n# List of products' paths. If your output directory contains more than just downloaded products, please provide them in a list, just like in the commented lines below\n#dataset = [output_dir+x for x in os.listdir(output_dir)]\ndataset = [\n     'output/S2B_MSIL1C_20230701T094549_N0509_R079_T34UEE_20230701T104205'\n ]\n# Output directiry for new images\ncompution_output = 'output/ndwi_rasters'\n\n# Iterating over single product\nfor item in dataset:\n    # Reading name from path\n    name = item.split('/')[-1]\n    # Reading green band into matrix\n    green = readRaster(item, 10, 'B03')\n    # Reading NIR band into matrix\n    nir = readRaster(item, 10, 'B08')\n    # Calculating NDWI matrix\n    ndwi = (green[0]-nir[0]) / (green[0]+nir[0])\n    # Setting treshhold for water-containing pixels\n    ndwi[ndwi >= 0] = 1\n    ndwi[ndwi < 0] = 0\n    # Creating SpatialReference object and setting it to match original's raster CRS\n    projection = osr.SpatialReference()\n    projection.ImportFromEPSG(green[1])\n    # Creating raster from matrix in GeoTiff format\n    npMatrixToGeotiff(f'{compution_output}/{name}.tif', ndwi, gdal_array.NumericTypeCodeToGDALTypeCode(np.float32), projection.ExportToWkt(), green[2])\n\nAfter successfuly creating and saving new images, we can now visualize them in Python using raterio package.\n\nimg = rasterio.open('output/ndwi_rasters/S2B_MSIL1C_20230701T094549_N0509_R079_T34UEE_20230701T104205.tif')\nfrom rasterio.plot import show\nshow(img)","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a-1#id-4-3-computing","position":29},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction"},"type":"lvl1","url":"/hda-pystac-client","position":0},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction"},"content":"🚀 Launch in JupyterHub\n\nThis notebook shows the basic use of DestinE Data Lake Harmonised Data Access using pystac-client.\nIt will include iterating through Collections and Items, and perform simple spatio-temporal searches.\n\n","type":"content","url":"/hda-pystac-client","position":1},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl3":"Obtain DEDL Access Token to use the HDA service"},"type":"lvl3","url":"/hda-pystac-client#obtain-dedl-access-token-to-use-the-hda-service","position":2},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl3":"Obtain DEDL Access Token to use the HDA service"},"content":"\n\npip install --quiet --upgrade destinelab\n\nimport requests\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/hda-pystac-client#obtain-dedl-access-token-to-use-the-hda-service","position":3},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl3":"Set username and password as environment variables to be used for DEDL data access"},"type":"lvl3","url":"/hda-pystac-client#set-username-and-password-as-environment-variables-to-be-used-for-dedl-data-access","position":4},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl3":"Set username and password as environment variables to be used for DEDL data access"},"content":"\n\nimport os\n\nos.environ[\"EODAG__DEDL__AUTH__CREDENTIALS__USERNAME\"] = DESP_USERNAME\nos.environ[\"EODAG__DEDL__AUTH__CREDENTIALS__PASSWORD\"] = DESP_PASSWORD\n\n","type":"content","url":"/hda-pystac-client#set-username-and-password-as-environment-variables-to-be-used-for-dedl-data-access","position":5},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl2":"Create pystac client object for HDA STAC API"},"type":"lvl2","url":"/hda-pystac-client#create-pystac-client-object-for-hda-stac-api","position":6},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl2":"Create pystac client object for HDA STAC API"},"content":"We first connect to an API by retrieving the root catalog, or landing page, of the API with the \n\nClient.open function.\n\nfrom pystac_client import Client\n\nHDA_API_URL = \"https://hda.data.destination-earth.eu/stac\"\ncat = Client.open(HDA_API_URL, headers=auth_headers)\n\n","type":"content","url":"/hda-pystac-client#create-pystac-client-object-for-hda-stac-api","position":7},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl3":"Query all available collections","lvl2":"Create pystac client object for HDA STAC API"},"type":"lvl3","url":"/hda-pystac-client#query-all-available-collections","position":8},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl3":"Query all available collections","lvl2":"Create pystac client object for HDA STAC API"},"content":"As with a static catalog the get_collections function will iterate through the Collections in the Catalog.\nNotice that because this is an API it can get all the Collections through a single call, rather than having to fetch each one individually.\n\nfrom rich.console import Console\nimport rich.table\n\nconsole = Console()\n\nhda_collections = cat.get_collections()\n\ntable = rich.table.Table(title=\"HDA collections\", expand=True)\ntable.add_column(\"ID\", style=\"cyan\", justify=\"right\",no_wrap=True)\ntable.add_column(\"Title\", style=\"violet\", no_wrap=True)\nfor collection in hda_collections:\n    table.add_row(collection.id, collection.title)\nconsole.print(table)\n\n","type":"content","url":"/hda-pystac-client#query-all-available-collections","position":9},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl3":"Obtain provider information for each individual collection","lvl2":"Create pystac client object for HDA STAC API"},"type":"lvl3","url":"/hda-pystac-client#obtain-provider-information-for-each-individual-collection","position":10},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl3":"Obtain provider information for each individual collection","lvl2":"Create pystac client object for HDA STAC API"},"content":"\n\ntable = rich.table.Table(title=\"HDA collections | Providers\", expand=True)\ntable.add_column(\"Title\", style=\"cyan\", justify=\"right\", no_wrap=True)\ntable.add_column(\"Provider\", style=\"violet\", no_wrap=True)\n\nhda_collections = cat.get_collections()\n\nfor collection in hda_collections:\n    collection_details = cat.get_collection(collection.id)\n    provider = ','.join(str(x.name) for x in collection_details.providers)\n    table.add_row(collection_details.title, provider)\nconsole.print(table)\n\n","type":"content","url":"/hda-pystac-client#obtain-provider-information-for-each-individual-collection","position":11},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl3":"Inspect Items of a Collection","lvl2":"Create pystac client object for HDA STAC API"},"type":"lvl3","url":"/hda-pystac-client#inspect-items-of-a-collection","position":12},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl3":"Inspect Items of a Collection","lvl2":"Create pystac client object for HDA STAC API"},"content":"The main functions for getting items return iterators, where pystac-client will handle retrieval of additional pages when needed. Note that one request is made for the first ten items, then a second request for the next ten.\n\ncoll_name = 'EO.ESA.DAT.SENTINEL-1.L1_GRD'\nsearch = cat.search(\n    max_items=10,\n    collections=[coll_name],\n    bbox=[-72.5,40.5,-72,41],\n    datetime=\"2023-09-09T00:00:00Z/2023-09-20T23:59:59Z\"\n)\n\ncoll_items = search.item_collection()\nconsole.print(f\"For collection {coll_name} we found {len(coll_items)} items\")\n\nimport geopandas\n\ndf = geopandas.GeoDataFrame.from_features(coll_items.to_dict(), crs=\"epsg:4326\")\ndf.head()\n\n\n\n","type":"content","url":"/hda-pystac-client#inspect-items-of-a-collection","position":13},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl3":"Inspect STAC assets of an item","lvl2":"Create pystac client object for HDA STAC API"},"type":"lvl3","url":"/hda-pystac-client#inspect-stac-assets-of-an-item","position":14},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl3":"Inspect STAC assets of an item","lvl2":"Create pystac client object for HDA STAC API"},"content":"\n\nimport rich.table\n\nselected_item = coll_items[3]\n\ntable = rich.table.Table(title=\"Assets in STAC Item\")\ntable.add_column(\"Asset Key\", style=\"cyan\", no_wrap=True)\ntable.add_column(\"Description\")\nfor asset_key, asset in selected_item.assets.items():\n    table.add_row(asset_key, asset.title)\n\nconsole.print(table)\n\nfrom IPython.display import Image\n\nImage(url=selected_item.assets[\"thumbnail\"].href, width=500)\n\ndown_uri = selected_item.assets[\"downloadLink\"].href\nconsole.print(f\"Download link of asset is {down_uri}\")\n\n","type":"content","url":"/hda-pystac-client#inspect-stac-assets-of-an-item","position":15},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl4":"Download asset to JupyterLab","lvl3":"Inspect STAC assets of an item","lvl2":"Create pystac client object for HDA STAC API"},"type":"lvl4","url":"/hda-pystac-client#download-asset-to-jupyterlab","position":16},{"hierarchy":{"lvl1":"HDA PySTAC-Client Introduction","lvl4":"Download asset to JupyterLab","lvl3":"Inspect STAC assets of an item","lvl2":"Create pystac client object for HDA STAC API"},"content":"\n\nselected_item.id\n\nselected_item.assets[\"downloadLink\"]\n\n# Make http request for remote file data\ndata = requests.get(selected_item.assets[\"downloadLink\"].href,\n                   headers=auth_headers)\nmtype = selected_item.assets[\"downloadLink\"].media_type.split(\"/\")[1]\n# Save file data to local copy\nwith open(f\"{selected_item.id}.{mtype}\", 'wb')as file:\n    file.write(data.content)","type":"content","url":"/hda-pystac-client#download-asset-to-jupyterlab","position":17},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd","position":0},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data"},"content":"🚀 Launch in JupyterHub\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd","position":1},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl2":"1. Prerequisites"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-1-prerequisites","position":2},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl2":"1. Prerequisites"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-1-prerequisites","position":3},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"1.1 DestinE account","lvl2":"1. Prerequisites"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-1-1-destine-account","position":4},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"1.1 DestinE account","lvl2":"1. Prerequisites"},"content":"Firstly, to work with HDA we will need account on DestinE Core Service Platfrom website. You can register under this url: \n\nhttps://​platform​.destine​.eu/\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-1-1-destine-account","position":5},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"1.2 Libraries","lvl2":"1. Prerequisites"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-1-2-libraries","position":6},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"1.2 Libraries","lvl2":"1. Prerequisites"},"content":"\n\nimport datetime\nimport requests\nimport numpy as np\nimport rasterio\nimport matplotlib.pyplot as plt\nimport requests\nimport zipfile\nfrom getpass import getpass\nimport io\nfrom rasterio.mask import mask\nimport os\nimport destinelab as deauth\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-1-2-libraries","position":7},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"1.3 Prerequisites data","lvl2":"1. Prerequisites"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-1-3-prerequisites-data","position":8},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"1.3 Prerequisites data","lvl2":"1. Prerequisites"},"content":"Before reuqesting some data from DEDL, let’s specify what data we want to obtain. We will define 3 variables:\n\nStart date and end date,\n\nOutput directory,\n\nGeometry of interesting us area\n\nStart date and end date will define our timerange in reuqest. HDA will search only for products that were obtained between those two dates.\n\nOutput directory will define directory for downloaded products.\n\nGeometry wll define our area of interest. It will be passed as BBOX (Bounding Box), as a list of coordinates - Xmin, Ymin, Xmax, Ymax. All coordinates will be defined in EPSG:4326.\n\n# Timerange of data that we want to recieve\nstart_date = '2023-07-01'\nend_date = '2023-07-07'\n# Output directory of our desired data\noutput_dir = 'sen_1/'\n# Geometry in form of a BBOX\nbbox = [20.8510, 52.0976, 21.2712, 52.3345,]\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-1-3-prerequisites-data","position":9},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl2":"2. Work with HDA"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-work-with-hda","position":10},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl2":"2. Work with HDA"},"content":"HDA (Harmonized Data Access) uses STAC protocol, that allows its user access the Earth Observation data, stored in various provides. Thanks to that, HDA serves as an one stream of data, allowing for comfortable work with sattelite imagery.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-work-with-hda","position":11},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl2":"2.1 API URLs"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-1-api-urls","position":12},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl2":"2.1 API URLs"},"content":"HDA, as all API, is build upon many endpoints. In this notebook we will use only one for collections and searching. Below there are definitions of those endpoints. We will be using the common one site, but you can change it to central, lumi or leonardo if you want.\n\nCOLLECTIONS_URL = 'https://hda.data.destination-earth.eu/stac/collections'\nSEARCH_URL = 'https://hda.data.destination-earth.eu/stac/search'\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-1-api-urls","position":13},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl2":"2.2 Listing available collections"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-2-listing-available-collections","position":14},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl2":"2.2 Listing available collections"},"content":"Firstly lets see to what collections we can get access, while using HDA.\n\ndef get_stac_collections(api_url):\n    response = requests.get(api_url)\n    if response.status_code == 200:\n        stac_data = response.json()['collections']\n        collections = [x['id'] for x in stac_data]\n        return collections\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n    \nget_stac_collections(COLLECTIONS_URL)\n\nAs you can see, there are many dataset, that can be access using just one single tool - HDA. In this notebook we will use only Sentinel-1 (GRDH) images, so our collection will be EO.ESA.DAT.SENTINEL-1.L1_GRD.\n\ncollections = ['EO.ESA.DAT.SENTINEL-1.L1_GRD']\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-2-listing-available-collections","position":15},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"2.3 Authorization","lvl2":"2.2 Listing available collections"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-3-authorization","position":16},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"2.3 Authorization","lvl2":"2.2 Listing available collections"},"content":"As stated before, to use HDA you will need an account on DestinE. Using your credentials, you will be able to generate access token, that will be needed in upcoming requests. In listing collections’ cell you didn’t have to create token, because only more advanced requests (like listing, searching and downloading items) need it.\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-3-authorization","position":17},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"2.4 Find newest product","lvl2":"2.2 Listing available collections"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-4-find-newest-product","position":18},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"2.4 Find newest product","lvl2":"2.2 Listing available collections"},"content":"After defining all prerequired data and obtaining access token, we can start searching for interesting us products. To do that, we will firstly create body of a POST request with ours parameters. Then, we will send it to HDA and, if request is successful, we will read from response download link.\n\ndef search_items(access_token: str, search_url: str, collection: str, \n                 bbox: list[float | int], start_date: str, end_date: str):\n    body = {\n        'datetime': f'{start_date}T00:00:00Z/{end_date}T23:59:59Z',\n        'collections': [collection],\n        'bbox': bbox\n    }\n    response = requests.post(search_url, json=body, headers={'Authorization': 'Bearer {}'.format(access_token)})\n    if response.status_code != 200:\n        print(f'Error in search request: {response.status_code} - {response.text}')\n        return None\n    else:\n        print(\"Request successful! Reading data...\")\n        products_list = [(feature.get('assets').get('downloadLink').get('href'), feature.get('links')[0].get('title')) for feature in response.json().get('features', [])]\n        return products_list\n\ncollections_items = []\nfor c in collections:\n    collections_items.append(search_items(access_token, SEARCH_URL, c, bbox, start_date, end_date))\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-4-find-newest-product","position":19},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"2.5 Download founded images","lvl2":"2.2 Listing available collections"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-5-download-founded-images","position":20},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"2.5 Download founded images","lvl2":"2.2 Listing available collections"},"content":"After obtaining URLs for each interesting us product, we can download it with one request. Product will be downloaded compressed in zip format.\n\ndef hda_download(access_token: str, url: str, output: str):\n    response = requests.get(url,stream=True,headers={'Authorization': 'Bearer {}'.format(access_token), 'Accept-Encoding': None})\n    if response.status_code == 200:\n        print('Downloading dataset...')\n        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n            z.extractall(output)\n        print('The dataset has been downloaded to: {}'.format(output))\n    else:\n        print('Request Unsuccessful! Error-Code: {}'.format(response.status_code))\n\nfor collection in collections_items:\n    for item in collection:\n        url = item[0]\n        product_id = item[1]\n        download_path = output_dir + product_id\n        hda_download(access_token, url, download_path)\n        break\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-2-5-download-founded-images","position":21},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl2":"4. Urban areas extraction by tresholding"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-urban-areas-extraction-by-tresholding","position":22},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl2":"4. Urban areas extraction by tresholding"},"content":"The analysis of Synthetic Aperture Radar (SAR) images enables the detection of variations in radar wave reflections, which is crucial for identifying urbanized areas. SAR images exhibit distinct reflection patterns that allow for the delineation of urban areas, including agglomerations and smaller urban settlements. Urbanized areas stand out in these images primarily due to the intense reflection of radar waves by man-made structures, roads, and other artificial elements of infrastructure, facilitating their clear identification contrasted with natural terrain. Such analysis of areas like the Warsaw agglomeration can serve for monitoring city growth and development, urban planning, assessment of natural and anthropogenic threats, and environmental management. The image clearly depicts the Warsaw agglomeration area, adjacent to the region between the Warsaw and Łódź agglomerations. This space is currently planned for development to connect both agglomerations. In the central part of the frame, there are plans for establishing a large airport complex.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-urban-areas-extraction-by-tresholding","position":23},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"4.1 Reading and preprocessing data","lvl2":"4. Urban areas extraction by tresholding"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-1-reading-and-preprocessing-data","position":24},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"4.1 Reading and preprocessing data","lvl2":"4. Urban areas extraction by tresholding"},"content":"The first step in our analysis will be data reading. The data will be read using the rasterio library from the output folder path provided in the previous section.\n\n# Function to find files containing specific substrings (\"vv\" and \"vh\") in their names\ndef find_files_with_names(folder_path, *names):\n    vv_path = None  \n    vh_path = None  \n    \n    # Traverse the directory tree starting at folder_path\n    for root, dirs, files in os.walk(folder_path):\n        if dirs:  # Check if there are subdirectories\n            first_subfolder = dirs[0]\n            measurement_folder = os.path.join(root, first_subfolder, \"measurement\")\n            \n            if os.path.isdir(measurement_folder):\n                # Look through the files in the \"measurement\" folder\n                for file_name in os.listdir(measurement_folder):\n                    if \"vv\" in file_name:\n                        vv_path = os.path.join(measurement_folder, file_name)\n                    elif \"vh\" in file_name:\n                        vh_path = os.path.join(measurement_folder, file_name)\n                break  # Exit after finding the files\n    return vv_path, vh_path\n\n# Find the \"vv\" and \"vh\" files in the specified directory\ninput_file_vv, input_file_vh = find_files_with_names(output_dir, \"vv\", \"vh\")\n\n# Open the \"vv\" file and read its data and profile\nwith rasterio.open(input_file_vv) as src_vv:\n    vv_band = src_vv.read(1)\n    profile_vv = src_vv.profile\n\n# Open the \"vh\" file and read its data and profile\nwith rasterio.open(input_file_vh) as src_vh:\n    vh_band = src_vh.read(1)\n    profile_vh = src_vh.profile\n\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-1-reading-and-preprocessing-data","position":25},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"4.2 Pixel value covertion to decibels","lvl2":"4. Urban areas extraction by tresholding"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-2-pixel-value-covertion-to-decibels","position":26},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"4.2 Pixel value covertion to decibels","lvl2":"4. Urban areas extraction by tresholding"},"content":"After reading the data, we can convert the pixel values to decibels. Data from Sentinel-1 is converted to the decibel scale (dB) to better visualize differences in the intensity of radar wave reflection, which aids in the analysis of urban areas due to their specific reflection characteristics.\n\ndef convert_to_db(image):\n    # Replace zeros with a small positive value to avoid taking log of zero\n    image[image == 0] = np.finfo(float).eps  \n\n    # Calculate dB values\n    image_db = 10 * np.log10(image)\n\n    # Replace infinite values with the maximum finite value\n    max_value = np.nanmax(image_db[np.isfinite(image_db)])  \n    image_db[np.isinf(image_db)] = max_value  \n\n    return image_db\n\nvv_band_db = convert_to_db(vv_band)\nvh_band_db = convert_to_db(vh_band)\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-2-pixel-value-covertion-to-decibels","position":27},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"4.3 Data analysis and tresholding","lvl2":"4. Urban areas extraction by tresholding"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-3-data-analysis-and-tresholding","position":28},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"4.3 Data analysis and tresholding","lvl2":"4. Urban areas extraction by tresholding"},"content":"The next step involves plotting histograms of the decibel pixel values for both the VV and VH bands. These histograms provide insights into the distribution of radar reflection intensities, aiding in understanding the characteristics of the observed area. The chosen thresholds, 23.5 dB for VV and 21.5 dB for VH, are selected based on prior knowledge or analysis requirements specific to urban areas to segment regions of interest in the images. The thresholding method applied here converts pixel values above the threshold to 1 and those below to 0, aiding in the delineation of features or areas with specific radar reflection characteristics typical of urban environments.\n\nfig, axs = plt.subplots(1, 2, figsize=(18, 5))\n\n# Histogram of VV Image in dB\naxs[0].hist(vv_band_db.flatten(), bins=50, color='blue', alpha=0.7, label='VV dB')\naxs[0].set_title('Histogram of VV Image in dB')\naxs[0].set_xlabel('Radar Reflection Value (dB)')\naxs[0].set_ylabel('Number of Pixels')\naxs[0].legend()\naxs[0].grid(True, linestyle='--', alpha=0.5)\n\n# Histogram of VH Image in dB\naxs[1].hist(vh_band_db.flatten(), bins=50, color='red', alpha=0.7, label='VH dB')\naxs[1].set_title('Histogram of VH Image in dB')\naxs[1].set_xlabel('Radar Reflection Value (dB)')\naxs[1].set_ylabel('Number of Pixels')\naxs[1].legend()\naxs[1].grid(True, linestyle='--', alpha=0.5)\n\nplt.show()\n\n# Function to apply a threshold to an image\n# Pixels greater than the threshold are set to 1, others to 0\ndef thresholding(image, threshold):\n    return (image > threshold).astype(np.uint8)\n\n# Define threshold values for VV and VH bands\nthreshold_value_vv = 23.5\nthreshold_value_vh = 21.5\n\n# Apply thresholding to the VV and VH band\nvv_band_threshold = thresholding(vv_band_db, threshold_value_vv)\nvh_band_threshold = thresholding(vh_band_db, threshold_value_vh)\n\n# Combine the thresholded images using a logical AND operation\n# The result is 1 where both VV and VH are above their respective thresholds\ncombined_threshold = np.logical_and(vv_band_threshold, vh_band_threshold).astype(np.uint8)\n\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-3-data-analysis-and-tresholding","position":29},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"4.4 Images ploting","lvl2":"4. Urban areas extraction by tresholding"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-4-images-ploting","position":30},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"4.4 Images ploting","lvl2":"4. Urban areas extraction by tresholding"},"content":"Next on the subplots we can visualize the differences in radio signal reflection for various polarization types. By displaying the original and decibel-transformed images for both VV and VH polarizations, we can observe distinct features and intensities in urban areas. Furthermore, by combining information from both polarizations through thresholding, we enhance our ability to mask urbanized regions, providing a more comprehensive understanding of the observed area’s urban characteristics.\n\n# Create a figure with a 3x2 grid of subplots, setting the overall figure size\nfig, ax = plt.subplots(3, 2, figsize=(18, 18)) \n\n# Display the original VV image\nax[0, 1].imshow(vv_band, cmap='viridis')\nax[0, 1].set_title('Original VV Image')\n\n# Display the VV image in decibels\nax[1, 1].imshow(vv_band_db, cmap='viridis')\nax[1, 1].set_title('VV Image in dB')\n\n# Display the VV image after thresholding\nax[2, 1].imshow(vv_band_threshold, cmap='inferno')\nax[2, 1].set_title('VV Image after Thresholding')\n\n# Display the original VH image\nax[0, 0].imshow(vh_band, cmap='viridis')\nax[0, 0].set_title('Original VH Image')\n\n# Display the VH image in decibels\nax[1, 0].imshow(vh_band_db, cmap='viridis')\nax[1, 0].set_title('VH Image in dB')\n\n# Display the VH image after thresholding\nax[2, 0].imshow(vh_band_threshold, cmap='inferno')\nax[2, 0].set_title('VH Image after Thresholding')\n\n# Turn off the axes for all subplots to improve the visual presentation\nfor i in range(2):\n    for j in range(3):\n        ax[j, i].axis('off')\n\n# Show the complete figure\nplt.show()\n\n# Display the combined threshold image using a specific colormap\nplt.figure(figsize=(18, 9))  \nplt.imshow(combined_threshold, cmap='twilight_shifted')\nplt.title('Combined Threshold Image')  \nplt.axis('off')  \nplt.show()\n\nThis method of data analysis facilitates straightforward masking of urbanized areas. In the processed image, the Warsaw agglomeration is clearly visible on the right side. A star-shaped accumulation of population is apparent in Warsaw along major transportation corridors such as railways and expressways. Smaller urban centers are visible in the image. Furthermore, it can be observed that the number of masked pixels accumulates as we approach the city center, indicating denser urban development in the downtown area compared to the suburbs.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-4-images-ploting","position":31},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"4.5 Cuantitive analysis","lvl2":"4. Urban areas extraction by tresholding"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-5-cuantitive-analysis","position":32},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"4.5 Cuantitive analysis","lvl2":"4. Urban areas extraction by tresholding"},"content":"After processing our satellite images, we can conduct quantitative analysis. The characteristics of the satellite data enable us to analyze individual pixel values, which is crucial for detailed and statistical studies. In our case, having the data thresholded and the size of each pixel being 10x10 meters, we can calculate the percentage and the adequate size of the urbanized area.\n\n# Calculate the number of urban pixels by summing up the binary values in the combined threshold image\nurban_pixels = np.sum(combined_threshold)\n\n# Define the area covered by a single pixel (in square meters)\npixel_area = 10 * 10  # Assuming each pixel represents a 10m x 10m area\n\n# Calculate the total urban area in square meters\nurban_area = urban_pixels * pixel_area\n\n# Calculate the total number of pixels in the combined threshold image\ntotal_pixels = combined_threshold.size\n\n# Calculate the percentage of urbanized area in the image\nurban_percentage = (urban_pixels / total_pixels) * 100\n\n# Convert urban area from square meters to hectares\nurban_area_hectares = urban_area / 10_000  \n\n# Convert total area from square meters to hectares\ntotal_area_hectares = (total_pixels * pixel_area) / 10_000\n\nprint(f\"Percentage of Urbanized Area: {urban_percentage:.2f}%\")\nprint(f\"Urbanized Area: {urban_area_hectares:.2f} ha\")\nprint(f\"Total Image Area: {total_area_hectares:.2f} ha\")\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-5-cuantitive-analysis","position":33},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"4.6 Results download","lvl2":"4. Urban areas extraction by tresholding"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-6-results-download","position":34},{"hierarchy":{"lvl1":"Using HDA to Find and Download Data for Urban Area Monitoring Using Sentinel-1 Data","lvl3":"4.6 Results download","lvl2":"4. Urban areas extraction by tresholding"},"content":"\n\n# Define output file names\noutput_file_db_vv = 'vv_db_urban_warsaw.tif'\noutput_file_db_vh = 'vh_db_urban_warsaw.tif'\noutput_file_threshold_combined = 'warsaw_urban_threshold_combined.tif'\n\n# Update the data type in the profile to float32 for dB conversion\nprofile_vv.update(dtype=rasterio.float32)\nprofile_vh.update(dtype=rasterio.float32)\n\n# Write the result data in dB to the output file\nwith rasterio.open(output_file_db_vv, 'w', **profile_vv) as dst:\n    dst.write(vv_band_db.astype(np.float32), 1)\n    \nwith rasterio.open(output_file_db_vh, 'w', **profile_vh) as dst:\n    dst.write(vh_band_db.astype(np.float32), 1)\n\n# Update the data type in the VV profile to uint8 for the combined thresholded image\nprofile_vv.update(dtype=rasterio.uint8)\n\n# Write the combined thresholded image data to the output file\nwith rasterio.open(output_file_threshold_combined, 'w', **profile_vv) as dst:\n    dst.write(combined_threshold, 1)","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-1-l1-grd#id-4-6-results-download","position":35},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake"},"type":"lvl1","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a","position":0},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake"},"content":"🚀 Launch in JupyterHub\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a","position":1},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"1. Prerequisites"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-1-prerequisites","position":2},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"1. Prerequisites"},"content":"\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-1-prerequisites","position":3},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"1.1 DestinE account","lvl2":"1. Prerequisites"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-1-1-destine-account","position":4},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"1.1 DestinE account","lvl2":"1. Prerequisites"},"content":"Firstly, to work with HDA we will need account on DestinE Core Service Platfrom website. You can register under this url: \n\nhttps://​platform​.destine​.eu/\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-1-1-destine-account","position":5},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"1.2 Python’s packages","lvl2":"1. Prerequisites"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-1-2-pythons-packages","position":6},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"1.2 Python’s packages","lvl2":"1. Prerequisites"},"content":"\n\nimport requests\nimport zipfile\nimport io\nimport destinelab as deauth\nfrom getpass import getpass\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-1-2-pythons-packages","position":7},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"1.3 Prerequiared data","lvl2":"1. Prerequisites"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-1-3-prerequiared-data","position":8},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"1.3 Prerequiared data","lvl2":"1. Prerequisites"},"content":"Before reuqesting some data from DEDL HDA, let’s specify what data we want to obtain. We will define 3 variables:\n\nStart date and end date,\n\nOutput directory,\n\nGeometry of interesting us area\n\nStart date and end date will define our timerange in reuqest. HDA will search only for products that were obtained between those two dates.\n\nOutput directory will define directory for downloaded products.\n\nGeometry wll define our area of interest. It will be passed as BBOX (Bounding Box), as a list of coordinates - Xmin, Ymin, Xmax, Ymax. All coordinates will be defined in EPSG:4326.\n\n# Timerange of data that we want to recieve\nstart_date = '2023-07-01'\nend_date = '2023-07-07'\n# Output directory of our desired data\noutput_dir = 'output/'\n# Geometry in form of a BBOX\nbbox = [21.61868,53.66627,21.90926,53.82351]\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-1-3-prerequiared-data","position":9},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"2. Work with HDA"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-work-with-hda","position":10},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"2. Work with HDA"},"content":"HDA (Harmonized Data Access) uses STAC protocol, that allows its user access the Earth Observation data, stored in various provides. Thanks to that, HDA serves as an one stream of data, allowing for comfortable work with sattelite imagery.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-work-with-hda","position":11},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"2.1 API URLs"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-1-api-urls","position":12},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"2.1 API URLs"},"content":"HDA, as all API, is build upon many endpoints. In this notebook we will use only one for collections and searching. Below there are definitions of those endpoints. We will be using the common one site, but you can change it to central, lumi or leonardo if you want.\n\nCOLLECTIONS_URL = 'https://hda.data.destination-earth.eu/stac/collections'\nSEARCH_URL = 'https://hda.data.destination-earth.eu/stac/search'\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-1-api-urls","position":13},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"2.2 Listing available collections"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-2-listing-available-collections","position":14},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"2.2 Listing available collections"},"content":"Firstly lets see to which collections we can get access, while using HDA.\n\ndef get_stac_collections(api_url):\n    response = requests.get(api_url)\n    if response.status_code == 200:\n        stac_data = response.json()['collections']\n        collections = [x['id'] for x in stac_data]\n        return collections\n    else:\n        print(f\"Error: {response.status_code} - {response.text}\")\n        return None\n    \nget_stac_collections(COLLECTIONS_URL)\n\nAs you can see, there are many dataset, that can be access using just one single tool - HDA. In this notebook we will use only Sentinel-2 images, so our collections will be EO.ESA.DAT.SENTINEL-2.MSI.L1C and EO.ESA.DAT.SENTINEL-2.MSI.L2A.\n\ncollections = ['EO.ESA.DAT.SENTINEL-2.MSI.L1C', 'EO.ESA.DAT.SENTINEL-2.MSI.L2A']\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-2-listing-available-collections","position":15},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"2.3 Authorization","lvl2":"2.2 Listing available collections"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-3-authorization","position":16},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"2.3 Authorization","lvl2":"2.2 Listing available collections"},"content":"As stated before, to use HDA you will need an account on DestinE. Using your credentials, you will be able to generate access token, that will be needed in upcoming requests. In listing collections’ cell you didn’t have to create token, because only more advanced requests (like listing, searching and downloading items) need it.\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-3-authorization","position":17},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"2.4 Find newest product","lvl2":"2.2 Listing available collections"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-4-find-newest-product","position":18},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"2.4 Find newest product","lvl2":"2.2 Listing available collections"},"content":"After defining all prerequired data and obtaining access token, we can start searching for interesting us products. To do that, we will firstly create body of a POST request with ours parameters. Then, we will send it to HDA and, if request is successful, we will read from response download link.\n\ndef search_items(access_token: str, search_url: str, collection: str, \n                 bbox: list[float | int], start_date: str, end_date: str):\n    body = {\n        'datetime': f'{start_date}T00:00:00Z/{end_date}T23:59:59Z',\n        'collections': [collection],\n        'bbox': bbox\n    }\n    response = requests.post(search_url, json=body, headers={'Authorization': 'Bearer {}'.format(access_token)})\n    if response.status_code != 200:\n        print(f'Error in search request: {response.status_code} - {response.text}')\n        return None\n    else:\n        print(\"Request successful! Reading data...\")\n        products_list = [(feature.get('assets').get('downloadLink').get('href'), feature.get('links')[0].get('title')) for feature in response.json().get('features', [])]\n        return products_list\n\nTo obtain products from two levels of Sentinel-2 - L2A and L1C, we will use loop, iterating over every single collection.\n\ncollections_items = []\nfor c in collections:\n    collections_items.append(search_items(access_token, SEARCH_URL, c, bbox, start_date, end_date))\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-4-find-newest-product","position":19},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"2.5 Download founded images","lvl2":"2.2 Listing available collections"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-5-download-founded-images","position":20},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"2.5 Download founded images","lvl2":"2.2 Listing available collections"},"content":"After obtaining download links for each of interesting us product, we can finally download it. With single request, we will download compressed product in zip format to provided directory. Function will set filename as product’s id. Mind that Sentinel-2 products might be over 1 GB, so it may take a few minutes to download them, based on your internet connection.\n\ndef hda_download(access_token: str, url: str, output: str):\n    response = requests.get(url,stream=True,headers={'Authorization': 'Bearer {}'.format(access_token), 'Accept-Encoding': None})\n    if response.status_code == 200:\n        print('Downloading dataset...')\n        with zipfile.ZipFile(io.BytesIO(response.content)) as z:\n            z.extractall(output)\n        print('The dataset has been downloaded to: {}'.format(output))\n    else:\n        print('Request Unsuccessful! Error-Code: {}'.format(response.status_code))\n\nFrom previous section, we obtained 2D list - with one dimension being collection and second being one item (single product). Becouse of that, we will use two loops to iterate over single products.\n\nfor collection in collections_items:\n    for item in collection:\n        url = item[0]\n        product_id = item[1]\n        download_path = output_dir + product_id\n        hda_download(access_token, url, download_path)\n        break\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-2-5-download-founded-images","position":21},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"4. Simple data computing - obtaining NDWI"},"type":"lvl2","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-4-simple-data-computing-obtaining-ndwi","position":22},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl2":"4. Simple data computing - obtaining NDWI"},"content":"In this chapter we will conduct simple data computing. As stated before, this notebook concentrate on monitoring of Śniadrwy lake, so we will try to calculate NDWI index for each pixel and create raster from it. Using all downloaded items, we will be able to monitor lake status from entire month.\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-4-simple-data-computing-obtaining-ndwi","position":23},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"4.1 Libraries","lvl2":"4. Simple data computing - obtaining NDWI"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-4-1-libraries","position":24},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"4.1 Libraries","lvl2":"4. Simple data computing - obtaining NDWI"},"content":"In this chapter we will try to compute obtained by us imagery data, with usage of Python and its spatial-oriented packages.\n\nimport rasterio\nfrom osgeo import gdal, gdal_array, osr\nimport numpy as np\nimport os\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-4-1-libraries","position":25},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"4.2 Functions for reading, calculating and saving raster data","lvl2":"4. Simple data computing - obtaining NDWI"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-4-2-functions-for-reading-calculating-and-saving-raster-data","position":26},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"4.2 Functions for reading, calculating and saving raster data","lvl2":"4. Simple data computing - obtaining NDWI"},"content":"Here we present you some functions for reading raster data into Numpy matrix, calculating NDWI with NIR and GREEN matrixes and saving result as a new raster. We will conduct such calculation for each downloaded item. In the end, we will obtain NDWI data on Śniadrwy Lake from whole week.\n\ndef getFullPath(dir: str, resolution: int, band: str):\n    if not os.path.isdir(dir):\n        raise ValueError(f\"Provided path does not exist: {dir}\")\n    elif resolution not in [10,20,60]:\n        raise ValueError(f\"Provided resolution does not exist: R{resolution}m\")\n    else:\n        full_path = dir\n        while True:\n            content = os.listdir(full_path)\n            if len(content) == 0:\n                raise ValueError(f\"Directory empty: {full_path}\")\n            elif len(content) == 1:\n                if full_path[-1] != '/':\n                    full_path = full_path + '/' + content[0]\n                else:\n                    full_path = full_path + content[0]\n            else:\n                if 'GRANULE' in content:\n                    full_path = full_path + '/' + 'GRANULE'\n                    break\n                else:\n                    raise ValueError(f\"Unsupported dir architecture: {full_path}\")\n        full_path = full_path + '/' + os.listdir(full_path)[0]\n        full_path = full_path + '/' + \"IMG_DATA\"\n        if len(os.listdir(full_path)) == 3:\n            full_path = full_path + '/' + f'R{resolution}m'\n            images = os.listdir(full_path)\n            for img in images:\n                if band in img:\n                    return full_path + '/' + img\n            raise ValueError(f'No such band {band} in directory: {full_path}')\n        else:\n            images = os.listdir(full_path)\n            for img in images:\n                if band in img:\n                    return full_path + '/' + img\n            raise ValueError(f'No such band {band} in directory: {full_path}')\n\n# Get transformation matrix from raster\ndef getTransform(pathToRaster):\n    dataset = gdal.Open(pathToRaster)\n    transformation = dataset.GetGeoTransform()\n    return transformation\n\n# Read raster and return pixels' values matrix as int16, new transformation matrix, crs\ndef readRaster(path, resolution, band):\n    path = getFullPath(path, resolution, band)\n    trans = getTransform(path) # trzeba zdefiniować który kanał\n    raster, crs = rasterToMatrix(path)\n    return raster.astype(np.int16), crs, trans\n\ndef rasterToMatrix(pathToRaster):\n    with rasterio.open(pathToRaster) as src:\n        matrix = src.read(1)\n    return matrix, src.crs.to_epsg()\n\n# Transform numpy's matrix to geotiff; pass new raster's filepath, matrix with pixels' values, gdal file type, transformation matrix, projection, nodata value\ndef npMatrixToGeotiff(filepath, matrix, gdalType, projection, transformMatrix, nodata = None):\n    driver = gdal.GetDriverByName('Gtiff')\n    if len(matrix.shape) > 2:\n        (bandNr, yRes, xRes) = matrix.shape\n        image = driver.Create(filepath, xRes, yRes, bandNr, gdalType)\n        for b in range(bandNr):\n            b = b + 1\n            band = image.GetRasterBand(b)\n            if nodata is not None:\n                band.SetNoDataValue(nodata)\n            band.WriteArray(matrix[b-1,:,:])\n            band.FlushCache\n    else:\n        bandNr = 1\n        (yRes, xRes) = matrix.shape\n        image = driver.Create(filepath, xRes, yRes, bandNr, gdalType)\n        print(type(image))\n        band = image.GetRasterBand(bandNr)\n        if nodata is not None:\n            band.SetNoDataValue(nodata)\n        band.WriteArray(matrix)\n        band.FlushCache\n    image.SetGeoTransform(transformMatrix)\n    image.SetProjection(projection)\n    del driver, image, band\n\n","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-4-2-functions-for-reading-calculating-and-saving-raster-data","position":27},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"4.3 Computing","lvl2":"4. Simple data computing - obtaining NDWI"},"type":"lvl3","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-4-3-computing","position":28},{"hierarchy":{"lvl1":"How to use HDA to find and download data for conducting monitoring of Śniadrwy lake","lvl3":"4.3 Computing","lvl2":"4. Simple data computing - obtaining NDWI"},"content":"With usage of defined functions, we will now generate NWDI rasters. Only data that will be needed in this step is a list with paths to our products (extracted from zip archive). Function readRaster will choose specified band from specified path.\n\n# List of products' paths. If your output directory contains more than just downloaded products, please provide them in a list, just like in the commented lines below\n#dataset = [output_dir+x for x in os.listdir(output_dir)]\ndataset = [\n     'output/S2B_MSIL1C_20230701T094549_N0509_R079_T34UEE_20230701T104205'\n ]\n# Output directiry for new images\ncompution_output = 'output/ndwi_rasters'\n\n# Iterating over single product\nfor item in dataset:\n    # Reading name from path\n    name = item.split('/')[-1]\n    # Reading green band into matrix\n    green = readRaster(item, 10, 'B03')\n    # Reading NIR band into matrix\n    nir = readRaster(item, 10, 'B08')\n    # Calculating NDWI matrix\n    ndwi = (green[0]-nir[0]) / (green[0]+nir[0])\n    # Setting treshhold for water-containing pixels\n    ndwi[ndwi >= 0] = 1\n    ndwi[ndwi < 0] = 0\n    # Creating SpatialReference object and setting it to match original's raster CRS\n    projection = osr.SpatialReference()\n    projection.ImportFromEPSG(green[1])\n    # Creating raster from matrix in GeoTiff format\n    npMatrixToGeotiff(f'{compution_output}/{name}.tif', ndwi, gdal_array.NumericTypeCodeToGDALTypeCode(np.float32), projection.ExportToWkt(), green[2])\n\nAfter successfuly creating and saving new images, we can now visualize them in Python using raterio package.\n\nimg = rasterio.open('output/ndwi_rasters/S2B_MSIL1C_20230701T094549_N0509_R079_T34UEE_20230701T104205.tif')\nfrom rasterio.plot import show\nshow(img)","type":"content","url":"/dedl-hda-eo-esa-dat-sentinel-2-msi-l2a#id-4-3-computing","position":29},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables"},"type":"lvl1","url":"/hda-rest-queryables","position":0},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables"},"content":"🚀 Launch in JupyterHub\n\n","type":"content","url":"/hda-rest-queryables","position":1},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl4":"How to use the queryables API"},"type":"lvl4","url":"/hda-rest-queryables#how-to-use-the-queryables-api","position":2},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl4":"How to use the queryables API"},"content":"The queryables API returns a list of variable terms that can be used for filtering the specified collection.\n\nThe queryables API presents the appropriate filters for the selected dataset, determined by the chosen values. If the user selects a certain variable, the choice is narrowed down for other variables. This notebook demonstrates how to filter data in a specific collection using the list of variable terms returned by the queryables API.\n\nThe notebook is dedicated to collections from C3S and DestinE digital twins. C3S and DestinE digital twins datasets offer a wide range of possibilities in terms of information and constraints and the queryables API is a valuable tool for exploring these datasets.\n\nThroughout this notebook, you will learn:\n\nAuthenticate: How to authenticate for searching and access DEDL collections.\n\nQueryables: How to exploit the STAC API filter extension features. The “queryables” API helps users to determine the property names and types available for filtering data.\n\nSearch data:  How to search DEDL data using filters obtained by the “queryables” API.\n\nDownload data: How to download DEDL data through HDA.\n\nThe detailed HDA API and definition of each endpoint and parameters is available in the HDA Swagger UI at:\n\n\n STAC API - Filter Extension \n\nPrequisites: - For queryables API: none - For filtering data inside collections : \n\nDestinE user account\n\n","type":"content","url":"/hda-rest-queryables#how-to-use-the-queryables-api","position":3},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl2":"Authenticate"},"type":"lvl2","url":"/hda-rest-queryables#authenticate","position":4},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl2":"Authenticate"},"content":"","type":"content","url":"/hda-rest-queryables#authenticate","position":5},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl3":"Define some constants for the API URLs","lvl2":"Authenticate"},"type":"lvl3","url":"/hda-rest-queryables#define-some-constants-for-the-api-urls","position":6},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl3":"Define some constants for the API URLs","lvl2":"Authenticate"},"content":"In this section, we define the relevant constants, holding the URL strings for the different endpoints.\n\n# Collection https://hda.data.destination-earth.eu/ui/dataset/EO.ECMWF.DAT.CAMS_EUROPE_AIR_QUALITY_FORECASTS as default\nCOLLECTION_ID = \"EO.ECMWF.DAT.CAMS_EUROPE_AIR_QUALITY_FORECASTS\"\n\n# Core API\nHDA_API_URL = \"https://hda.data.destination-earth.eu\"\n\n# STAC API\n## Core\nSTAC_API_URL = f\"{HDA_API_URL}/stac\"\n\n## Item Search\nSEARCH_URL = f\"{STAC_API_URL}/search\"\n\n## Collections\nCOLLECTIONS_URL = f\"{STAC_API_URL}/collections\"\n\n## Queryables\nQUERYABLES_URL = f\"{STAC_API_URL}/queryables\"\nQUERYABLES_BY_COLLECTION_ID = f\"{COLLECTIONS_URL}/{COLLECTION_ID}/queryables\"\nHDA_FILTERS =''\n\n## HTTP Success\nHTTP_SUCCESS_CODE = 200\n\n","type":"content","url":"/hda-rest-queryables#define-some-constants-for-the-api-urls","position":7},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl3":"Import the relevant modules and define some functions","lvl2":"Authenticate"},"type":"lvl3","url":"/hda-rest-queryables#import-the-relevant-modules-and-define-some-functions","position":8},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl3":"Import the relevant modules and define some functions","lvl2":"Authenticate"},"content":"We start off by importing the relevant modules for DestnE authentication, HTTP requests, json handling, widgets and some utility.\n\nimport destinelab as deauth\n\npip install --quiet --upgrade DEDLUtils\n\nfrom DEDLUtils import dedl_utilities\n#import dedl_utilities\n\nimport requests\nimport json\nfrom getpass import getpass\n\nimport ipywidgets as widgets\nfrom IPython.display import display, clear_output, HTML\nfrom ipywidgets import Layout, Box\nimport datetime\n\nfrom rich.console import Console\nimport rich.table\n\nfrom IPython.display import JSON\n\n\n","type":"content","url":"/hda-rest-queryables#import-the-relevant-modules-and-define-some-functions","position":9},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl4":"Obtain Authentication Token","lvl3":"Import the relevant modules and define some functions","lvl2":"Authenticate"},"type":"lvl4","url":"/hda-rest-queryables#obtain-authentication-token","position":10},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl4":"Obtain Authentication Token","lvl3":"Import the relevant modules and define some functions","lvl2":"Authenticate"},"content":"To perform a query on HDA we need to be authenticated.\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/hda-rest-queryables#obtain-authentication-token","position":11},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl2":"Queryables"},"type":"lvl2","url":"/hda-rest-queryables#queryables","position":12},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl2":"Queryables"},"content":"The “queryables” API helps users to determine the property names and types available for filtering data inside a specific collection.\n\nBelow a dropdown menu to choose the collection. We can choose the collection of which we want to inspect the filters.\n\nprefix = \"EO.ECMWF\"\n\ndedlUtils=dedl_utilities.DEDLUtilities(COLLECTION_ID )\ndedlUtils.create_collections_dropdown(prefix)\n\n# Layout\n# Define the layout for the dropdown\ndropdown_layout = Layout(display='space-between', justify_content='center', width='90%')\n# Create a box to hold the dropdown with the specified layout\nbox = Box([dedlUtils.dropdown, dedlUtils.outputArea], layout=dropdown_layout)\ndisplay( box)  \n\n\n\n\nThe QUERYABLES ENDPOINT (above) returns the applicable filters under the section named ‘properties’.\n\nThe ‘properties’ section contains\n\nthe name of the filter, title,\n\nthe filter type,\n\nthe possible filter values, enum (conditioned by the values selected for the other filters)\n\nand the the default (or chosen) default applied\n\nWe can print the’properties’ section for the selected collection in the table below.\nThe table shows the filters and the values applied by default when we perform a search for the chosen dataset without specifying any filter.\n\nfilters_resp=requests.get(dedlUtils.queryablesByCollectionId)\nfilters_resp.raise_for_status()\n\nfilters = filters_resp.json()[\"properties\"]\ndedlQueryablesUtils=dedl_utilities.DEDLQueryablesUtilities(dedlUtils.collectionId)\nconsole = Console()\nconsole.print(dedlQueryablesUtils.create_queryables_table(filters))\n\nCalling the queryables API specifying filters, that means using as parameters the values chosen for filtering the selected dataset, the API replies with the applicable filters, conditioned by the chosen values.\nThen if the user selects a certain value for a parameter then the choice is narrowed down for other variables.\n\nThe queryables API, in this way, helps user to build a correct search request for the given dataset.\n\nBelow an interactive example, to see that once you select a value for a property the choice is narrowed down for other variables.\n\n# Event listeners\ndedlQueryablesUtils=dedl_utilities.DEDLQueryablesUtilities(dedlUtils.collectionId)\ndedlQueryablesUtils.update_queryables_dropdowns()\ndisplay(dedlQueryablesUtils.dropdownContainer, dedlQueryablesUtils.outputArea)\n\n","type":"content","url":"/hda-rest-queryables#queryables","position":13},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl3":"Filtering a collection with the list returned by the queryable API","lvl2":"Queryables"},"type":"lvl3","url":"/hda-rest-queryables#filtering-a-collection-with-the-list-returned-by-the-queryable-api","position":14},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl3":"Filtering a collection with the list returned by the queryable API","lvl2":"Queryables"},"content":"This section wil show how to use the list of variable terms returned by the queryables API for filtering a specific dataset.\n\n","type":"content","url":"/hda-rest-queryables#filtering-a-collection-with-the-list-returned-by-the-queryable-api","position":15},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl5":"If you choose a digital twins collection, check if the access is granted","lvl3":"Filtering a collection with the list returned by the queryable API","lvl2":"Queryables"},"type":"lvl5","url":"/hda-rest-queryables#if-you-choose-a-digital-twins-collection-check-if-the-access-is-granted","position":16},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl5":"If you choose a digital twins collection, check if the access is granted","lvl3":"Filtering a collection with the list returned by the queryable API","lvl2":"Queryables"},"content":"If DT access is not granted, you will not be able to search and access DT data.\n\nauth.is_DTaccess_allowed(access_token)\n\n","type":"content","url":"/hda-rest-queryables#if-you-choose-a-digital-twins-collection-check-if-the-access-is-granted","position":17},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl4":"Build the query from the selected values","lvl3":"Filtering a collection with the list returned by the queryable API","lvl2":"Queryables"},"type":"lvl4","url":"/hda-rest-queryables#build-the-query-from-the-selected-values","position":18},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl4":"Build the query from the selected values","lvl3":"Filtering a collection with the list returned by the queryable API","lvl2":"Queryables"},"content":"The parameters chosen in the previous steps can be used to build the corresponding HDA queries.\n\n# The JSON objects containing the generic query parameters:\njsonGenericQuery = '{\"collections\": [\"'+dedlUtils.collectionId+'\"], \"datetime\": \"2024-04-01T00:00:00Z/2024-04-19T00:00:00Z\"}'\n# Convert JSON strings to Python dictionaries\ndictQuery = json.loads(jsonGenericQuery)\n\n# Include the filters selected in the previous steps inside the JSON containing the generic query parameters:\ndictQuery['query'] = dedlQueryablesUtils.hdaFilters\n# Convert the merged dictionary back to a JSON string\nqueryJson = json.dumps(dictQuery, indent=4)\n\nprint(queryJson)\n\n","type":"content","url":"/hda-rest-queryables#build-the-query-from-the-selected-values","position":19},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl3":"Search","lvl2":"Queryables"},"type":"lvl3","url":"/hda-rest-queryables#search","position":20},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl3":"Search","lvl2":"Queryables"},"content":"\n\nresponse = requests.post(\"https://hda.data.destination-earth.eu/stac/search\", headers=auth_headers, json= json.loads(queryJson) )\n#print(response)\n# Requests to ADS data always return a single item containing all the requested data\nproduct = response.json()[\"features\"][0]\nJSON(product, expanded= False)\n\n","type":"content","url":"/hda-rest-queryables#search","position":21},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl2":"Download"},"type":"lvl2","url":"/hda-rest-queryables#download","position":22},{"hierarchy":{"lvl1":"HDA Tutorial - Queryables","lvl2":"Download"},"content":"Once we have found the product we can download it:\n\ndownload_url = product[\"assets\"][\"downloadLink\"][\"href\"]\nprint(download_url )\n\nfrom tqdm import tqdm\nimport time\nimport re\ndirect_download_url=''\n\nresponse = requests.get(download_url, headers=auth_headers)\nif (response.status_code == HTTP_SUCCESS_CODE):\n    direct_download_url = product['assets']['downloadLink']['href']\nelse:\n    print(response.text)\n\n\n# we poll as long as the data is not ready\nif direct_download_url=='':\n    while url := response.headers.get(\"Location\"):\n        print(f\"order status: {response.json()['status']}\")\n        response = requests.get(url, headers=auth_headers, stream=True)\n        response.raise_for_status()\n\nfilename = re.findall('filename=\\\"?(.+)\\\"?', response.headers[\"Content-Disposition\"])[0]\ntotal_size = int(response.headers.get(\"content-length\", 0))\n\nprint(f\"downloading {filename}\")\n\nwith tqdm(total=total_size, unit=\"B\", unit_scale=True) as progress_bar:\n    with open(filename, 'wb') as f:\n        for data in response.iter_content(1024):\n            progress_bar.update(len(data))\n            f.write(data)","type":"content","url":"/hda-rest-queryables#download","position":23},{"hierarchy":{"lvl1":"HDA Tutorial"},"type":"lvl1","url":"/hda-rest-full-version","position":0},{"hierarchy":{"lvl1":"HDA Tutorial"},"content":"🚀 Launch in JupyterHub\n\n","type":"content","url":"/hda-rest-full-version","position":1},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"First steps using Harmonised Data access API"},"type":"lvl3","url":"/hda-rest-full-version#first-steps-using-harmonised-data-access-api","position":2},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"First steps using Harmonised Data access API"},"content":"- Discover data of DestinE Data Portfolio - Search data of DestinE Data Portfolio and visualize the results - Access Data of DestinE Data Portfolio and visualize the thumbnails\n\nThis notebook demonstrates how to use the HDA (Harmonized Data Access) API by sending a few HTTP requests to the API, using Python code.\n\nThroughout this quickstart notebook, you will learn:\n\nDiscover: How to discover DEDL services and data collections through HDA.\n\nAuthenticate: How to authenticate to search and access DEDL collections.\n\nSearch data:  How to search DEDL data through HDA.\n\nViisualize search results: How to see the results.\n\nDownload data: How to download DEDL data through HDA.\n\nThe detailed API and definition of each endpoint and parameters is available in the HDA Swagger UI at:\n\nhttps://​hda​.data​.destination​-earth​.eu​/docs/\n\nPrerequisites: - For Data discovery: none - For Data access : \n\nDestinE user account\n\n","type":"content","url":"/hda-rest-full-version#first-steps-using-harmonised-data-access-api","position":3},{"hierarchy":{"lvl1":"HDA Tutorial","lvl2":"Discover"},"type":"lvl2","url":"/hda-rest-full-version#discover","position":4},{"hierarchy":{"lvl1":"HDA Tutorial","lvl2":"Discover"},"content":"\n\n","type":"content","url":"/hda-rest-full-version#discover","position":5},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Settings","lvl2":"Discover"},"type":"lvl3","url":"/hda-rest-full-version#settings","position":6},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Settings","lvl2":"Discover"},"content":"","type":"content","url":"/hda-rest-full-version#settings","position":7},{"hierarchy":{"lvl1":"HDA Tutorial","lvl4":"Import the relevant modules","lvl3":"Settings","lvl2":"Discover"},"type":"lvl4","url":"/hda-rest-full-version#import-the-relevant-modules","position":8},{"hierarchy":{"lvl1":"HDA Tutorial","lvl4":"Import the relevant modules","lvl3":"Settings","lvl2":"Discover"},"content":"We start off by importing the relevant modules for HTTP requests and json handling.\n\nfrom typing import Union\nimport requests\nimport json\nimport urllib.parse\nfrom IPython.display import JSON\nfrom IPython.display import Image\n\nimport geopandas\nimport folium\nimport folium.plugins\nfrom branca.element import Figure\nimport shapely.geometry\n\n","type":"content","url":"/hda-rest-full-version#import-the-relevant-modules","position":9},{"hierarchy":{"lvl1":"HDA Tutorial","lvl4":"Define some constants for the API URLs","lvl3":"Settings","lvl2":"Discover"},"type":"lvl4","url":"/hda-rest-full-version#define-some-constants-for-the-api-urls","position":10},{"hierarchy":{"lvl1":"HDA Tutorial","lvl4":"Define some constants for the API URLs","lvl3":"Settings","lvl2":"Discover"},"content":"In this section, we define the relevant constants, holding the URL strings for the different endpoints.\n\n# IDS\nSERVICE_ID = \"dedl-hook\"\nCOLLECTION_ID = \"EO.EUM.DAT.SENTINEL-3.SL_1_RBT___\"\nITEM_ID = \"S3B_SL_1_RBT____20240918T102643_20240918T102943_20240919T103839_0179_097_336_2160_PS2_O_NT_004\"\n\n# Core API\nHDA_API_URL = \"https://hda.data.destination-earth.eu\"\nSERVICES_URL = f\"{HDA_API_URL}/services\"\nSERVICE_BY_ID_URL = f\"{SERVICES_URL}/{SERVICE_ID}\"\n\n# STAC API\n## Core\nSTAC_API_URL = f\"{HDA_API_URL}/stac\"\nCONFORMANCE_URL = f\"{STAC_API_URL}/conformance\"\n\n## Item Search\nSEARCH_URL = f\"{STAC_API_URL}/search\"\nDOWNLOAD_URL = f\"{STAC_API_URL}/download\"\n\n## Collections\nCOLLECTIONS_URL = f\"{STAC_API_URL}/collections\"\nCOLLECTION_BY_ID_URL = f\"{COLLECTIONS_URL}/{COLLECTION_ID}\"\n\n## Items\nCOLLECTION_ITEMS_URL = f\"{COLLECTIONS_URL}/{COLLECTION_ID}/items\"\nCOLLECTION_ITEM_BY_ID_URL = f\"{COLLECTIONS_URL}/{COLLECTION_ID}/items/{ITEM_ID}\"\n\n## HTTP Success\nHTTP_SUCCESS_CODE = 200\n\n","type":"content","url":"/hda-rest-full-version#define-some-constants-for-the-api-urls","position":11},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Core API","lvl2":"Discover"},"type":"lvl3","url":"/hda-rest-full-version#core-api","position":12},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Core API","lvl2":"Discover"},"content":"We can start off by requesting the HDA landing page, which provides links to the API definition, the available services  (links services and service-doc) as well as the STAC API index.\n\nresponse=requests.get(HDA_API_URL)\nJSON(response.json())\n\n","type":"content","url":"/hda-rest-full-version#core-api","position":13},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"STAC API","lvl2":"Discover"},"type":"lvl3","url":"/hda-rest-full-version#stac-api","position":14},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"STAC API","lvl2":"Discover"},"content":"The HDA is plugged to a STAC API.\nThe STAC API entry point is set to the /stac endpoint and provides the search capabilities provided by the DEDL STAC interface.\n\nprint(STAC_API_URL)\nJSON(requests.get(STAC_API_URL).json())\n\n","type":"content","url":"/hda-rest-full-version#stac-api","position":15},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Discover DEDL Services","lvl2":"Discover"},"type":"lvl3","url":"/hda-rest-full-version#discover-dedl-services","position":16},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Discover DEDL Services","lvl2":"Discover"},"content":"\n\nThe /services endpoint will return the list of the DEDL services available for users of the platform.\n\nprint(SERVICES_URL)\nJSON(requests.get(SERVICES_URL).json())\n\nThrough the /services endpoint is also possible discover services related to a certain topic:\n\nJSON(requests.get(SERVICES_URL,params = {\"q\": \"dask\"}).json())\n\nThe API can also describe a specific service, identified by its serviceID (e.g. dedl-hook).\n\nThe links describes and described by contains the reference documentation.\n\nprint(SERVICE_BY_ID_URL)\nJSON(requests.get(SERVICE_BY_ID_URL).json())\n\n","type":"content","url":"/hda-rest-full-version#discover-dedl-services","position":17},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Discover DEDL data collections","lvl2":"Discover"},"type":"lvl3","url":"/hda-rest-full-version#discover-dedl-data-collections","position":18},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Discover DEDL data collections","lvl2":"Discover"},"content":"It is also possible discover data collections related to a certain topic and provided by a certain provider in a specic time interval.\nWe specify an open time interval in order to have collections with data starting from a certain datetime.\n\nresponse = requests.get(COLLECTIONS_URL,params = {\"q\": \"ozone,methane,fire\",\"provider\":\"eumetsat\",\"datetime\":'2024-01-01T00:00:00Z/..'})\n\nJSON(response.json(), expanded=False)\n\n","type":"content","url":"/hda-rest-full-version#discover-dedl-data-collections","position":19},{"hierarchy":{"lvl1":"HDA Tutorial","lvl2":"Authenticate"},"type":"lvl2","url":"/hda-rest-full-version#authenticate","position":20},{"hierarchy":{"lvl1":"HDA Tutorial","lvl2":"Authenticate"},"content":"","type":"content","url":"/hda-rest-full-version#authenticate","position":21},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Obtain Authentication Token","lvl2":"Authenticate"},"type":"lvl3","url":"/hda-rest-full-version#obtain-authentication-token","position":22},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Obtain Authentication Token","lvl2":"Authenticate"},"content":"\n\nimport json\nimport os\nfrom getpass import getpass\nimport destinelab as deauth\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/hda-rest-full-version#obtain-authentication-token","position":23},{"hierarchy":{"lvl1":"HDA Tutorial","lvl2":"Search"},"type":"lvl2","url":"/hda-rest-full-version#search","position":24},{"hierarchy":{"lvl1":"HDA Tutorial","lvl2":"Search"},"content":"\n\n","type":"content","url":"/hda-rest-full-version#search","position":25},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"List Available Collections","lvl2":"Search"},"type":"lvl3","url":"/hda-rest-full-version#list-available-collections","position":26},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"List Available Collections","lvl2":"Search"},"content":"The /stac/collections endpoint returns a FeatureCollection object, listing all STAC collections available to the user.\n\nprint(COLLECTIONS_URL)\nJSON(requests.get(COLLECTIONS_URL).json())\n\nBy providing a specific collectionID (e.g. EO.EUM.DAT.SENTINEL-3.SL_1_RBT___), the user can get the metadata for a specific Collection.\nThe collection used for this tutorial is \n\nSLSTR Level 1B Radiances and Brightness Temperatures - Sentinel-3\n\nprint(COLLECTION_BY_ID_URL)\nJSON(requests.get(COLLECTION_BY_ID_URL).json())\n\n","type":"content","url":"/hda-rest-full-version#list-available-collections","position":27},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Search for Items in a specific collection","lvl2":"Search"},"type":"lvl3","url":"/hda-rest-full-version#search-for-items-in-a-specific-collection","position":28},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Search for Items in a specific collection","lvl2":"Search"},"content":"It is also possible to get the list of items available in a given Collection using a simple search and sorting the results.\n\nFILTER = \"?datetime=2024-09-18T00:00:00Z/2024-09-20T23:59:59Z&bbox=-10,34,-5,42.5&sortby=datetime&limit=5\"\n\nprint(COLLECTION_ITEMS_URL+FILTER)\nresponse=requests.get(COLLECTION_ITEMS_URL+FILTER, headers=auth_headers)  \n\nJSON(response.json())            \n\n","type":"content","url":"/hda-rest-full-version#search-for-items-in-a-specific-collection","position":29},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"The search endpoint","lvl2":"Search"},"type":"lvl3","url":"/hda-rest-full-version#the-search-endpoint","position":30},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"The search endpoint","lvl2":"Search"},"content":"The STAC API also provides an item endpoint (/stac/search).\nThis endpoint allows users to efficiently search for items that match the specified input filters.\n\nBy default, the /stac/search endpoint will return the first 20 items found in all the collections available at the /stac/collections endpoint.\nFilters can be added either via query parameters in a GET request or added to the JSON body of a POST request.\n\nThe full detail for each available filter is available in the \n\nAPI documentation.\n\nThe query parameters are added at the end of the URL as a query string: ?param1=val1&param2=val2&param3=val3\n\nFILTER = \"&datetime=2024-09-18T00:00:00Z/2024-09-20T23:59:59Z&bbox=-10,34,-5,42.5&sortby=datetime&limit=10\"\nSEARCH_QUERY_STRING = \"?collections=\"+COLLECTION_ID+FILTER\nresponse=requests.get(SEARCH_URL + SEARCH_QUERY_STRING, headers=auth_headers)\n\nJSON(response.json())    \n\nThe same filters can be added as the JSON body of a POST request.\n\nBODY = {\n    \"collections\": [\n        COLLECTION_ID,\n    ],\n    \"datetime\" : \"2024-09-18T00:00:00Z/2024-09-20T23:59:59Z\",\n    \"bbox\": [-10,34,\n              -5,42.5 ],\n    \"sortby\": [{\"field\": \"datetime\",\"direction\": \"desc\"}\n              ],\n    \"limit\": 10,\n}\n\nresponse=requests.post(SEARCH_URL, json=BODY, headers=auth_headers)\n\nJSON(response.json())    \n\n","type":"content","url":"/hda-rest-full-version#the-search-endpoint","position":31},{"hierarchy":{"lvl1":"HDA Tutorial","lvl2":"Visualize"},"type":"lvl2","url":"/hda-rest-full-version#visualize","position":32},{"hierarchy":{"lvl1":"HDA Tutorial","lvl2":"Visualize"},"content":"\n\n","type":"content","url":"/hda-rest-full-version#visualize","position":33},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Visualize search results in a table","lvl2":"Visualize"},"type":"lvl3","url":"/hda-rest-full-version#visualize-search-results-in-a-table","position":34},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Visualize search results in a table","lvl2":"Visualize"},"content":"Search results can be visualized on a map.\n\ndf = geopandas.GeoDataFrame.from_features(response.json()['features'], crs=\"epsg:4326\")\ndf.head()\n\n","type":"content","url":"/hda-rest-full-version#visualize-search-results-in-a-table","position":35},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Visualize search results in a map","lvl2":"Visualize"},"type":"lvl3","url":"/hda-rest-full-version#visualize-search-results-in-a-map","position":36},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Visualize search results in a map","lvl2":"Visualize"},"content":"\n\n#map1 = folium.Map([38, 0],\n#                  zoom_start=4, tiles='Esri Ocean Basemap', attr='Tiles &copy; Esri &mdash; Source: Esri, DeLorme, NAVTEQ')\n\n#map1 = folium.Map([38, 0],zoom_start=4)\n\nmap1 = folium.Map([38, 0],zoom_start=4, tiles=None)\n\nnasa_wms = folium.WmsTileLayer(\n    url='https://gibs.earthdata.nasa.gov/wms/epsg4326/best/wms.cgi',\n    name='NASA Blue Marble',\n    layers='BlueMarble_ShadedRelief',\n    format='image/png',\n    transparent=True,\n    attr='NASA'\n)\nnasa_wms.add_to(map1)\n\nresults=folium.GeoJson( response.json(),name='Search results',style_function=lambda feature: {\n        \"fillColor\": \"#005577\",\n        \"color\": \"black\",\n        \"weight\": 1\n    })\n\nresults.add_to(map1)\n\n\nbbox=[-10,34,-5,42.5]\nbb=folium.GeoJson(\n    shapely.geometry.box(*bbox),name='Search bounding box',style_function=lambda feature: {\n        \"fillColor\": \"#ff0000\",\n        \"color\": \"black\",\n        \"weight\": 2,\n        \"dashArray\": \"5, 5\",\n    }\n)\nbb.add_to(map1)\n\n# Add layer control to toggle visibility\nfolium.LayerControl().add_to(map1)\n\n\n#display(fig)\nmap1\n\n\n","type":"content","url":"/hda-rest-full-version#visualize-search-results-in-a-map","position":37},{"hierarchy":{"lvl1":"HDA Tutorial","lvl2":"Download"},"type":"lvl2","url":"/hda-rest-full-version#download","position":38},{"hierarchy":{"lvl1":"HDA Tutorial","lvl2":"Download"},"content":"The items belonging to a specific collection can be downloaded entirely, or it is possible to download a single asset of a chosen item.\n\n","type":"content","url":"/hda-rest-full-version#download","position":39},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Download a specific item","lvl2":"Download"},"type":"lvl3","url":"/hda-rest-full-version#download-a-specific-item","position":40},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Download a specific item","lvl2":"Download"},"content":"To get the metadata specific to a given item (identified by its itemID in a collection, the user can request the /stac/collections/{collectionID}/items/{itemID}endpoint.\n\nprint(COLLECTION_ITEM_BY_ID_URL)\nresponse=requests.get(COLLECTION_ITEM_BY_ID_URL, headers=auth_headers) \nJSON(response.json())             \n\nThe metadata of a given item contains also the download link that the user can use to download a specific item.\n\nresult = json.loads(response.text)\ndownloadUrl = result['assets']['downloadLink']['href']\nprint(downloadUrl)\n\nresp_dl = requests.get(downloadUrl,stream=True,headers=auth_headers)\n\n# If the request was successful, download the file\nif (resp_dl.status_code == HTTP_SUCCESS_CODE):\n        print(\"Downloading \"+ ITEM_ID + \"...\")\n        filename = ITEM_ID + \".zip\"\n        with open(filename, 'wb') as f:\n            for chunk in resp_dl.iter_content(chunk_size=1024): \n                if chunk:\n                    f.write(chunk)\n                    f.flush()\n        print(\"The dataset has been downloaded to: {}\".format(filename))\nelse: print(\"Request Unsuccessful! Error-Code: {}\".format(response.status_code))\n\n","type":"content","url":"/hda-rest-full-version#download-a-specific-item","position":41},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Download a specific asset of an item","lvl2":"Download"},"type":"lvl3","url":"/hda-rest-full-version#download-a-specific-asset-of-an-item","position":42},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Download a specific asset of an item","lvl2":"Download"},"content":"The metadata of a given item contains also the single assets download link, that the user can use to download a specific asset of the chosen item.\nIn the example below we download the asset: “xfdumanifest.xml”\n\ndownloadUrl = result['assets']['xfdumanifest.xml']['href']\nprint(downloadUrl)\n\nresp_dl = requests.get(downloadUrl,stream=True,headers=auth_headers)\n\n# If the request was successful, download the file\nif (resp_dl.status_code == HTTP_SUCCESS_CODE):\n        print(\"Downloading \"+ result['assets']['xfdumanifest.xml']['title'] + \"...\")\n        filename = result['assets']['xfdumanifest.xml']['title']\n        with open(filename, 'wb') as f:\n            for chunk in resp_dl.iter_content(chunk_size=1024): \n                if chunk:\n                    f.write(chunk)\n                    f.flush()\n        print(\"The dataset has been downloaded to: {}\".format(filename))\nelse: print(\"Request Unsuccessful! Error-Code: {}\".format(response.status_code))\n\n","type":"content","url":"/hda-rest-full-version#download-a-specific-asset-of-an-item","position":43},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Visualize the quicklook asset","lvl2":"Download"},"type":"lvl3","url":"/hda-rest-full-version#visualize-the-quicklook-asset","position":44},{"hierarchy":{"lvl1":"HDA Tutorial","lvl3":"Visualize the quicklook asset","lvl2":"Download"},"content":"\n\nImage(url=result['assets']['thumbnail']['href'], width=500)","type":"content","url":"/hda-rest-full-version#visualize-the-quicklook-asset","position":45},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start"},"type":"lvl1","url":"/hda-rest-quick-start","position":0},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start"},"content":"🚀 Launch in JupyterHub\n\n","type":"content","url":"/hda-rest-quick-start","position":1},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start","lvl3":"First steps using Harmonised Data access API"},"type":"lvl3","url":"/hda-rest-quick-start#first-steps-using-harmonised-data-access-api","position":2},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start","lvl3":"First steps using Harmonised Data access API"},"content":"- Discover Data of DestinE Data Portfolio - Access Data of DestinE Data Portfolio\n\nThis notebook demonstrates how to use the HDA (Harmonized Data Access) API by sending a few HTTP requests to the API, using Python code.\n\nThroughout this quickstart notebook, you will learn:\n\nDiscover: How to discover DEDL collections and services through HDA.\n\nAuthenticate: How to authenticate fro searching and access DEDL collections.\n\nSearch data:  How to search DEDL data through HDA.\n\nDownload data: How to download DEDL data through HDA.\n\nThe detailed API and definition of each endpoint and parameters is available in the HDA Swagger UI at:\n\nhttps://​hda​.data​.destination​-earth​.eu​/docs/\n\nPrerequisites: - For Data discovery: none - For Data access : \n\nDestinE user account\n\n","type":"content","url":"/hda-rest-quick-start#first-steps-using-harmonised-data-access-api","position":3},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start","lvl2":"Discover"},"type":"lvl2","url":"/hda-rest-quick-start#discover","position":4},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start","lvl2":"Discover"},"content":"\n\n","type":"content","url":"/hda-rest-quick-start#discover","position":5},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start","lvl3":"Import the relevant modules","lvl2":"Discover"},"type":"lvl3","url":"/hda-rest-quick-start#import-the-relevant-modules","position":6},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start","lvl3":"Import the relevant modules","lvl2":"Discover"},"content":"\n\nimport requests\nimport json\nfrom getpass import getpass\nfrom tqdm import tqdm\nimport time\nimport re\n\nimport destinelab as deauth\n\nfrom IPython.display import JSON\n\n","type":"content","url":"/hda-rest-quick-start#import-the-relevant-modules","position":7},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start","lvl3":"Define some constants for the API URL","lvl2":"Discover"},"type":"lvl3","url":"/hda-rest-quick-start#define-some-constants-for-the-api-url","position":8},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start","lvl3":"Define some constants for the API URL","lvl2":"Discover"},"content":"\n\n# Define the collection to be used\nCOLLECTION_ID = \"EO.EUM.DAT.SENTINEL-3.OL_2_WRR___\"\n\n# Core API\nHDA_API_URL = \"https://hda.data.destination-earth.eu\"\n\n# STAC API\n## Core\nSTAC_API_URL = f\"{HDA_API_URL}/stac\"\n\n## Collections\nCOLLECTIONS_URL = f\"{STAC_API_URL}/collections\"\nCOLLECTION_BY_ID_URL = f\"{COLLECTIONS_URL}/{COLLECTION_ID}\"\n\n## Items\nCOLLECTION_ITEMS_URL = f\"{COLLECTIONS_URL}/{COLLECTION_ID}/items\"\n\n## Item Search\nSEARCH_URL = f\"{STAC_API_URL}/search\"\n\n## HTTP Success\nHTTP_SUCCESS_CODE = 200\n\n","type":"content","url":"/hda-rest-quick-start#define-some-constants-for-the-api-url","position":9},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start","lvl3":"Discover data","lvl2":"Discover"},"type":"lvl3","url":"/hda-rest-quick-start#discover-data","position":10},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start","lvl3":"Discover data","lvl2":"Discover"},"content":"Below an example for discovering collections concerning Chlorophyll-a Concentration and algal pigment.\n\nresponse = requests.get(COLLECTIONS_URL,params = {\"q\": \"Chlorophyll-a Concentration,algal pigment\"})\n\nJSON(response.json(), expanded=False)\n\n","type":"content","url":"/hda-rest-quick-start#discover-data","position":11},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start","lvl2":"Authenticate"},"type":"lvl2","url":"/hda-rest-quick-start#authenticate","position":12},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start","lvl2":"Authenticate"},"content":"\n\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\nauth = deauth.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = auth.get_token()\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\nauth_headers = {\"Authorization\": f\"Bearer {access_token}\"}\n\n","type":"content","url":"/hda-rest-quick-start#authenticate","position":13},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start","lvl2":"Search"},"type":"lvl2","url":"/hda-rest-quick-start#search","position":14},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start","lvl2":"Search"},"content":"\n\nOnce selected a collection it is possible to search for items that match the specified input filters and order the results.\n\nThe collection used for this tutorial is \n\nOLCI Level 2 Ocean Colour Reduced Resolution - Sentinel-3\n\nBODY = {\n    \"collections\": [\n        COLLECTION_ID,\n    ],\n    \"datetime\" : \"2024-09-08T00:00:00Z/2024-09-09T23:59:59Z\",\n    \"bbox\": [-11,35,\n              50,72 ],\n    \"sortby\": [{\"field\": \"datetime\",\"direction\": \"desc\"}\n              ],\n    \"limit\": 3,\n}\n\nr=requests.post(SEARCH_URL, json=BODY, headers=auth_headers)\nif(r.status_code!= 200):\n    (print(r.text))\nr.raise_for_status()\nJSON(r.json(), expanded=False)   \n\n","type":"content","url":"/hda-rest-quick-start#search","position":15},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start","lvl2":"Download"},"type":"lvl2","url":"/hda-rest-quick-start#download","position":16},{"hierarchy":{"lvl1":"HDA Tutorial - Quick start","lvl2":"Download"},"content":"Once obtained the search results we can download the returned data.\n\n#select the first item in the result to download\nproduct = r.json()[\"features\"][0]\n\n# DownloadLink is an asset representing the whole product\ndownload_url = product[\"assets\"][\"downloadLink\"][\"href\"]\nITEM_ID = product[\"id\"]\n\nresponse = requests.get(download_url,stream=True,headers=auth_headers)\n\n# If the request was successful, download the file\nif (response.status_code == HTTP_SUCCESS_CODE):\n        print(\"Downloading ...\")\n        filename = ITEM_ID + \".zip\"\n        with open(filename, 'wb') as f:\n            for chunk in r.iter_content(chunk_size=1024): \n                if chunk:\n                    f.write(chunk)\n                    f.flush()\n        print(\"The dataset has been downloaded to: {}\".format(filename))\nelse: print(\"Request Unsuccessful! Error-Code: {}\".format(response.status_code))","type":"content","url":"/hda-rest-quick-start#download","position":17},{"hierarchy":{"lvl1":"Access to Hook services"},"type":"lvl1","url":"/dedl-hook-access","position":0},{"hierarchy":{"lvl1":"Access to Hook services"},"content":"🚀 Launch in JupyterHub\n\n%pip install pycurl\n\nimport json\nfrom io import BytesIO\nfrom urllib.parse import urlencode\nimport getpass\nimport pycurl\nimport requests\nfrom getpass import getpass\nfrom IPython.display import JSON\n\n","type":"content","url":"/dedl-hook-access","position":1},{"hierarchy":{"lvl1":"Access to Hook services","lvl2":"Autehentication - function"},"type":"lvl2","url":"/dedl-hook-access#autehentication-function","position":2},{"hierarchy":{"lvl1":"Access to Hook services","lvl2":"Autehentication - function"},"content":"\n\nimport requests\nfrom lxml import html\nfrom urllib.parse import parse_qs, urlparse\n\nIAM_URL = \"https://auth.destine.eu/\"\nCLIENT_ID = \"dedl-hook\"\nREALM = \"desp\"\nSERVICE_URL = \"https://odp.data.destination-earth.eu/odata/v1/\"\n\n\nclass DESPAuth:\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n\n    def get_token(self):\n        with requests.Session() as s:\n\n            # Get the auth url\n            auth_url = (\n                html.fromstring(\n                    s.get(\n                        url=IAM_URL\n                        + \"/realms/\"\n                        + REALM\n                        + \"/protocol/openid-connect/auth\",\n                        params={\n                            \"client_id\": CLIENT_ID,\n                            \"redirect_uri\": SERVICE_URL,\n                            \"scope\": \"openid\",\n                            \"response_type\": \"code\",\n                        },\n                    ).content.decode()\n                )\n                .forms[0]\n                .action\n            )\n\n            # Login and get auth code\n            login = s.post(\n                auth_url,\n                data={\n                    \"username\": self.username,\n                    \"password\": self.password,\n                },\n                allow_redirects=False,\n            )\n\n            # We expect a 302, a 200 means we got sent back to the login page and there's probably an error message\n            if login.status_code == 200:\n                tree = html.fromstring(login.content)\n                error_message_element = tree.xpath('//span[@id=\"input-error\"]/text()')\n                error_message = (\n                    error_message_element[0].strip()\n                    if error_message_element\n                    else \"Error message not found\"\n                )\n                raise Exception(error_message)\n\n            if login.status_code != 302:\n                raise Exception(\"Login failed\")\n\n            auth_code = parse_qs(urlparse(login.headers[\"Location\"]).query)[\"code\"][0]\n\n            # Use the auth code to get the token\n            response = requests.post(\n                IAM_URL + \"/realms/\" + REALM + \"/protocol/openid-connect/token\",\n                data={\n                    \"client_id\": CLIENT_ID,\n                    \"redirect_uri\": SERVICE_URL,\n                    \"code\": auth_code,\n                    \"grant_type\": \"authorization_code\",\n                    \"scope\": \"\",\n                },\n            )\n\n            if response.status_code != 200:\n                raise Exception(\"Failed to get token\")\n\n            token = response.json()[\"access_token\"]\n\n            return token\n\n\nclass DEDLAuth:\n    def __init__(self, desp_access_token):\n        self.desp_access_token = desp_access_token\n\n    def get_token(self):\n        DEDL_TOKEN_URL = \"https://identity.data.destination-earth.eu/auth/realms/dedl/protocol/openid-connect/token\"\n        DEDL_CLIENT_ID = \"hda-public\"\n        AUDIENCE = \"hda-public\"\n\n        data = {\n            \"grant_type\": \"urn:ietf:params:oauth:grant-type:token-exchange\",\n            \"subject_token\": self.desp_access_token,\n            \"subject_issuer\": \"desp-oidc\",\n            \"subject_token_type\": \"urn:ietf:params:oauth:token-type:access_token\",\n            \"client_id\": DEDL_CLIENT_ID,\n            \"audience\": AUDIENCE,\n        }\n\n        response = requests.post(DEDL_TOKEN_URL, data=data)\n\n        print(\"Response code:\", response.status_code)\n\n        if response.status_code == 200:\n            dedl_token = response.json()[\"access_token\"]\n            return dedl_token\n        else:\n            print(response.json())\n            print(\"Error obtaining DEDL access token\")\n\n\nclass AuthHandler:\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n        self.desp_access_token = None\n        self.dedl_access_token = None\n\n    def get_token(self):\n        # Get DESP auth token\n        desp_auth = DESPAuth(self.username, self.password)\n        self.desp_access_token = desp_auth.get_token()\n\n        # Get DEDL auth token\n        dedl_auth = DEDLAuth(self.desp_access_token)\n        self.dedl_access_token = dedl_auth.get_token()\n\n        return self.dedl_access_token\n\n","type":"content","url":"/dedl-hook-access#autehentication-function","position":3},{"hierarchy":{"lvl1":"Access to Hook services","lvl2":"Authetication"},"type":"lvl2","url":"/dedl-hook-access#authetication","position":4},{"hierarchy":{"lvl1":"Access to Hook services","lvl2":"Authetication"},"content":"\n\n# Enter DESP credentials.\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\n\ntoken = AuthHandler(DESP_USERNAME, DESP_PASSWORD)\naccess_token = token.get_token()\n\n# Check the status of the request\nif access_token is not None:\n\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\n\nelse:\n\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\n","type":"content","url":"/dedl-hook-access#authetication","position":5},{"hierarchy":{"lvl1":"Access to Hook services","lvl2":"Get a list of avilable Hooks"},"type":"lvl2","url":"/dedl-hook-access#get-a-list-of-avilable-hooks","position":6},{"hierarchy":{"lvl1":"Access to Hook services","lvl2":"Get a list of avilable Hooks"},"content":"\n\napi_headers = {\"Authorization\": \"Bearer \" + access_token}\nservice_root_url = \"https://odp.data.destination-earth.eu/odata/v1/\"\nresult = requests.get(service_root_url + \"Workflows\", headers=api_headers).json()\n\n# Assuming 'result' is a JSON array\nfor item in result[\"value\"]:\n    for key, value in item.items():\n        print(f\"{key}: {value}\")\n    print()  # Print an empty line to separate each item\n\nLicense: MIT \nCopyright: © 2024 EUMETSAT ","type":"content","url":"/dedl-hook-access#get-a-list-of-avilable-hooks","position":7},{"hierarchy":{"lvl1":"Hook - Perform data harvesting"},"type":"lvl1","url":"/dedl-hook-data-harvest","position":0},{"hierarchy":{"lvl1":"Hook - Perform data harvesting"},"content":"🚀 Launch in JupyterHub\n\nThe first step is to import the dependencies that allow the script to run\n\nimport json\nfrom io import BytesIO\nfrom urllib.parse import urlencode\nimport getpass\nimport pycurl\nimport requests\nfrom IPython.display import JSON\n\nThe following implement methods retrieve the token required to run the workflow\n\nimport requests\nfrom lxml import html\nfrom urllib.parse import parse_qs, urlparse\n\nIAM_URL = \"https://auth.destine.eu/\"\nCLIENT_ID = \"dedl-hook\"\nREALM = \"desp\"\nSERVICE_URL = \"https://odp.data.destination-earth.eu/odata/v1/\"\nTEST_RUN_ID = \"004\"\n\n\nclass DESPAuth:\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n\n    def get_token(self):\n        with requests.Session() as s:\n\n            # Get the auth url\n            auth_url = html.fromstring(s.get(url=IAM_URL + \"/realms/\" + REALM + \"/protocol/openid-connect/auth\",\n                                     params = {\n                                            \"client_id\": CLIENT_ID,\n                                            \"redirect_uri\": SERVICE_URL,\n                                            \"scope\": \"openid\",\n                                            \"response_type\": \"code\"\n                                     }\n                                       ).content.decode()).forms[0].action\n            \n            # Login and get auth code\n            login = s.post(auth_url,\n                            data = {\n                                \"username\" : self.username,\n                                \"password\" : self.password,\n                            },\n                            allow_redirects=False\n            )\n\n\n            # We expect a 302, a 200 means we got sent back to the login page and there's probably an error message\n            if login.status_code == 200:\n                tree = html.fromstring(login.content)\n                error_message_element = tree.xpath('//span[@id=\"input-error\"]/text()')\n                error_message = error_message_element[0].strip() if error_message_element else 'Error message not found'\n                raise Exception(error_message)\n\n            if login.status_code != 302:\n                raise Exception(\"Login failed\")\n            \n\n            auth_code = parse_qs(urlparse(login.headers[\"Location\"]).query)['code'][0]\n\n            # Use the auth code to get the token\n            response = requests.post(IAM_URL + \"/realms/\" + REALM + \"/protocol/openid-connect/token\",\n                    data = {\n                        \"client_id\" : CLIENT_ID,\n                        \"redirect_uri\" : SERVICE_URL,\n                        \"code\" : auth_code,\n                        \"grant_type\" : \"authorization_code\",\n                        \"scope\" : \"\"\n                    }\n                )\n            \n            if response.status_code != 200:\n                raise Exception(\"Failed to get token\")\n\n            token = response.json()['access_token']\n        \n\n            return token\n\nclass DEDLAuth:\n    def __init__(self, desp_access_token):\n        self.desp_access_token = desp_access_token\n\n    def get_token(self):\n        DEDL_TOKEN_URL='https://identity.data.destination-earth.eu/auth/realms/dedl/protocol/openid-connect/token'\n        DEDL_CLIENT_ID='hda-public'\n        AUDIENCE='hda-public'\n        \n        data = { \n            \"grant_type\": \"urn:ietf:params:oauth:grant-type:token-exchange\", \n            \"subject_token\": self.desp_access_token,\n            \"subject_issuer\": \"desp-oidc\",\n            \"subject_token_type\": \"urn:ietf:params:oauth:token-type:access_token\",\n            \"client_id\": DEDL_CLIENT_ID,\n            \"audience\": AUDIENCE\n        }\n\n        response = requests.post(DEDL_TOKEN_URL, data=data)\n        \n        print(\"Response code:\", response.status_code)\n\n        if response.status_code == 200: \n            dedl_token = response.json()[\"access_token\"]\n            return dedl_token\n        else: \n            print(response.json())\n            print(\"Error obtaining DEDL access token\")\n            \nclass AuthHandler:\n    def __init__(self, username, password):\n        self.username = username\n        self.password = password\n        self.desp_access_token = None\n        self.dedl_access_token = None\n    \n    def get_token(self):\n        # Get DESP auth token\n        desp_auth = DESPAuth(self.username, self.password)\n        self.desp_access_token = desp_auth.get_token()\n        \n        # Get DEDL auth token\n        dedl_auth = DEDLAuth(self.desp_access_token)\n        self.dedl_access_token = dedl_auth.get_token()\n        \n        return self.dedl_access_token\n\nUsers must provide their credentials to the DESP platform to retrieve an access token\n\nprint('Provide credentials for the DESP')\ndesp_username = input('DESP Username: ')\ndesp_password = getpass.getpass(prompt='DESP Password: ', stream=None) \n\ntoken = AuthHandler(desp_username, desp_password)          \naccess_token = token.get_token()\napi_headers = {'Authorization': 'Bearer ' + access_token}\n\nWe can review an accepted parameters for workflows\n\nworkflow_options_url = SERVICE_URL + \"Workflows?$filter=Name+eq+\\'data-harvest\\'&$expand=WorkflowOptions\"\nresult = requests.get(workflow_options_url, headers=api_headers).json()\navailable_workflows = json.dumps(result,indent=2)\nJSON(result)\n\nPrepare Worflow Options for data-harvest workflow\n\nworkflow = \"data-harvest\"\nidentifier_list = [\"S2A_MSIL2A_20240501T095031_N0510_R079_T33UXT_20240501T135852.SAFE\"]\norder_body_custom_bucket = {\n        \"Name\": \"DEDL - Hook introduction support \" + workflow + \" - \" + TEST_RUN_ID,\n        \"WorkflowName\": workflow,\n        \"IdentifierList\": identifier_list,\n        \"WorkflowOptions\":[\n            {\"Name\": \"output_storage\", \"Value\": \"TEMPORARY\"},\n            {\"Name\": \"source_type\", \"Value\": \"DESP\"},\n            {\"Name\": \"desp_source_collection\", \"Value\": \"EO.ESA.DAT.SENTINEL-2.MSI.L2A\"},\n            {\"Name\": \"desp_source_username\", \"Value\": desp_username},\n            {\"Name\": \"desp_source_password\", \"Value\": desp_password},\n            \n        ]\n    }\n\nMake a request to run the workflow with the parameters just set\n\nrequest = requests.post(\n    SERVICE_URL + \"BatchOrder/OData.CSC.Order\",\n    json.dumps(order_body_custom_bucket),\n    headers=api_headers\n)\nresp = request.json()\nprint(request.status_code)\norder_id = resp['value']['Id']\nJSON(resp, indent=2)\n\nReview information about the products processed in the order\n\nbatch_order_items = requests.get(SERVICE_URL + 'BatchOrder(' + str(order_id) + ')/Products', headers=api_headers).json()\nJSON(batch_order_items, indent=2)\n\nReview processed product status\n\nbatch_order_items['value'][0]['Status']\n\nReview processed product DownloadLink\n\nbatch_order_items['value'][0]['DownloadLink']","type":"content","url":"/dedl-hook-data-harvest","position":1},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest"},"type":"lvl1","url":"/tutorial","position":0},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest"},"content":"🚀 Launch in JupyterHub\n\nThe detailed API and definition of each endpoint and parameters is available in the OnDemand Processing API OData v1  OpenAPI documentation found at:\n\n\nhttps://​odp​.data​.destination​-earth​.eu​/odata​/docs\n\nFurther documentation is available at:\n\n\nhttps://​destine​-data​-lake​-docs​.data​.destination​-earth​.eu​/en​/latest​/dedl​-big​-data​-processing​-services​/Hook​-service​/Hook​-service​.html\n\n","type":"content","url":"/tutorial","position":1},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl2":"Install python package requirements and import environment variables"},"type":"lvl2","url":"/tutorial#install-python-package-requirements-and-import-environment-variables","position":2},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl2":"Install python package requirements and import environment variables"},"content":"\n\n# Note: The destinelab python package (which helps with authentication) is available already if you are using Python DEDL kernel\n# Otherwise, the destinelab python package can be installed by uncommenting the following line\n\n# For the importing of environment variables using the load_dotenv(...) command \n%pip install python-dotenv\n# for example code navigating private S3 compatible storage (PRIVATE bucket storage)\n%pip install boto3\n\n\nimport os\nimport json\nimport requests\nfrom dotenv import load_dotenv\nfrom getpass import getpass\nimport destinelab as destinelab\n\n# Load (optional) notebook specific environment variables from .env_tutorial\nload_dotenv(\"./.env_tutorial\", override=True)\n\n","type":"content","url":"/tutorial#install-python-package-requirements-and-import-environment-variables","position":3},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl2":"Authentification - Get token"},"type":"lvl2","url":"/tutorial#authentification-get-token","position":4},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl2":"Authentification - Get token"},"content":"\n\n# By default users should use their DESP credentials to get an Access_token\n# This token is added as an Authorisation Header when interacting with the Hook Service API\n\n# Enter DESP credentials.\nDESP_USERNAME = input(\"Please input your DESP username or email: \")\nDESP_PASSWORD = getpass(\"Please input your DESP password: \")\ntoken = destinelab.AuthHandler(DESP_USERNAME, DESP_PASSWORD)\n\naccess_token = token.get_token()\n\n# Check the status of the request\nif access_token is not None:\n    print(\"DEDL/DESP Access Token Obtained Successfully\")\n    # Save API headers\n    api_headers = {\"Authorization\": \"Bearer \" + access_token}\nelse:\n    print(\"Failed to Obtain DEDL/DESP Access Token\")\n\n","type":"content","url":"/tutorial#authentification-get-token","position":5},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl2":"Setup static variables"},"type":"lvl2","url":"/tutorial#setup-static-variables","position":6},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl2":"Setup static variables"},"content":"\n\n# Hook service url (ending with odata/v1/ - e.g. https://odp.data.destination-earth.eu/odata/v1/)\nhook_service_root_url = \"https://odp.data.destination-earth.eu/odata/v1/\"\n\n","type":"content","url":"/tutorial#setup-static-variables","position":7},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl2":"List available workflows"},"type":"lvl2","url":"/tutorial#list-available-workflows","position":8},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl2":"List available workflows"},"content":"Next we can check what possible workflows are available to us by using methodhttps://odp.data.destination-earth.eu/odata/v1/Workflows\n\n# Send request and return json object listing all provided workfows, ordered by Id\nresult = requests.get(\n    f\"{hook_service_root_url}Workflows?$orderby=Id asc\", headers=api_headers   \n).json()\n\nprint(\"List of available DEDL provided Hooks\")\nfor i in range(len(result[\"value\"])):\n    print(\n        f\"Name:{str(result['value'][i]['Name']).ljust(20, ' ')}DisplayName:{str(result['value'][i]['DisplayName'])}\"\n    )  # print JSON string\n\n# Print result JSON object: containing provided workflow list\nworkflow_details = json.dumps(result, indent=4)\nprint(workflow_details)\n\n","type":"content","url":"/tutorial#list-available-workflows","position":9},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl2":"Select a workflow and see parameters"},"type":"lvl2","url":"/tutorial#select-a-workflow-and-see-parameters","position":10},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl2":"Select a workflow and see parameters"},"content":"\n\nIf we want to see the details of a specific workflow, showing us the parameters that can be set for that workflow, we can add a filter to the query as follows:\n\nhttps://odp.data.destination-earth.eu/odata/v1/Workflows?$expand=WorkflowOptions&$filter=(Name eq data-harvest)\n\n\\expand=WorkflowOptions** shows all parameters accepted by workflow   \n**\\\\filter=(Name eq data-harvest) narrows the result to workflow called “data-harvest”\n\n# Select workflow : defaults to data-harvest\nworkflow = os.getenv(\"HOOK_WORKFLOW\", \"data-harvest\")\nprint(f\"workflow: {workflow}\")\n\n# Send request\nresult = requests.get(\n    f\"{hook_service_root_url}Workflows?$expand=WorkflowOptions&$filter=(Name eq '{workflow}')\",\n    headers=api_headers,\n).json()\nworkflow_details = json.dumps(result, indent=4)\nprint(workflow_details)  # print formatted workflow_details, a JSON string\n\n","type":"content","url":"/tutorial#select-a-workflow-and-see-parameters","position":11},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl2":"Order selected workflow"},"type":"lvl2","url":"/tutorial#order-selected-workflow","position":12},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl2":"Order selected workflow"},"content":"\n\nThe order selected above will now be configured and executed.\n\ne.g. workflow = “data-harvest”.\n\nMake an order to ‘harvest data’ using Harmonised Data Access API.\n\ni.e. data from an input source can be transferred to a Private bucket or a Temporary storage bucket.\n\n","type":"content","url":"/tutorial#order-selected-workflow","position":13},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl3":"Name your order","lvl2":"Order selected workflow"},"type":"lvl3","url":"/tutorial#name-your-order","position":14},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl3":"Name your order","lvl2":"Order selected workflow"},"content":"\n\n# Here we set the variable order_name, this will allow us to:\n# Easily identify the running process (e.g. when checking the status)\n# order_name is added as a suffix to the order 'Name'\norder_name = os.getenv(\"HOOK_ORDER_NAME\") or input(\"Name your order: \")\nprint(f\"order_name:{order_name}\")\n\n","type":"content","url":"/tutorial#name-your-order","position":15},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl3":"Define output storage","lvl2":"Order selected workflow"},"type":"lvl3","url":"/tutorial#define-output-storage","position":16},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl3":"Define output storage","lvl2":"Order selected workflow"},"content":"\n\nIn workflow parameters, among others values, storage to retreive the result has to be provided.Two possibilites:\n\nUse your user storage\n\nUse a temporary storage\n\n","type":"content","url":"/tutorial#define-output-storage","position":17},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl4":"1. - Your user storage (provided by DEDL ISLET service)","lvl3":"Define output storage","lvl2":"Order selected workflow"},"type":"lvl4","url":"/tutorial#id-1-your-user-storage-provided-by-dedl-islet-service","position":18},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl4":"1. - Your user storage (provided by DEDL ISLET service)","lvl3":"Define output storage","lvl2":"Order selected workflow"},"content":"\n\nExample using a S3 bucket created with ISLET Storage service  - result will be available in this bucket\n\nworkflow parameter: {“Name”: “output_storage”, “Value”: “PRIVATE”}\n\n# Output storage - Islet service\n\n# Note: If you want the output to go to your own PRIVATE bucket rather than TEMPORARY storage (expires after 2 weeks),\n#       i) This Configuration will need to be updated with your output_bucket, output_storage_access_key, output_secret_key, output_prefix\n#       ii) You will need to change the output_storage in the order to PRIVATE and add the necessary source_ parameters (see workflow options and commented example)\n\n# URL of the S3 endpoint in the Central Site (or lumi etc.)\noutput_storage_url = \"https://s3.central.data.destination-earth.eu\"\n# output_storage_url = \"https://s3.lumi.data.destination-earth.eu\"\n\n# Name of the object storage bucket where the results will be stored.\noutput_bucket = os.getenv(\"HOOK_OUTPUT_BUCKET\", \"your-bucket-name\")\nprint(f\"output_bucket            : {output_bucket}\")\n\n# Islet object storage credentials (openstack ec2 credentials)\noutput_storage_access_key = os.getenv(\"HOOK_OUTPUT_STORAGE_ACCESS_KEY\", \"your-access-key\")\noutput_storage_secret_key = os.getenv(\"HOOK_OUTPUT_STORAGE_SECRET_KEY\", \"your-secret-key\")\nprint(f\"output_storage_access_key: {output_storage_access_key}\")\nprint(f\"output_storage_secret_key: {output_storage_secret_key}\")\n\n\n# This is the name of the folder in your output_bucket where the output of the hook will be stored.\n# Here we concatenate 'dedl' with the 'workflow' and 'order_name'\noutput_prefix = f\"dedl-{workflow}-{order_name}\"\nprint(f\"output_prefix            : {output_prefix}\")\n\n","type":"content","url":"/tutorial#id-1-your-user-storage-provided-by-dedl-islet-service","position":19},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl4":"2 - Use temporary storage","lvl3":"Define output storage","lvl2":"Order selected workflow"},"type":"lvl4","url":"/tutorial#id-2-use-temporary-storage","position":20},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl4":"2 - Use temporary storage","lvl3":"Define output storage","lvl2":"Order selected workflow"},"content":"\n\nThe result of processing will be stored in shared storage and download link provided in the output product details\n\nworkflow parameter: {“Name”: “output_storage”, “Value”: “TEMPORARY”}\n\n","type":"content","url":"/tutorial#id-2-use-temporary-storage","position":21},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl3":"Define parameters and send order","lvl2":"Order selected workflow"},"type":"lvl3","url":"/tutorial#define-parameters-and-send-order","position":22},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl3":"Define parameters and send order","lvl2":"Order selected workflow"},"content":"\n\n# URL of the STAC server where your collection/item can be downloaded\nstac_hda_api_url = \"https://hda.data.destination-earth.eu/stac\"\n\n# Note: The data (collection_id and data_id) will have been previously discovered and searched for\n\n# Set collection where the item can be found : defaults to example for data-harvest\ncollection_id = os.getenv(\"HOOK_COLLECTION_ID\", \"EO.ESA.DAT.SENTINEL-2.MSI.L1C\")\nprint(f\"STAC collection url: {stac_hda_api_url}/collections/{collection_id}\")\n\n# Set the Item to Retrieve : defaults to example for data-harvest. If Multiple Values, provide comma separated list\ndata_id = os.getenv(\"HOOK_DATA_ID\", \"S2A_MSIL1C_20230910T050701_N0509_R019_T47VLH_20230910T074321.SAFE\")\nprint(f\"data_id: {data_id}\")\nidentifier_list = [data_id_element.strip() for data_id_element in data_id.split(',')]\n\n# Get boolean value from String, default (False)\nis_private_storage = os.getenv(\"HOOK_IS_PRIVATE_STORAGE\", \"False\") == \"True\"\nprint(f\"is_private_storage: {is_private_storage}\")\n\n# we use source_type to add DESP or EXTERNAL specific configuration\nsource_type = os.getenv(\"HOOK_SOURCE_TYPE\", \"DESP\")\nprint(f\"source_type: {source_type}\")\n\nif source_type == \"EXTERNAL\":\n    EXTERNAL_USERNAME = os.getenv(\"HOOK_EXTERNAL_USERNAME\", \"EXTERNAL_USERNAME\")\n    EXTERNAL_PASSWORD = os.getenv(\"HOOK_EXTERNAL_PASSWORD\", \"EXTERNAL_PASSWORD\")\n    EXTERNAL_TOKEN_URL = os.getenv(\"HOOK_EXTERNAL_TOKEN_URL\", \"EXTERNAL_TOKEN_URL\")\n    EXTERNAL_CLIENT_ID = os.getenv(\"HOOK_EXTERNAL_CLIENT_ID\", \"EXTERNAL_CLIENT_ID\")\n\n########## BUILD ORDER BODY : CHOOSE PRIVATE or TEMPORARY output_storage ##########\n\n# Initialise the order_body\norder_body_custom_bucket = {\n    \"Name\": \"Tutorial \" + workflow + \" - \" + order_name,\n    \"WorkflowName\": workflow,\n    \"IdentifierList\": identifier_list,\n    \"WorkflowOptions\": [],\n}\n\n\n##### Configure PRIVATE OR TEMPORARY STORAGE #####\nif is_private_storage:\n\n    print(\"##### Preparing Order Body for PRIVATE STORAGE #####\")\n    order_body_custom_bucket[\"WorkflowOptions\"].extend(\n        [\n            {\"Name\": \"output_storage\", \"Value\": \"PRIVATE\"},\n            {\"Name\": \"output_s3_access_key\", \"Value\": output_storage_access_key},\n            {\"Name\": \"output_s3_secret_key\", \"Value\": output_storage_secret_key},\n            {\"Name\": \"output_s3_path\", \"Value\": f\"s3://{output_bucket}/{output_prefix}\"},\n            {\"Name\": \"output_s3_endpoint_url\", \"Value\": output_storage_url}\n        ]\n    )\n\nelse:\n\n    print(\"##### Preparing Order Body for TEMPORARY STORAGE #####\")\n    order_body_custom_bucket[\"WorkflowOptions\"].extend(\n        [\n            {\"Name\": \"output_storage\", \"Value\": \"TEMPORARY\"},\n        ]\n    )\n\n##### Configure SOURCE_TYPE and associated parameters #####\nif source_type == \"DESP\":\n\n    # Using DESP credentials is standard way of executing Hooks.\n    print(\"##### Preparing Order Body for access to DEDL HDA using DESP Credentials #####\")\n    order_body_custom_bucket[\"WorkflowOptions\"].extend(\n        [\n            {\"Name\": \"source_type\", \"Value\": \"DESP\"},\n            {\"Name\": \"desp_source_username\", \"Value\": DESP_USERNAME},\n            {\"Name\": \"desp_source_password\", \"Value\": DESP_PASSWORD},\n            {\"Name\": \"desp_source_collection\", \"Value\": collection_id}\n        ]\n    )\n\nelif source_type == \"EXTERNAL\":\n\n    # Build your order body : Example using EXTERNAL source type and source_catalogue_api_type STAC.\n    # This would allow you to access products directly from a configured STAC server\n    # Here we show an example configuration of a STAC server with OIDC security, that could be adapted to your needs (change urls, etc)\n    # This is shown for example purposes only. The standard way of configuring is with DESP source_type seen above.\n    print(\"##### Preparing Order Body for access to EXTERNAL STAC Server using EXTERNAL Credentials #####\")\n    order_body_custom_bucket[\"WorkflowOptions\"].extend(\n        [\n            {\"Name\": \"source_type\", \"Value\": \"EXTERNAL\"},\n            {\"Name\": \"source_catalogue_api_url\", \"Value\": stac_hda_api_url},\n            {\"Name\": \"source_catalogue_api_type\", \"Value\": \"STAC\"},\n            {\"Name\": \"source_token_url\", \"Value\": EXTERNAL_TOKEN_URL},\n            {\"Name\": \"source_grant_type\", \"Value\": \"PASSWORD\"},\n            {\"Name\": \"source_auth_header_name\", \"Value\": \"Authorization\"},\n            {\"Name\": \"source_username\", \"Value\": EXTERNAL_USERNAME},\n            {\"Name\": \"source_password\", \"Value\": EXTERNAL_PASSWORD},\n            {\"Name\": \"source_client_id\", \"Value\": EXTERNAL_CLIENT_ID},\n            {\"Name\": \"source_client_secret\", \"Value\": \"\"},\n            {\"Name\": \"source_catalogue_collection\", \"Value\": collection_id}\n        ]\n    )\n\nelse:\n\n    print(\"source_type not equal to DESP or EXTERNAL\")\n\n\n\n########## ADDITIONAL OPTIONS ##########\n\nadditional_options = []\n\n# Checks environment variables for the form HOOK_ADDITIONAL1=\"NAME=12345;VALUE=abcdef\"\nfor env_key, env_value in os.environ.items():\n    if env_key.startswith('HOOK_ADDITIONAL'):\n        #print(f\"{env_key}: {env_value}\")        \n        parts = env_value.split(';')\n        # Extract the name and value\n        name = parts[0].split('=')[1]\n        value = parts[1].split('=')[1]\n        value_type = parts[2].split('=')[1]\n        additional_options.append({\"Name\": name, \"Value\": value if value_type == 'str' else int(value)})\n\nprint(f\"addditional_options:{additional_options}\")\n\nif additional_options:\n    print(\"Adding additional_options\")\n    order_body_custom_bucket[\"WorkflowOptions\"].extend(additional_options)\n\n########## BUILD ORDER BODY : END ##########\n\n# Uncomment this to see the final order body\n# print(json.dumps(order_body_custom_bucket, indent=4))\n\n\n# Send order\norder_request = requests.post(\n    hook_service_root_url + \"BatchOrder/OData.CSC.Order\",\n    json.dumps(order_body_custom_bucket),\n    headers=api_headers,\n).json()\n\n# If code = 201, the order has been successfully sent\n\n# Print order_request JSON object: containing order_request details\norder_reques_details = json.dumps(order_request, indent=4)\nprint(order_reques_details)\n\norder_id = order_request['value']['Id']\nprint(f\"order 'Id' from order_request: {order_id}\")\n\n\nIt is possible to order multiple product using endpoint:\nhttps://odp.data.destination-earth.eu/odata/v1/BatchOrder/OData.CSC.Order\n\n","type":"content","url":"/tutorial#define-parameters-and-send-order","position":23},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl2":"Check The status of the order"},"type":"lvl2","url":"/tutorial#check-the-status-of-the-order","position":24},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl2":"Check The status of the order"},"content":"Possible status values\n\nqueued (i.e. queued for treatment but not started)\n\nin_progress (i.e. order being treated)\n\ncompleted (i.e. order is complete and data ready)\n\n\n# ProductionOrders endpoint gives status of orders (only with one item attached)\n# Otherwise use BatchOrder(XXXX)/Products endpoint to get status of individual items associated with order\nif len(identifier_list) == 1:\n    order_status_url = f\"{hook_service_root_url}ProductionOrders\"\n    params = {\"$filter\": f\"id eq {order_id}\"}\n    order_status_response = requests.get(order_status_url, params=params, headers=api_headers).json()\n    print(json.dumps(order_status_response, indent=4))\n\n# Get Status of all items of an order in this way\norder_status_response = requests.get(\n    f\"{hook_service_root_url}BatchOrder({order_id})/Products\",\n    headers=api_headers,\n).json()\nprint(json.dumps(order_status_response, indent=4))\n\n\n","type":"content","url":"/tutorial#check-the-status-of-the-order","position":25},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl2":"Access workflow output"},"type":"lvl2","url":"/tutorial#access-workflow-output","position":26},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl2":"Access workflow output"},"content":"\n\n","type":"content","url":"/tutorial#access-workflow-output","position":27},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl4":"Private storage","lvl2":"Access workflow output"},"type":"lvl4","url":"/tutorial#private-storage","position":28},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl4":"Private storage","lvl2":"Access workflow output"},"content":"Let us now check our private storage using this boto3 script.\nYou can also go and check this in the Islet service using the Horizon user interface\n\n# PRIVATE STORAGE: Prints contents of Private Bucket\nimport boto3\n\nif is_private_storage:\n\n    s3 = boto3.client(\n        \"s3\",\n        aws_access_key_id=output_storage_access_key,\n        aws_secret_access_key=output_storage_secret_key,\n        endpoint_url=output_storage_url,\n    )\n\n    paginator = s3.get_paginator(\"list_objects_v2\")\n    pages = paginator.paginate(Bucket=output_bucket, Prefix=output_prefix + \"/\")\n\n    for page in pages:\n        try:\n            for obj in page[\"Contents\"]:\n                print(obj[\"Key\"])\n        except KeyError:\n            print(\"No files exist\")\n            exit(1)\n\n","type":"content","url":"/tutorial#private-storage","position":29},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl3":"Temporary storage","lvl2":"Access workflow output"},"type":"lvl3","url":"/tutorial#temporary-storage","position":30},{"hierarchy":{"lvl1":"Hook Tutorial - Data Harvest","lvl3":"Temporary storage","lvl2":"Access workflow output"},"content":"\n\n# List order items within a production order\n# When the output_storage is of type TEMPORARY we can get a DownloadLink from the following code (Can also optionally download items here in code with the flag is_download_products)\n\n# If TEMPORARY storage\nif not is_private_storage:\n\n    # Set to True to download products at the same level as the notebook file. File name will be in format \"output-{workflow}-{order_id}-{product_id}.zip\"\n    is_download_products = False\n\n    # Get Status of all items of an order in this way\n    product_status_response = requests.get(\n        f\"{hook_service_root_url}BatchOrder({order_id})/Products\",\n        headers=api_headers,\n    ).json()\n    print(json.dumps(product_status_response, indent=4))\n\n    if is_download_products:\n\n        is_all_products_completed = True\n        # We only attempt to download products when each of the items is in complete status.\n        for i in range(len(product_status_response[\"value\"])):\n\n            product_id = product_status_response[\"value\"][i][\"Id\"]\n            product_status = product_status_response[\"value\"][i][\"Status\"]\n\n            if product_status != \"completed\":\n                is_all_products_completed = False\n\n        # Can download if all products completed\n        if is_all_products_completed:\n\n            for i in range(len(product_status_response[\"value\"])):\n\n                product_id = product_status_response[\"value\"][i][\"Id\"]\n                product_status = product_status_response[\"value\"][i][\"Status\"]\n\n                # Infer the url of the product\n                url_product = f\"{hook_service_root_url}BatchOrder({order_id})/Product({product_id})/$value\"\n                print(f\"url_product: {url_product}\")\n                # Download the product\n                r = requests.get(\n                    url_product, headers=api_headers, allow_redirects=True\n                )\n                product_file_name = f\"output-{workflow}-{order_id}-{product_id}.zip\"\n                open(product_file_name, \"wb\").write(r.content)\n                print(f\"Download Complete: product_file_name: {product_file_name}\")\n\n        else:\n            print(f\"Status for order:{order_id} - At least one of the products does not have the status of 'completed'.\")\n","type":"content","url":"/tutorial#temporary-storage","position":31},{"hierarchy":{"lvl1":"STACK Service Dask"},"type":"lvl1","url":"/dedl-stackservice-dask","position":0},{"hierarchy":{"lvl1":"STACK Service Dask"},"content":"🚀 Launch in JupyterHub\n\n","type":"content","url":"/dedl-stackservice-dask","position":1},{"hierarchy":{"lvl1":"STACK Service Dask","lvl2":"Authentication via OIDC password grant flow"},"type":"lvl2","url":"/dedl-stackservice-dask#authentication-via-oidc-password-grant-flow","position":2},{"hierarchy":{"lvl1":"STACK Service Dask","lvl2":"Authentication via OIDC password grant flow"},"content":"The DEDL Stack client library holds the DaskOIDC class as a helper class to authenticate a user against the given identity provider of DestinE Data Lake.\nThe users password is directly handed over to the request object and is not stored.\nRefreshed token is used to request a new access token in case it is expired.\n\nThe DaskMultiCluster class provides an abstraction layer to spawn multiple Dask clusters, one per location, within the data lake. Each cluster will be composed of 2 workers per default, with adaptive scaling enabled towards a maximum of 10 workers. In addition, the workers are configured to have 2 cores and 2 GB RAM per default. This can be changed via the \n\ncluster options exposed up to the given service quota of the individual user role:\n\nWorker cores:\n\nmin: 1\n\nmax: ..::service-quota::..\n\nWorker memory:\n\nmin: 1 GB\n\nmax: ..::service-quota::.. GB\n\nDask Worker and Scheduler nodes are based on a custom build \n\ncontainer image with the aim to match the environment, Jupyter Kernel, of the DEDL JupyterLab instance. Warnings will be displayed if a version missmatch is detected. Feel free to use your custom image to run your workloads by replacing the container image in the cluster options object.\n\nfrom dedl_stack_client.authn import DaskOIDC\nfrom dedl_stack_client.dask import DaskMultiCluster\nfrom rich.prompt import Prompt\n\nmyAuth = DaskOIDC(username=Prompt.ask(prompt=\"Username\"))\nmyDEDLClusters = DaskMultiCluster(auth=myAuth)\nmyDEDLClusters.new_cluster()\n\nPrint the given client object details per location as well as the link to the Dask dashboard.\n\nwith myDEDLClusters.as_current(location=\"central\") as myclient:\n    print(myclient)\n    print(myclient.dashboard_link)\nwith myDEDLClusters.as_current(location=\"lumi\") as myclient:\n    print(myclient)\n    print(myclient.dashboard_link)\n\n","type":"content","url":"/dedl-stackservice-dask#authentication-via-oidc-password-grant-flow","position":3},{"hierarchy":{"lvl1":"STACK Service Dask","lvl3":"Shutdown the all clusters and free up all resources","lvl2":"Authentication via OIDC password grant flow"},"type":"lvl3","url":"/dedl-stackservice-dask#shutdown-the-all-clusters-and-free-up-all-resources","position":4},{"hierarchy":{"lvl1":"STACK Service Dask","lvl3":"Shutdown the all clusters and free up all resources","lvl2":"Authentication via OIDC password grant flow"},"content":"\n\nmyDEDLClusters.shutdown()","type":"content","url":"/dedl-stackservice-dask#shutdown-the-all-clusters-and-free-up-all-resources","position":5},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube"},"type":"lvl1","url":"/extremedt-datacube-xviewer","position":0},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube"},"content":"🚀 Launch in JupyterHub\n\n","type":"content","url":"/extremedt-datacube-xviewer","position":1},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl2":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT"},"type":"lvl2","url":"/extremedt-datacube-xviewer#create-a-dashboard-based-on-data-cube-populated-with-data-obtained-from-weather-and-geophysical-extremes-digital-twin-dt-extremedt","position":2},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl2":"Create a dashboard based on Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT"},"content":"DISCLAIMERIn order to deal with the code provided within this notebook, it is required to run it on user environment (local one or virtual machine).\n\nThis notebook covers:\n\nfind available data cubes and their urls\n\nfilter data area of interest\n\ncreate interactive dashboard using xcube - xviewer\n\nPresequites\n\nXcube\nInstall xcube and create a new environment mamba create --name xcube --channel conda-forge xcubeORInstall xcube in the current environment  mamba install --channel conda-forge xcube\n\nXarraypip pip install xarrayORconda  conda install -c conda-forge xarray dask netCDF4 bottleneck\n\n","type":"content","url":"/extremedt-datacube-xviewer#create-a-dashboard-based-on-data-cube-populated-with-data-obtained-from-weather-and-geophysical-extremes-digital-twin-dt-extremedt","position":3},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl2":"Prepre your environment"},"type":"lvl2","url":"/extremedt-datacube-xviewer#prepre-your-environment","position":4},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl2":"Prepre your environment"},"content":"\n\nfrom xcube.webapi.viewer import Viewer\nimport xarray as xr\nimport requests\n\n","type":"content","url":"/extremedt-datacube-xviewer#prepre-your-environment","position":5},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl2":"Connect with Extreme DT data cube"},"type":"lvl2","url":"/extremedt-datacube-xviewer#connect-with-extreme-dt-data-cube","position":6},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl2":"Connect with Extreme DT data cube"},"content":"The data cube provides data:\n\nFour variables\n\n2t - Air temperature at 2 meteres above grond [K]\n\n2d - Dew point temperature at 2 meteres above grond [K]\n\nsp - Surface pressure [Pa]\n\nForecast 2024.04.04-13 + 96 hours for each date\n\nHourly step\n\nWorld\n\n","type":"content","url":"/extremedt-datacube-xviewer#connect-with-extreme-dt-data-cube","position":7},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl2":"Select proper data cube"},"type":"lvl2","url":"/extremedt-datacube-xviewer#select-proper-data-cube","position":8},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl2":"Select proper data cube"},"content":"Data cubes on s3 bucket are stored under URL \n\nhttps://​s3​.central​.data​.destination​-earth​.eu​/swift​/v1​/dedl​_datacube.Data cubes are stored in two directories:\n\nExtremeDT - the newest one\n\narchive - from preicus days\n\nFile nameing convention:dt_extreme_YYYYMMDD.zarr/YYYYMMDD - is the date when forecast starts (step 0)\n\nResultsAfter exectution of code below, the list of urls linked to available cubes will be printed.\n\n# URL to s3 where  ExtremeDT data cubes are stored\ndatacube_url = 'https://s3.central.data.destination-earth.eu/swift/v1/dedl_datacube'\nresponse = requests.get(datacube_url)\n\nif response.status_code == 200:\n    lines = response.text.splitlines()\n    zarr_items = [line for line in lines if line.endswith('.zarr') or line.endswith('.zarr/')]\n    if zarr_items:\n        for item in zarr_items:\n            print(item)\n            new_url = f\"{datacube_url}/{item}\"\n            print(\"New URL:\", new_url)\n    else:\n        print(\"No .zarr files or directories found.\")\nelse:\n    print(\"Failed to fetch contents. Status code:\", response.status_code)\n\nurl = 'https://s3.waw3-1.cloudferro.com/swift/v1/s5p_l3/ExtremeDT/dt_extreme_20240412.zarr/'\n\n","type":"content","url":"/extremedt-datacube-xviewer#select-proper-data-cube","position":9},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl3":"Area of interest","lvl2":"Select proper data cube"},"type":"lvl3","url":"/extremedt-datacube-xviewer#area-of-interest","position":10},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl3":"Area of interest","lvl2":"Select proper data cube"},"content":"Upload data for selected area and verify what variables are provided. In this case uplaod data for Kenya. List of available variables should be returend.\n\nkenya_bbox = [33.501,   # West\n              -4.677,   # South\n              41.899,   # East\n              5.193]    # North\n\nkenya_dt = xr.open_zarr(url).sel(lon=slice(kenya_bbox[0], \n                                            kenya_bbox[2]), \n                                lat=slice(kenya_bbox[3], \n                                            kenya_bbox[1]),\n                                                         )\n\nlist(kenya_dt.keys())\n\n","type":"content","url":"/extremedt-datacube-xviewer#area-of-interest","position":11},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl2":"Prepare Data for visualization"},"type":"lvl2","url":"/extremedt-datacube-xviewer#prepare-data-for-visualization","position":12},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl2":"Prepare Data for visualization"},"content":"\n\n","type":"content","url":"/extremedt-datacube-xviewer#prepare-data-for-visualization","position":13},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl3":"Convert units","lvl2":"Prepare Data for visualization"},"type":"lvl3","url":"/extremedt-datacube-xviewer#convert-units","position":14},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl3":"Convert units","lvl2":"Prepare Data for visualization"},"content":"\n\nkenya_dt['2t'] -= 273.15    # Conversion to Celcius degrees\nkenya_dt['2d'] -= 273.15    # Conversion to Celcius degrees\nkenya_dt['sp'] /= 100       # Conversion to hectoPascals\n\nkenya_dt['2t'].attrs['units'] = '°C'\nkenya_dt['2d'].attrs['units'] = '°C'\nkenya_dt['sp'].attrs['units'] = 'hPa'\n\n","type":"content","url":"/extremedt-datacube-xviewer#convert-units","position":15},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl2":"Define style of visualization"},"type":"lvl2","url":"/extremedt-datacube-xviewer#define-style-of-visualization","position":16},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl2":"Define style of visualization"},"content":"\n\nError message starting with:404 GET /viewer/config/config.json (127.0.0.1): xcube viewer has not been been configured\n404 GET /viewer/config/config.json (127.0.0.1) 2.14mscould occur.It is normal and does not affect on proper functioning of xviewer.\n\nviewer = Viewer(server_config={\n    \"Styles\": [\n        {\n            \"Identifier\": \"dt_legend\",          # Style's name\n            \"ColorMappings\": {\n                \"2t\": {                         # Variable's name\n                    \"ValueRange\": [10, 30],     # Variable's values range\n                    \"ColorBar\": \"coolwarm\"      # colorbar\n                },\n                \"2d\": {                         # Variable's name\n                    \"ValueRange\": [0, 20],\n                    \"ColorBar\": \"coolwarm\",\n                },\n                'sp': {                         # Variable's name\n                    \"ValueRange\": [0, 1000],    # Variable's values range\n                    \"ColorBar\": \"viridis\"       # colorbar\n                },\n            }\n        },\n    ]\n})\n\n","type":"content","url":"/extremedt-datacube-xviewer#define-style-of-visualization","position":17},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl3":"Add style to the data cube","lvl2":"Define style of visualization"},"type":"lvl3","url":"/extremedt-datacube-xviewer#add-style-to-the-data-cube","position":18},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl3":"Add style to the data cube","lvl2":"Define style of visualization"},"content":"\n\nviewer.add_dataset(kenya_dt, \n                   style=\"dt_legend\")\n\n","type":"content","url":"/extremedt-datacube-xviewer#add-style-to-the-data-cube","position":19},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl2":"Run the dashboard"},"type":"lvl2","url":"/extremedt-datacube-xviewer#run-the-dashboard","position":20},{"hierarchy":{"lvl1":"Interactive Dashboard for ExtremeDT Weather Forecast Data with xcube","lvl2":"Run the dashboard"},"content":"\n\nviewer.info()       # open dashboard in separate tab in browser\n#viewer.show()      # open dashboard in current Jupyter Notebook","type":"content","url":"/extremedt-datacube-xviewer#run-the-dashboard","position":21},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes"},"type":"lvl1","url":"/extremedt-datacube","position":0},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes"},"content":"🚀 Launch in JupyterHub\n\n","type":"content","url":"/extremedt-datacube","position":1},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl2":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT"},"type":"lvl2","url":"/extremedt-datacube#data-cube-populated-with-data-obtained-from-weather-and-geophysical-extremes-digital-twin-dt-extremedt","position":2},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl2":"Data Cube populated with data obtained from Weather and Geophysical Extremes Digital Twin (DT) - ExtremeDT"},"content":"This notebook covers:\n\nfind available data cubes and their urls\n\nupload data cube\n\nplot map for desired area, time and variable\n\nplot time series chart for selected variable in specifc time for specific location\n\ncreate interactive dashboard using xcube - xviewer\n\n","type":"content","url":"/extremedt-datacube#data-cube-populated-with-data-obtained-from-weather-and-geophysical-extremes-digital-twin-dt-extremedt","position":3},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl2":"Prepre your environment"},"type":"lvl2","url":"/extremedt-datacube#prepre-your-environment","position":4},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl2":"Prepre your environment"},"content":"\n\nimport xarray as xr\nimport numpy as np\nimport matplotlib.pyplot as plt\nimport dask\nimport requests\n\n","type":"content","url":"/extremedt-datacube#prepre-your-environment","position":5},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl2":"Connect with Extreme DT data cube"},"type":"lvl2","url":"/extremedt-datacube#connect-with-extreme-dt-data-cube","position":6},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl2":"Connect with Extreme DT data cube"},"content":"The data cube provides data:\n\nFour variables\n\n2t - Air temperature at 2 meters above grond [K]\n\n2d - Dew point temperature at 2 meters above grond [K]\n\nsp - Surface pressure [Pa]\n\nForecast from 10.04.2024 + 96 hours\n\nHourly step\n\nWorld\n\n","type":"content","url":"/extremedt-datacube#connect-with-extreme-dt-data-cube","position":7},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl2":"Select proper data cube"},"type":"lvl2","url":"/extremedt-datacube#select-proper-data-cube","position":8},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl2":"Select proper data cube"},"content":"Data cubes on s3 bucket are stored under URL \n\nhttps://​s3​.central​.data​.destination​-earth​.eu​/swift​/v1​/dedl​_datacube.Data cubes are stored in two directories:\n\nExtremeDT - the newest one\n\narchive - from prievous days\n\nFile nameing convention:dt_extreme_YYYYMMDD.zarr/YYYYMMDD - is the date when forecast starts (step 0)\n\nResultsAfter exectution of code below, the list of urls linked to available cubes will be printed.\n\n# URL to s3 where  ExtremeDT data cubes are stored\ndatacube_url = 'https://s3.central.data.destination-earth.eu/swift/v1/dedl_datacube'\nresponse = requests.get(datacube_url)\n\nif response.status_code == 200:\n    lines = response.text.splitlines()\n    zarr_items = [line for line in lines if line.endswith(\".zarr\") or line.endswith(\".zarr/\")]\n    if zarr_items:\n        for item in zarr_items:\n            print(item)\n            new_url = f\"{datacube_url}/{item}\"\n            print(\"New URL:\", new_url)\n    else:\n        print(\"No .zarr files or directories found.\")\nelse:\n    print(\"Failed to fetch contents. Status code:\", response.status_code)\n\n\nGet info about the newset data cube.\n\n# Paste into url variable link to the newest data cube\nurl = 'https://s3.central.data.destination-earth.eu/swift/v1/dedl_datacube/ExtremeDT/dt_extreme_20240410.zarr/'\n\n","type":"content","url":"/extremedt-datacube#select-proper-data-cube","position":9},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl2":"Let’s make some test"},"type":"lvl2","url":"/extremedt-datacube#lets-make-some-test","position":10},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl2":"Let’s make some test"},"content":"\n\n","type":"content","url":"/extremedt-datacube#lets-make-some-test","position":11},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl3":"Area of interest","lvl2":"Let’s make some test"},"type":"lvl3","url":"/extremedt-datacube#area-of-interest","position":12},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl3":"Area of interest","lvl2":"Let’s make some test"},"content":"Upload data for selected area and verify what variables are provided. In this case uplaod data for Africa. List of available variables should be returend.\n\nafrica_bbox = [-20,     # West\n                -40,    # South\n                60,     # East\n                40]     #North\n\nafrica_dt = xr.open_zarr(url).sel(lon=slice(africa_bbox[0], \n                                            africa_bbox[2]), \n                                lat=slice(africa_bbox[3], \n                                            africa_bbox[1]),\n                                                         )\n\nlist(africa_dt.keys())\n\nPlot map of air temperature for Africa.\n\nlon = africa_dt['lon']\nlat = africa_dt['lat']\ntemperature = africa_dt['2t'][0, 0] - 273.15 # Conversion to Celcius degrees\n\nplt.figure(figsize=(10, 6))\nplt.pcolormesh(lon, lat, temperature, cmap='coolwarm')\nplt.colorbar(label='Temperature (°C)')\nplt.title('Temperature Map')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\n\n","type":"content","url":"/extremedt-datacube#area-of-interest","position":13},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl2":"Get data for specific time range"},"type":"lvl2","url":"/extremedt-datacube#get-data-for-specific-time-range","position":14},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl2":"Get data for specific time range"},"content":"Get data from 10st of April to 11th of April.\n\nafrica_bbox = [-20,     # West\n                -40,    # South\n                60,     # East\n                40]     #North\n\nafrica_dt = xr.open_zarr(url).sel(lon=slice(africa_bbox[0], \n                                            africa_bbox[2]), \n                                lat=slice(africa_bbox[3], \n                                            africa_bbox[1]),\n                                time=slice('20240410T000000', '20240411T000000')\n                                                         )\n\nprint(africa_dt.time)\n\n","type":"content","url":"/extremedt-datacube#get-data-for-specific-time-range","position":15},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl2":"Obtain data for specific variable and time"},"type":"lvl2","url":"/extremedt-datacube#obtain-data-for-specific-variable-and-time","position":16},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl2":"Obtain data for specific variable and time"},"content":"Obtain surface pressure data from 10th of April to 11th of April.\n\nafrica_bbox = [-20,     # West\n                -40,    # South\n                60,     # East\n                40]     #North\n\nafrica_dt = xr.open_zarr(url)['sp'].sel(lon=slice(africa_bbox[0], \n                                            africa_bbox[2]), \n                                lat=slice(africa_bbox[3], \n                                            africa_bbox[1]),\n                                time=slice('20240410T000000', '20240411T000000')\n                                                         )\n\nprint(africa_dt.var)\n\nPlot map of surface pressure over Africa.\n\nlon = africa_dt['lon']\nlat = africa_dt['lat']\ntemperature = africa_dt[0] / 100    # Conversion to hectoPascals\n\nplt.figure(figsize=(10, 6))\nplt.pcolormesh(lon, lat, temperature, cmap='viridis')\nplt.colorbar(label='Surface Pressure (hPa)')\nplt.title('Surface pressure Map')\nplt.xlabel('Longitude')\nplt.ylabel('Latitude')\n\n","type":"content","url":"/extremedt-datacube#obtain-data-for-specific-variable-and-time","position":17},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl2":"Time series"},"type":"lvl2","url":"/extremedt-datacube#time-series","position":18},{"hierarchy":{"lvl1":"ExtremeDT Weather Data Cubes","lvl2":"Time series"},"content":"Verify if it is possible to create time series chart (for 96 hours) from DT output - air temperature over Nairobi.\n\nafrica_bbox = [-20,     # West\n                -40,    # South\n                60,     # East\n                40]     #North\n\nafrica_dt = xr.open_zarr(url).sel(lon=slice(africa_bbox[0], \n                                            africa_bbox[2]), \n                                lat=slice(africa_bbox[3], \n                                            africa_bbox[1]),\n                                )\n                                                    \n\nCreate a chart.\n\n# Define Nairobi coordinates\nnairobi_lat = -1.286389\nnairobi_lon = 36.817223\n\nlat = africa_dt['lat']\nlon = africa_dt['lon']\nnearest_lat_idx = np.abs(lat - nairobi_lat).argmin()\nnearest_lon_idx = np.abs(lon - nairobi_lon).argmin()\n\ntemperature_nairobi = africa_dt['2t'][:, :, nearest_lat_idx, nearest_lon_idx] - 273.15\ntime_values = africa_dt.time.values\n\nplt.figure(figsize=(10, 6))\nplt.plot(time_values, temperature_nairobi, marker='o', color='b')\nplt.title('Air Temperature Time Series for Nairobi')\nplt.xlabel('Time')\nplt.ylabel('Temperature (°C)')\nplt.grid(True)\nplt.show()\n","type":"content","url":"/extremedt-datacube#time-series","position":19},{"hierarchy":{"lvl1":"STACK service - Dask 101"},"type":"lvl1","url":"/stack-dask-101","position":0},{"hierarchy":{"lvl1":"STACK service - Dask 101"},"content":"🚀 Launch in JupyterHub\n\n","type":"content","url":"/stack-dask-101","position":1},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl2":"Dask Core Library (APIs)"},"type":"lvl2","url":"/stack-dask-101#dask-core-library-apis","position":2},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl2":"Dask Core Library (APIs)"},"content":"Dask provides several APIs, also called collections, to enable distributed+parallel execution on larger-than-memory datasets.\nWe can think of Dask’s APIs at a high and a low level:\n\n\n\nHigh-level collections:  Dask provides high-level Array, Bag, and DataFrame\ncollections that mimic NumPy, lists, and pandas but can operate in parallel on\ndatasets that don’t fit into memory.\n\nLow-level collections:  Dask also provides low-level Tasks (Delayed and Futures)\ncollections that give you finer control to build custom parallel and distributed computations.\n\nIn this tutorial we will focus on Dask Arrays and Tasks (Delayed and Futures). Please visit the \n\nDask Examples and \n\nDask Tutorial for additional information.\n\n","type":"content","url":"/stack-dask-101#dask-core-library-apis","position":3},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl3":"dask.array - parallelized numpy","lvl2":"Dask Core Library (APIs)"},"type":"lvl3","url":"/stack-dask-101#dask-array-parallelized-numpy","position":4},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl3":"dask.array - parallelized numpy","lvl2":"Dask Core Library (APIs)"},"content":"Parallel, larger-than-memory, n-dimensional array using blocked algorithms.\n\nParallel: Uses all of the cores on your computer\n\nLarger-than-memory:  Lets you work on datasets that are larger than your available memory by breaking up your array into many small pieces, operating on those pieces in an order that minimizes the memory footprint of your computation, and effectively streaming data from disk.\n\nBlocked Algorithms:  Perform large computations by performing many smaller computations.\n\n\n\nIn other words, Dask Array implements a subset of the NumPy ndarray interface using blocked algorithms, cutting up the large array into many small arrays. This lets us compute on arrays larger than memory using all of our cores. We coordinate these blocked algorithms using Dask graphs.\n\nIn this notebook, we’ll build some understanding by implementing some blocked algorithms from scratch.\nWe’ll then use Dask Array to analyze large datasets, in parallel, using a familiar NumPy-like API.\n\nRelated Documentation\n\nArray documentation\n\nArray screencast\n\nArray API\n\nArray examples\n\n","type":"content","url":"/stack-dask-101#dask-array-parallelized-numpy","position":5},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl4":"Arrays - Example","lvl3":"dask.array - parallelized numpy","lvl2":"Dask Core Library (APIs)"},"type":"lvl4","url":"/stack-dask-101#arrays-example","position":6},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl4":"Arrays - Example","lvl3":"dask.array - parallelized numpy","lvl2":"Dask Core Library (APIs)"},"content":"\n\nA dask array looks and feels a lot like a numpy array. However, a dask array doesn’t directly hold any data. Instead, it symbolically represents the computations needed to generate the data. Nothing is actually computed until the actual numerical values are needed. This mode of operation is called “lazy”; it allows one to build up complex, large calculations symbolically before turning them over the scheduler for execution.\n\nIf we want to create a numpy array of all ones, we do it like this:\n\nimport numpy as np\nshape = (1000, 4000)\nones_np = np.ones(shape)\nones_np\n\nThis array contains exactly 32 MB of data:\n\nprint('%.1f MB' % (ones_np.nbytes / 1e6))\n\nNow let’s create the same array using dask’s array interface.\n\nimport dask.array as da\nones = da.ones(shape)\nones\n\nThis works, but we didn’t tell Dask how to split up the array, so it is not optimized for distributed computation.\n\nA crucial difference with Dask is that we must specify the chunks argument. “Chunks” describes how the array is split up over many sub-arrays.\n\nThere are several ways to \n\nspecify chunks.\n\nchunk_shape = (1000, 1000)\nones = da.ones(shape, chunks=chunk_shape)\nones\n\nNotice that we just see a symbolic representation of the array, including its shape, dtype, and chunksize. No data has been generated yet. When we call .compute() on a dask array, the computation is trigger and the dask array becomes a numpy array.\n\nones.compute()\n\nIn order to understand what happened when we called .compute(), we can visualize the Dask graph, the symbolic operations, that make up the array.\n\nones.visualize(format='svg')\n\nThe array has four chunks. To generate it, Dask calls np.ones four times and then concatenates this together into one array.\n\nRather than immediately loading a Dask array (which puts all the data into RAM), it is more common to reduce the data somehow. For example:\n\nsum_of_ones = ones.sum()\nsum_of_ones.visualize(format='svg')\n\nHere we see Dask’s strategy for finding the sum. This simple example illustrates the beauty of Dask: it automatically designs an algorithm appropriate for custom operations with big data.\n\nIf we make our operation more complex, the graph gets more complex.\n\nfancy_calculation = (ones * ones[::-1, ::-1]).mean()\nfancy_calculation.visualize(format='svg')\n\n","type":"content","url":"/stack-dask-101#arrays-example","position":7},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl3":"dask.delayed - parallelize generic Python code","lvl2":"Dask Core Library (APIs)"},"type":"lvl3","url":"/stack-dask-101#dask-delayed-parallelize-generic-python-code","position":8},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl3":"dask.delayed - parallelize generic Python code","lvl2":"Dask Core Library (APIs)"},"content":"What if you don’t have an Dask array or Dask dataframe? Instead of having blocks where the function is applied to each block, you can decorate functions with @delayed and have the functions themselves be lazy. Rather than compute its result immediately, it records what needs to be computed as a task into a graph that we’ll run later on parallel hardware.\n\nThis is a simple way to use Dask to parallelize existing codebases or build \n\ncomplex systems.\n\nRelated Documentation\n\nDelayed documentation\n\nDelayed screencast\n\nDelayed API\n\nDelayed examples\n\nDelayed best practices\n\nA typical workfow Read-Transform-Write workflow are most often implemented as outlined hereafter.\nIn general, most workflows containing a for-loop can benefit from dask.delayed.import dask\n    \n@dask.delayed\ndef process_file(filename):\n    data = read_a_file(filename)\n    data = do_a_transformation(data)\n    destination = f\"results/{filename}\"\n    write_out_data(data, destination)\n    return destination\n\nresults = []\nfor filename in filenames:\n    results.append(process_file(filename))\n    \ndask.compute(results)\n\n","type":"content","url":"/stack-dask-101#dask-delayed-parallelize-generic-python-code","position":9},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl5":"dask.delayed - Example","lvl3":"dask.delayed - parallelize generic Python code","lvl2":"Dask Core Library (APIs)"},"type":"lvl5","url":"/stack-dask-101#dask-delayed-example","position":10},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl5":"dask.delayed - Example","lvl3":"dask.delayed - parallelize generic Python code","lvl2":"Dask Core Library (APIs)"},"content":"For demonstration purposes we will create simple functions to perform simple operations like add two numbers together, but they sleep for a random amount of time to simulate real work.\n\nimport time\n\ndef inc(x):\n    time.sleep(0.1)\n    return x + 1\n\ndef dec(x):\n    time.sleep(0.1)\n    return x - 1\n\ndef add(x, y):\n    time.sleep(0.2)\n    return x + y\n\nWe can run them like normal Python functions below\n\n%%time\nx = inc(1)\ny = dec(2)\nz = add(x, y)\nz\n\nThese ran one after the other, in sequence. Note though that the first two lines inc(1) and dec(2) don’t depend on each other, we could have called them in parallel.\n\nWe can call dask.delayed on these funtions to make them lazy. Rather than compute their results immediately, they record what we want to compute as a task into a graph that we’ll run later on parallel hardware.\n\nimport dask\ninc = dask.delayed(inc)\ndec = dask.delayed(dec)\nadd = dask.delayed(add)\n\nCalling these lazy functions is now almost free. We’re just constructing a graph\n\n%%time\nx = inc(1)\ny = dec(2)\nz = add(x, y)\nz\n\nVisualize computation\n\nz.visualize(format='svg', rankdir='LR')\n\nRun in parallel. Call .compute() when you want your result as a normal Python object\n\n%%time\nz.compute()\n\nParallelize Normal Python code\n\nNow we use dask.delayed in a normal for-loop Python code as given in the example above. This generates graphs instead of doing computations directly, but still looks like the code we had before. Dask is a convenient way to add parallelism to existing workflows.\n\n%%time\nzs = []\nfor i in range(256):\n    x = inc(i)\n    y = dec(x)\n    z = add(x, y)\n    zs.append(z)\n\nzs = dask.persist(*zs)   # trigger computation in the background\n\n","type":"content","url":"/stack-dask-101#dask-delayed-example","position":11},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl2":"Dask Cluster (dask.distributed)"},"type":"lvl2","url":"/stack-dask-101#dask-cluster-dask-distributed","position":12},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl2":"Dask Cluster (dask.distributed)"},"content":"Dask has the ability to run work on multiple machines using the distributed scheduler. dask.distributed is a lightweight library for distributed computing in Python. It extends both the concurrent.futures and Dask APIs to run on various clusters technologies such as Kubernetes, Yarn, SLURM, PBS, etc. .\nMost of the times when you are using Dask, you will be using a distributed scheduler, which exists in the context of a Dask cluster. When we talk about Dask Clusters we can think of those as depicted in the following:\n\n\n\n","type":"content","url":"/stack-dask-101#dask-cluster-dask-distributed","position":13},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl2":"Dask @ DEDL"},"type":"lvl2","url":"/stack-dask-101#dask-dedl","position":14},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl2":"Dask @ DEDL"},"content":"DestinE Data Lake utilises a deployment of \n\nDask Gateway on each location (bridge) in the data lake. Dask Gateway provides a secure, multi-tenant server for managing Dask clusters. It allows users to launch and use Dask clusters in a shared, centrally managed cluster environment, without requiring users to have direct access to the underlying cluster backend (e.g. Kubernetes, Hadoop/YARN, HPC Job queues, etc…).\n\nDask Gateway exposes a REST API to spawn clusters on demand. The overall architecture of Dask Gateway is depicted hereafter.\n","type":"content","url":"/stack-dask-101#dask-dedl","position":15},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl3":"How to connect and spawn a cluster?","lvl2":"Dask @ DEDL"},"type":"lvl3","url":"/stack-dask-101#how-to-connect-and-spawn-a-cluster","position":16},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl3":"How to connect and spawn a cluster?","lvl2":"Dask @ DEDL"},"content":"Central Site\n\naddress: \n\nhttp://​dask​.central​.data​.destination​-earth​.eu\n\nproxy_address: tcp://dask.central.data.destination-earth.eu:80\n\nLUMI Bridge\n\naddress: \n\nhttp://​dask​.lumi​.data​.destination​-earth​.eu\n\nproxy_address: tcp://dask.lumi.data.destination-earth.eu:80\n\nfrom dask_gateway.auth import GatewayAuth\nfrom getpass import getpass\nfrom destinelab import AuthHandler as DESP_AuthHandler\n\nclass DESPAuth(GatewayAuth):\n    def __init__(self, username: str):\n        self.auth_handler = DESP_AuthHandler(username, getpass(\"Please input your DESP password: \"))\n        self.access_token = self.auth_handler.get_token()\n    \n    def pre_request(self, _):\n        headers = {\"Authorization\": \"Bearer \" + self.access_token}\n        return headers, None\n\nOnly authenticated access is granted to the DEDL STACK service Dask, therefore a helper class to authenticate a user against the DESP identity management system is implemented. The users password is directly handed over to the request object and is not permanently stored.\n\nIn the following, please enter your DESP username and password. Again, the password will only be saved for the duration of this user session and will be remove as soon as the notebook/kernel is closed.\n\nfrom rich.prompt import Prompt\nmyAuth = DESPAuth(username=Prompt.ask(prompt=\"Username\"))\n\nfrom dask_gateway import Gateway\ngateway = Gateway(address=\"http://dask.central.data.destination-earth.eu\",\n                  proxy_address=\"tcp://dask.central.data.destination-earth.eu:80\",\n                  auth=myAuth)\n\nCluster creation and client instantiation to communicate with the new cluster\n\ncluster = gateway.new_cluster()\nclient = cluster.get_client()\ncluster\n\nUp to now the cluster will only consist of the distributed scheduler. If you want to spawn workers directly via Python adaptively, please use the following method call. With the following the cluster will be scaled to 2 workers initially. Depending on the load, Dask will add addtional workers, up to 5, if needed.\n\ncluster.adapt(minimum=2, maximum=5)\n\n","type":"content","url":"/stack-dask-101#how-to-connect-and-spawn-a-cluster","position":17},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl4":"dask.futures - non-blocking distributed calculations","lvl3":"How to connect and spawn a cluster?","lvl2":"Dask @ DEDL"},"type":"lvl4","url":"/stack-dask-101#dask-futures-non-blocking-distributed-calculations","position":18},{"hierarchy":{"lvl1":"STACK service - Dask 101","lvl4":"dask.futures - non-blocking distributed calculations","lvl3":"How to connect and spawn a cluster?","lvl2":"Dask @ DEDL"},"content":"We will now make use of the remote Dask Cluster using the Dask low-level collection dask.futures.\n\nSubmit arbitrary functions for computation in a parallelized, eager, and non-blocking way.\n\nThe futures interface (derived from the built-in concurrent.futures) provide fine-grained real-time execution for custom situations. We can submit individual functions for evaluation with one set of inputs, or evaluated over a sequence of inputs with submit() and map(). The call returns immediately, giving one or more futures, whose status begins as “pending” and later becomes “finished”. There is no blocking of the local Python session.\n\nImportant\n\nThis is the important difference between futures and delayed. Both can be used to support arbitrary task scheduling, but delayed is lazy (it just constructs a graph) whereas futures are eager. With futures, as soon as the inputs are available and there is compute available, the computation starts.\n\nRelated Documentation\n\nFutures documentation\n\nFutures screencast\n\nFutures examples\n\nThis is the same workflow that as given above in the dask.delayed section. It is a for-loop to iterate of certain files to perform a transformation and to write the result.def process_file(filename):\n    data = read_a_file(filename)\n    data = do_a_transformation(data)\n    destination = f\"results/{filename}\"\n    write_out_data(data, destination)\n    return destination\n\nfutures = []\nfor filename in filenames:\n    future = client.submit(process_file, filename)\n    futures.append(future)\n    \nfutures\n\nfrom time import sleep\n\n\ndef inc(x):\n    sleep(1)\n    return x + 1\n\nWe can run these function locally\n\ninc(1)\n\nOr we can submit them to run remotely with Dask. This immediately returns a future that points to the ongoing computation, and eventually to the stored result.\n\nfuture = client.submit(inc, 1)  # returns immediately with pending future\nfuture\n\nIf you wait a second, and then check on the future again, you’ll see that it has finished.\n\nfuture\n\nYou can block on the computation and gather the result with the .result() method.\n\nfuture.result()\n\nOther ways to wait for a futurefrom dask.distributed import wait, progress\nprogress(future)\n\nshows a progress bar in the notebook. This progress bar is also asynchronous, and doesn’t block the execution of other code in the meanwhile.wait(future)\n\nblocks and forces the notebook to wait until the computation pointed to by future is done. However, note that if the result of inc() is sitting in the cluster, it would take no time to execute the computation now, because Dask notices that we are asking for the result of a computation it already knows about. More on this later.\n\nOther ways to gather resultsclient.gather(futures)\n\ngathers results from more than one future.\n\nfrom dask.distributed import wait, progress\ndef inc(x):\n    sleep(1)\n    return x + 1\n\n\nfuture_x = client.submit(inc, 1)\nfuture_y = client.submit(inc, 2)\nfuture_z = client.submit(sum, [future_x, future_y])\nprogress(future_z)\n\nRemove your cluster again to free up resources when you are done.\n\ncluster.close(shutdown=True)","type":"content","url":"/stack-dask-101#dask-futures-non-blocking-distributed-calculations","position":19},{"hierarchy":{"lvl1":"STACK service - Python Client Dask"},"type":"lvl1","url":"/stack-python-client-dask","position":0},{"hierarchy":{"lvl1":"STACK service - Python Client Dask"},"content":"🚀 Launch in JupyterHub\n\n","type":"content","url":"/stack-python-client-dask","position":1},{"hierarchy":{"lvl1":"STACK service - Python Client Dask","lvl3":"Multi-cloud processing with Dask"},"type":"lvl3","url":"/stack-python-client-dask#multi-cloud-processing-with-dask","position":2},{"hierarchy":{"lvl1":"STACK service - Python Client Dask","lvl3":"Multi-cloud processing with Dask"},"content":"","type":"content","url":"/stack-python-client-dask#multi-cloud-processing-with-dask","position":3},{"hierarchy":{"lvl1":"STACK service - Python Client Dask","lvl4":"Overview","lvl3":"Multi-cloud processing with Dask"},"type":"lvl4","url":"/stack-python-client-dask#overview","position":4},{"hierarchy":{"lvl1":"STACK service - Python Client Dask","lvl4":"Overview","lvl3":"Multi-cloud processing with Dask"},"content":"","type":"content","url":"/stack-python-client-dask#overview","position":5},{"hierarchy":{"lvl1":"STACK service - Python Client Dask","lvl5":"Content","lvl4":"Overview","lvl3":"Multi-cloud processing with Dask"},"type":"lvl5","url":"/stack-python-client-dask#content","position":6},{"hierarchy":{"lvl1":"STACK service - Python Client Dask","lvl5":"Content","lvl4":"Overview","lvl3":"Multi-cloud processing with Dask"},"content":"DestinE Data Lake (DEDL) Stack Client\n\nMaking use of clients context manager\n\nUse Case: Pakistan Flood 2022","type":"content","url":"/stack-python-client-dask#content","position":7},{"hierarchy":{"lvl1":"STACK service - Python Client Dask","lvl5":"Duration: 15 min.","lvl4":"Overview","lvl3":"Multi-cloud processing with Dask"},"type":"lvl5","url":"/stack-python-client-dask#duration-15-min","position":8},{"hierarchy":{"lvl1":"STACK service - Python Client Dask","lvl5":"Duration: 15 min.","lvl4":"Overview","lvl3":"Multi-cloud processing with Dask"},"content":"\n\nPlease make sure Python DEDL kernel is used.\n\nDestinE Data Lake utilises a deployment of \n\nDask Gateway on each location (bridge) in the data lake. Dask Gateway provides a secure, multi-tenant server for managing Dask clusters. It allows users to launch and use Dask clusters in a shared, centrally managed cluster environment, without requiring users to have direct access to the underlying cluster backend (e.g. Kubernetes, Hadoop/YARN, HPC Job queues, etc…).\n\nDask Gateway exposes a REST API to spawn clusters on demand. The overall architecture of Dask Gateway is depicted hereafter.","type":"content","url":"/stack-python-client-dask#duration-15-min","position":9},{"hierarchy":{"lvl1":"STACK service - Python Client Dask","lvl2":"DEDL Dask Gateway"},"type":"lvl2","url":"/stack-python-client-dask#dedl-dask-gateway","position":10},{"hierarchy":{"lvl1":"STACK service - Python Client Dask","lvl2":"DEDL Dask Gateway"},"content":"Central Site\n\naddress: \n\nhttp://​dask​.central​.data​.destination​-earth​.eu\n\nproxy_address: tcp://dask.central.data.destination-earth.eu:80\n\nLUMI Bridge\n\naddress: \n\nhttp://​dask​.lumi​.data​.destination​-earth​.eu\n\nproxy_address: tcp://dask.lumi.data.destination-earth.eu:80\n\nOnly authenticated access is granted to the DEDL STACK service Dask, therefore a helper class to authenticate a user against the DESP identity management system is implemented. The users password is directly handed over to the request object and is not permanently stored.\n\nIn the following, please enter your DESP username and password. Again, the password will only be saved for the duration of this user session and will be remove as soon as the notebook/kernel is closed.\n\nfrom dask_gateway.auth import GatewayAuth\nfrom getpass import getpass\nfrom destinelab import AuthHandler as DESP_AuthHandler\n\nclass DESPAuth(GatewayAuth):\n    def __init__(self, username: str):\n        self.auth_handler = DESP_AuthHandler(username, getpass(\"Please input your DESP password: \"))\n        self.access_token = self.auth_handler.get_token()\n    \n    def pre_request(self, _):\n        headers = {\"Authorization\": \"Bearer \" + self.access_token}\n        return headers, None\n\nfrom rich.prompt import Prompt\nmyAuth = DESPAuth(username=Prompt.ask(prompt=\"Username\"))\n\n","type":"content","url":"/stack-python-client-dask#dedl-dask-gateway","position":11},{"hierarchy":{"lvl1":"STACK service - Python Client Dask","lvl2":"DestinE Data Lake (DEDL) Stack Client"},"type":"lvl2","url":"/stack-python-client-dask#destine-data-lake-dedl-stack-client","position":12},{"hierarchy":{"lvl1":"STACK service - Python Client Dask","lvl2":"DestinE Data Lake (DEDL) Stack Client"},"content":"The \n\nDEDL Stack Client is a Python library to facilitate the use of Stack Service Dask. The main objective is to provide an abstraction layer to interact with the various clusters on each DEDL bridge. Computations can be directed to the different Dask clusters by making use of a context manager as given in the following.\n\nfrom dedl_stack_client.dask import DaskMultiCluster\n\nmyDEDLClusters = DaskMultiCluster(auth=myAuth)\nmyDEDLClusters.new_cluster()\n\nmyDEDLClusters.get_cluster_url()\n\nWe can again showcase the execution of standard Python functions on the remote clusters.\nIn the following we will make use of dask.futures, non-blocking distributed calculations, utilising the map() method for task distribution. Detailed information about dask.futures can be found on the \n\nDask documention.\n\nThis approach allows for embarrassingly parallel task scheduling, which is very similar to Function as a Service capabilities.\n\nfrom time import sleep as wait\n\ndef apply_myfunc(x):\n    wait(1)\n    return x+1\n\nWe want to run apply_myfunc() on Central Site and wait for all results to be ready. my_filelist_central represents a filelist to be processed by apply_myfunc().\n\nmy_filelist_central = range(20)\nwith myDEDLClusters.as_current(location=\"central\") as myclient:\n    central_future = myclient.map(apply_myfunc, my_filelist_central)\n    results_central = myclient.gather(central_future)\n\nresults_central\n\nRun computation at LUMI bridge.\n\nmy_filelist_lumi = range(32)\nwith myDEDLClusters.as_current(location=\"lumi\") as myclient:\n    lumi_future = myclient.map(apply_myfunc, my_filelist_lumi)\n    results_lumi = myclient.gather(lumi_future)\n\nresults_lumi\n\n","type":"content","url":"/stack-python-client-dask#destine-data-lake-dedl-stack-client","position":13},{"hierarchy":{"lvl1":"STACK service - Python Client Dask","lvl2":"Limitations"},"type":"lvl2","url":"/stack-python-client-dask#limitations","position":14},{"hierarchy":{"lvl1":"STACK service - Python Client Dask","lvl2":"Limitations"},"content":"Python libraries use in the local environment need to match, same version, with those available in the Dask Cluster. If this is not the case, you will get a warning, code might work but not guaranteed.\n\nNo direct data exchange between Dask Workers across cloud locations possible. Each location acts as atmoic unit, however data can be easily exchanged via storage services such as S3.\n\n","type":"content","url":"/stack-python-client-dask#limitations","position":15},{"hierarchy":{"lvl1":"STACK service - Python Client Dask","lvl2":"Use Case example: Pakistan Flood 2022"},"type":"lvl2","url":"/stack-python-client-dask#use-case-example-pakistan-flood-2022","position":16},{"hierarchy":{"lvl1":"STACK service - Python Client Dask","lvl2":"Use Case example: Pakistan Flood 2022"},"content":"The complete use case is available on GitHub via \n\nhttps://​github​.com​/destination​-earth​/DestinE​_EUMETSAT​_PakistanFlood​_2022.\n\nThe use case demonstrates the multi-cloud capabilities of DEDL following the paradigm of data proximate computing. Data of the Global Flood Monitoring (GFM) service as well as Climate DT outputs, simulated by utilising ERA5 data have been use for flood risk assessment.\n\nData is stored as datacubes (zarr format) at Central Site and at LUMI bridge in object storage.\n\nimport s3fs\nimport xarray as xr\n\nxr.set_options(keep_attrs=True)\n\ns3fs_central = s3fs.S3FileSystem(\n    anon=True,\n    use_ssl=True,\n    client_kwargs={\"endpoint_url\": \"https://s3.central.data.destination-earth.eu\"})\n\ns3fs_lumi = s3fs.S3FileSystem(\n    anon=True,\n    use_ssl=True,\n    client_kwargs={\"endpoint_url\": \"https://s3.lumi.data.destination-earth.eu\"})\n\nWe can list the data available at Central Site.\n\ns3fs_central.ls(\"increment1-testdata\")\n\nRead data stored in S3 bucket at Central Site. The data we want to read is a single Zarr data store representing the GFM flood data over Pakistan for 2022-08-30.\n\nflood_map = xr.open_zarr(store=s3fs.S3Map(root=f\"increment1-testdata/2022-08-30.zarr\", s3=s3fs_central, check=False),\n                         decode_coords=\"all\",)[\"flood\"].assign_attrs(location=\"central\", resolution=20)\n#flood_map\n\nWe now want to run simple computation and compute the flooded area for the this day in August 2022.\n\nflooded_area_ = flood_map.sum()*20*20/1000.\n#flooded_area_\n\nSo far we haven’t computed anything, so lets do the computation now on the Dask cluster.\n\nfrom rich.console import Console\nfrom rich.prompt import Prompt\nconsole = Console()\n\nflooded_area = myDEDLClusters.compute(flooded_area_, sync=True)\nconsole.print(f\"Flooded area: {flooded_area.data} km2\")\n\nHow was that processing routed to Dask Gateway at Central Site?\n\nmyDEDLClusters.compute(flooded_area_, sync=True) checks for annotations (attributes) of array and maps that to available Dask Clusters.\n\nPreprocess GFM data at Central Site for visualiation\n\ndef preprocess_dataset(data_array: xr.DataArray, method: str):\n    data_array = data_array.squeeze()\n    steps = 500 // data_array.attrs[\"resolution\"]\n    coarsened = data_array.coarsen({'y': steps, 'x': steps}, boundary='trim')\n    if method == 'median':\n        data_array = (coarsened.median() > 0).astype('float32')\n    elif method == 'mean':\n        data_array = coarsened.mean()\n    elif method == 'max':\n        data_array = coarsened.max()\n    else:\n        raise NotImplementedError(method)\n    return data_array\n\nflood_prep_ = preprocess_dataset(flood_map, 'median')\n\nimport numpy as np\nflood_prep = myDEDLClusters.compute(flood_prep_, sync=True)\nflood_prep.rio.write_crs(\"epsg:4326\", inplace=True)\nflood_prep = flood_prep.rio.reproject(f\"EPSG:3857\", nodata=np.nan)\n\nVisualise flood data on map.\n\nimport leafmap\nfrom attr import dataclass\n\n@dataclass\nclass Extent:\n    min_x: float\n    min_y: float\n    max_x: float\n    max_y: float\n    crs: str\n    def get_center(self):\n        return (np.mean([self.min_y, self.max_y]),\n                np.mean([self.min_x,self.max_x]))\n\n\nroi_extent = Extent(65, 21, 71, 31, crs='EPSG:4326')\n\nm = leafmap.Map(center=roi_extent.get_center(),\n                zoom=8, height=600)\nm.add_raster(flood_prep, colormap=\"Blues\", layer_name=\"Flood\", nodata=0.)\n\nm\n\nRead data stored in S3 bucket at LUMI bridge (Finland). Data we want to read is a datacube generated from ERA-5 representing predicted rainfall data.\n\nrainfall = xr.open_zarr(store=s3fs.S3Map(root=f\"increment1-testdata/predicted_rainfall.zarr\",\n                                         s3=s3fs_lumi,\n                                         check=False),\n                        decode_coords=\"all\",)[\"tp\"].assign_attrs(location=\"lumi\", resolution=20)\n\nAnd again run the computation close to the data, therefore at LUMI bridge.\n\nFirst we compute the accumulated rainfall over Pakistan.\nSecondly we compute the average rainfall for August 2022 (monthly mean) at global scale.\n\nfrom datetime import datetime\n\ndef accum_rain_predictions(rain_data, startdate, enddate, extent):\n    rain_ = rain_data.sel(time=slice(startdate, enddate),\n                          latitude=slice(extent.max_y, extent.min_y),\n                          longitude=slice(extent.min_x, extent.max_x))\n    return rain_.cumsum(dim=\"time\", keep_attrs=True)*1000\n\n# compute accumulated rainfall over Pakistan\nacc_rain_ = accum_rain_predictions(rainfall, startdate=datetime(2022, 8, 18),\n                                                  enddate=datetime(2022, 8, 30),\n                                                  extent=roi_extent)\nacc_rain_ = acc_rain_.rename({\"longitude\":\"x\", \"latitude\":\"y\"})\n\nacc_rain = myDEDLClusters.compute(acc_rain_, sync=True)\n\ndef acc_rain_reproject(rain):\n    from rasterio.enums import Resampling\n    rain.rio.write_nodata(0, inplace=True)\n    rain.rio.write_crs('EPSG:4326', inplace=True)\n    return rain.rio.reproject('EPSG:3857', resolution=500, resampling=Resampling.bilinear)\n\nacc_rain = acc_rain_reproject(acc_rain)\n\nVisualise forecast data provided by the Digital Twin which could have been used for flood risk assessment or even alerting.\n\ntime_dim_len = acc_rain.shape[0]\nfor day in range(0, time_dim_len):\n    fpath_str = f\"./{day}.tif\"\n    acc_rain[day,:].rio.to_raster(fpath_str,\n                                  driver=\"COG\",\n                                  overview_count=10)\n\nimport leafmap\nfrom localtileserver import get_leaflet_tile_layer\n\nm = leafmap.Map(center=roi_extent.get_center(),\n                zoom=6, height=600)\n\nlayer_dict = {}\ndate_vals = np.datetime_as_string(acc_rain[\"time\"].values, unit='D')\nfor day in range(0, time_dim_len):\n    layer_dict[date_vals[day]]= get_leaflet_tile_layer(f\"./{day}.tif\",\n                                                       colormap=\"Blues\",\n                                                       indexes=[1],\n                                                       nodata=0.,\n                                                       vmin=acc_rain.min().values,\n                                                       vmax=acc_rain.max().values,\n                                                       opacity=0.85)\n\nm.add_local_tile(flood_prep,\n                 colormap=\"Blues\",\n                 nodata=0.)\nm.add_time_slider(layer_dict,\n                  layer=\"Accumluated Rainfall\",\n                  time_interval=1.)\nm\n\nmyDEDLClusters.shutdown()","type":"content","url":"/stack-python-client-dask#use-case-example-pakistan-flood-2022","position":17}]}